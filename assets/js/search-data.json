{
  
    
        "post0": {
            "title": "Ranzr Kaggle",
            "content": "This post will accomplish 2 things: . Demonstrate top 10 LB scoring submission in a multi-label image classification problem | Compare doing this in fastai vs pure pytorch . Note: I do not use fastai all the time myself. For example I found it a lot easier when working with StyleGAN (2 generators/2 discriminators) This is a &quot;See why fastai should be an additional tool in your toolbelt&quot; post, not a &quot;This is why fastai should be used exclusively post&quot; Now, I do realize that there are some things I prefer doing in pure pytorch. For example, I was working with StyleGan (2 generators/2 discriminators all in the same training loop) and I was having a hard time building it out and getting it working in fastai. So I used pure pytorch. I don&#39;t always use fastai myself. This isn&#39;t to be used as irrefutable proof that you should always use fastai. | Being a person who likes to simplify things as much as possible I thought &quot;Maybe I should really be doing everything in pure pytorch...&quot;. So I set out to do a new image classification problem using pure pytorch. I ended up switching back to fastai. That leads me to the first goal of the article: To show the differences between solving an image classicifation problem in pytorch vs fastai! . The second goal is to do this well. It always bothers me a bit that comparisons are done using toy problems or really easy problems. The truth is that every library looks simple, easy, and gets good results effortlessly on these. I could compare using MNIST or some other easy dataset, but a library being able to solve MNIST in a clean way isn&#39;t that valuable. . !pip install -Uqq fastai . import torch.nn as nn import torch import pandas as pd import timm from fastai.vision.all import * from fastcore.foundation import * from fastcore.xtras import * from pathlib import Path from torch import Tensor import numpy as np from torch.utils.data import Dataset, DataLoader import os from skimage import io import random import matplotlib.pyplot as plt import math import albumentations as A . path = Path(&#39;./tmp&#39;) path.ls() . (#6) [Path(&#39;tmp/train.csv&#39;),Path(&#39;tmp/sample_submission.csv&#39;),Path(&#39;tmp/train_annotations.csv&#39;),Path(&#39;tmp/train&#39;),Path(&#39;tmp/test&#39;),Path(&#39;tmp/image_shapes.csv&#39;)] . https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/overview . Data Cleaning . # shapes = L() # for i in range(len(ds)): # shapes.append(ds[i][&#39;x&#39;].shape) # shape_df = pd.DataFrame(np.array(shapes)) # shape_df[2] = 1 # shape_df.groupby([0,1]).count().sort_values(2,ascending=False).to_csv(path/&#39;image_shapes.csv&#39;) . Dataset . maybe something based on same patient tested multiple times | . torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods: . len so that len(dataset) returns the size of the dataset. getitem to support the indexing such that dataset[i] can be used to get ith sample . train = pd.read_csv(path/&#39;train.csv&#39;) # annot = pd.read_csv(path/&#39;train_annotations.csv&#39;) # samp_sub = pd.read_csv(path/&#39;sample_submission.csv&#39;) train[&#39;is_valid&#39;] = np.random.rand(len(train))&gt;0.8 train[&#39;StudyInstanceUID&#39;] = &#39;./tmp/train/&#39;+train[&#39;StudyInstanceUID&#39;]+&#39;.jpg&#39; train[&#39;multi-label&#39;] = None for r in range(len(train)): train.iloc[r,-1] = &#39;|&#39;.join([train.columns[i+1] for i in range(10) if train.iloc[r,i+1] == 1]) train.head() . StudyInstanceUID ETT - Abnormal ETT - Borderline ETT - Normal NGT - Abnormal NGT - Borderline NGT - Incompletely Imaged NGT - Normal CVC - Abnormal CVC - Borderline CVC - Normal Swan Ganz Catheter Present PatientID is_valid multi-label . 0 ./tmp/train/1.2.826.0.1.3680043.8.498.26697628953273228189375557799582420561.jpg | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | ec89415d1 | True | NGT - Normal | . 1 ./tmp/train/1.2.826.0.1.3680043.8.498.46302891597398758759818628675365157729.jpg | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | bf4c6da3c | False | ETT - Normal|NGT - Incompletely Imaged|CVC - Normal | . 2 ./tmp/train/1.2.826.0.1.3680043.8.498.23819260719748494858948050424870692577.jpg | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3fc1c97e5 | False | CVC - Borderline | . 3 ./tmp/train/1.2.826.0.1.3680043.8.498.68286643202323212801283518367144358744.jpg | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | c31019814 | False | CVC - Abnormal | . 4 ./tmp/train/1.2.826.0.1.3680043.8.498.10050203009225938259119000528814762175.jpg | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 207685cd1 | False | CVC - Normal | . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=ColReader(&#39;StudyInstanceUID&#39;), get_y=ColReader(&#39;multi-label&#39;, label_delim=&#39;|&#39;), splitter=ColSplitter(), item_tfms=Resize(224), batch_tfms=aug_transforms(size=224,p_lighting=0.9,flip_vert=False,max_zoom=1.2,max_warp=0)) dls = dblock.dataloaders(train) dls.show_batch(max_n=16) . learn = cnn_learner(dls,resnet18,metrics=accuracy) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /home/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . learn.fine_tune(5) . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss accuracy time . . 0.00% [0/96 00:00&lt;00:00] &lt;/div&gt; &lt;/div&gt; AssertionError Traceback (most recent call last) &lt;ipython-input-14-f30dc4cef83b&gt; in &lt;module&gt; -&gt; 1 learn.fine_tune(5) /opt/conda/lib/python3.6/site-packages/fastai/callback/schedule.py in fine_tune(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs) 155 &#34;Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR&#34; 156 self.freeze() --&gt; 157 self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) 158 base_lr /= 2 159 self.unfreeze() /opt/conda/lib/python3.6/site-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt) 110 scheds = {&#39;lr&#39;: combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final), 111 &#39;mom&#39;: combined_cos(pct_start, *(self.moms if moms is None else moms))} --&gt; 112 self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd) 113 114 # Cell /opt/conda/lib/python3.6/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt) 209 self.opt.set_hypers(lr=self.lr if lr is None else lr) 210 self.n_epoch = n_epoch --&gt; 211 self._with_events(self._do_fit, &#39;fit&#39;, CancelFitException, self._end_cleanup) 212 213 def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _do_fit(self) 200 for epoch in range(self.n_epoch): 201 self.epoch=epoch --&gt; 202 self._with_events(self._do_epoch, &#39;epoch&#39;, CancelEpochException) 203 204 def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _do_epoch(self) 195 def _do_epoch(self): 196 self._do_epoch_train() --&gt; 197 self._do_epoch_validate() 198 199 def _do_fit(self): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _do_epoch_validate(self, ds_idx, dl) 191 if dl is None: dl = self.dls[ds_idx] 192 self.dl = dl --&gt; 193 with torch.no_grad(): self._with_events(self.all_batches, &#39;validate&#39;, CancelValidException) 194 195 def _do_epoch(self): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /opt/conda/lib/python3.6/site-packages/fastai/learner.py in all_batches(self) 164 def all_batches(self): 165 self.n_iter = len(self.dl) --&gt; 166 for o in enumerate(self.dl): self.one_batch(*o) 167 168 def _do_one_batch(self): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in one_batch(self, i, b) 182 self.iter = i 183 self._split(b) --&gt; 184 self._with_events(self._do_one_batch, &#39;batch&#39;, CancelBatchException) 185 186 def _do_epoch_train(self): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) --&gt; 162 self(f&#39;after_{event_type}&#39;); final() 163 164 def all_batches(self): /opt/conda/lib/python3.6/site-packages/fastai/learner.py in __call__(self, event_name) 139 140 def ordered_cbs(self, event): return [cb for cb in self.cbs.sorted(&#39;order&#39;) if hasattr(cb, event)] --&gt; 141 def __call__(self, event_name): L(event_name).map(self._call_one) 142 143 def _call_one(self, event_name): /opt/conda/lib/python3.6/site-packages/fastcore/foundation.py in map(self, f, gen, *args, **kwargs) 152 def range(cls, a, b=None, step=None): return cls(range_of(a, b=b, step=step)) 153 --&gt; 154 def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs)) 155 def argwhere(self, f, negate=False, **kwargs): return self._new(argwhere(self, f, negate, **kwargs)) 156 def filter(self, f=noop, negate=False, gen=False, **kwargs): /opt/conda/lib/python3.6/site-packages/fastcore/basics.py in map_ex(iterable, f, gen, *args, **kwargs) 664 res = map(g, iterable) 665 if gen: return res --&gt; 666 return list(res) 667 668 # Cell /opt/conda/lib/python3.6/site-packages/fastcore/basics.py in __call__(self, *args, **kwargs) 649 if isinstance(v,_Arg): kwargs[k] = args.pop(v.i) 650 fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:] --&gt; 651 return self.func(*fargs, **kwargs) 652 653 # Cell /opt/conda/lib/python3.6/site-packages/fastai/learner.py in _call_one(self, event_name) 143 def _call_one(self, event_name): 144 if not hasattr(event, event_name): raise Exception(f&#39;missing {event_name}&#39;) --&gt; 145 for cb in self.cbs.sorted(&#39;order&#39;): cb(event_name) 146 147 def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state) /opt/conda/lib/python3.6/site-packages/fastai/callback/core.py in __call__(self, event_name) 42 (self.run_valid and not getattr(self, &#39;training&#39;, False))) 43 res = None &gt; 44 if self.run and _run: res = getattr(self, event_name, noop)() 45 if event_name==&#39;after_fit&#39;: self.run=True #Reset self.run to True at each end of fit 46 return res /opt/conda/lib/python3.6/site-packages/fastai/learner.py in after_batch(self) 492 if len(self.yb) == 0: return 493 mets = self._train_mets if self.training else self._valid_mets --&gt; 494 for met in mets: met.accumulate(self.learn) 495 if not self.training: return 496 self.lrs.append(self.opt.hypers[-1][&#39;lr&#39;]) /opt/conda/lib/python3.6/site-packages/fastai/learner.py in accumulate(self, learn) 414 def accumulate(self, learn): 415 bs = find_bs(learn.yb) --&gt; 416 self.total += learn.to_detach(self.func(learn.pred, *learn.yb))*bs 417 self.count += bs 418 @property /opt/conda/lib/python3.6/site-packages/fastai/metrics.py in accuracy(inp, targ, axis) 97 def accuracy(inp, targ, axis=-1): 98 &#34;Compute accuracy with `targ` when `pred` is bs * n_classes&#34; &gt; 99 pred,targ = flatten_check(inp.argmax(dim=axis), targ) 100 return (pred == targ).float().mean() 101 /opt/conda/lib/python3.6/site-packages/fastai/torch_core.py in flatten_check(inp, targ) 799 &#34;Check that `out` and `targ` have the same number of elements and flatten them.&#34; 800 inp,targ = TensorBase(inp.contiguous()).view(-1),TensorBase(targ.contiguous()).view(-1) --&gt; 801 test_eq(len(inp), len(targ)) 802 return inp,targ /opt/conda/lib/python3.6/site-packages/fastcore/test.py in test_eq(a, b) 33 def test_eq(a,b): 34 &#34;`test` that `a==b`&#34; &gt; 35 test(a,b,equals, &#39;==&#39;) 36 37 # Cell /opt/conda/lib/python3.6/site-packages/fastcore/test.py in test(a, b, cmp, cname) 23 &#34;`assert` that `cmp(a,b)`; display inputs and `cname or cmp.__name__` if it fails&#34; 24 if cname is None: cname=cmp.__name__ &gt; 25 assert cmp(a,b),f&#34;{cname}: n{a} n{b}&#34; 26 27 # Cell AssertionError: ==: 64 640 . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; abc . NameError Traceback (most recent call last) &lt;ipython-input-9-03cfd743661f&gt; in &lt;module&gt; -&gt; 1 abc NameError: name &#39;abc&#39; is not defined . class RanzrDataset(Dataset): def __init__(self, df, path, transform=None): &quot;&quot;&quot; Args: csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. transform (callable, optional): Optional transform to be applied on a sample. &quot;&quot;&quot; self.df = df self.path = path self.transform = transform def __len__(self): return len(self.df) def __getitem__(self, idx): img_name = os.path.join(self.path,self.df.iloc[idx, 0]) + &#39;.jpg&#39; image = io.imread(img_name) if self.transform: image = self.transform(image) y = Tensor(self.df.iloc[idx, 1:-2]) batch = {&#39;x&#39;: image, &#39;y&#39;: y} return batch def show_batch(self,rows=2, cols=5): idxs = random.sample(range(self.__len__()), cols*rows) fig, ax = plt.subplots(rows, cols,figsize=(4*cols,5*rows)) for row in range(0,rows): for col in range(0,cols): num = (row * cols + col) x = self[idxs[num]][&#39;x&#39;] lbls = pd.DataFrame([train.columns[1:-3],self[idxs[num]][&#39;y&#39;].numpy().astype(int)]).transpose() y = self[idxs[num]][&#39;y&#39;] ax[row,col].imshow(x) ax[row,col].set_title(&quot; n&quot;.join(list(lbls.loc[lbls[1]==1][0].values))) ax[row,col].get_xaxis().set_visible(False) ax[row,col].get_yaxis().set_visible(False) train_dl = DataLoader(RanzrDataset(train[train.is_valid == False],(path/&#39;train&#39;)), batch_size=16, shuffle=True, num_workers=8) valid_dl = DataLoader(RanzrDataset(train[train.is_valid == True],(path/&#39;train&#39;)), batch_size=16, shuffle=True, num_workers=8) . train_dl.dataset.show_batch() . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-7-35a6145a783c&gt; in &lt;module&gt; -&gt; 1 train_dl.dataset.show_batch() &lt;ipython-input-6-b0ce5a4bd448&gt; in show_batch(self, rows, cols) 34 for col in range(0,cols): 35 num = (row * cols + col) &gt; 36 x = self[idxs[num]][&#39;x&#39;] 37 38 lbls = pd.DataFrame([train.columns[1:-3],self[idxs[num]][&#39;y&#39;].numpy().astype(int)]).transpose() &lt;ipython-input-6-b0ce5a4bd448&gt; in __getitem__(self, idx) 18 img_name = os.path.join(self.path,self.df.iloc[idx, 0]) + &#39;.jpg&#39; 19 &gt; 20 image = io.imread(img_name) 21 if self.transform: image = self.transform(image) 22 /opt/conda/lib/python3.6/site-packages/skimage/io/_io.py in imread(fname, as_gray, plugin, **plugin_args) 46 47 with file_or_url_context(fname) as fname: &gt; 48 img = call_plugin(&#39;imread&#39;, fname, plugin=plugin, **plugin_args) 49 50 if not hasattr(img, &#39;ndim&#39;): /opt/conda/lib/python3.6/site-packages/skimage/io/manage_plugins.py in call_plugin(kind, *args, **kwargs) 207 (plugin, kind)) 208 --&gt; 209 return func(*args, **kwargs) 210 211 /opt/conda/lib/python3.6/site-packages/skimage/io/_plugins/imageio_plugin.py in imread(*args, **kwargs) 8 @wraps(imageio_imread) 9 def imread(*args, **kwargs): &gt; 10 return np.asarray(imageio_imread(*args, **kwargs)) /opt/conda/lib/python3.6/site-packages/imageio/core/functions.py in imread(uri, format, **kwargs) 263 264 # Get reader and read first --&gt; 265 reader = read(uri, format, &#34;i&#34;, **kwargs) 266 with reader: 267 return reader.get_data(0) /opt/conda/lib/python3.6/site-packages/imageio/core/functions.py in get_reader(uri, format, mode, **kwargs) 170 171 # Create request object --&gt; 172 request = Request(uri, &#34;r&#34; + mode, **kwargs) 173 174 # Get format /opt/conda/lib/python3.6/site-packages/imageio/core/request.py in __init__(self, uri, mode, **kwargs) 122 123 # Parse what was given --&gt; 124 self._parse_uri(uri) 125 126 # Set extension /opt/conda/lib/python3.6/site-packages/imageio/core/request.py in _parse_uri(self, uri) 258 # Reading: check that the file exists (but is allowed a dir) 259 if not os.path.exists(fn): --&gt; 260 raise FileNotFoundError(&#34;No such file: &#39;%s&#39;&#34; % fn) 261 else: 262 # Writing: check that the directory to write to does exist FileNotFoundError: No such file: &#39;/home/github/fastblog/_notebooks/tmp/train/tmp/train/1.2.826.0.1.3680043.8.498.11428406517036900264027317719140491691.jpg.jpg&#39; . Transforms . Loss Function . class CrossEntropyLossOneHot(nn.Module): def __init__(self): super(CrossEntropyLossOneHot, self).__init__() self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, preds, labels): return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1)) . Optimizer . from torch.optim import lr_scheduler class WarmRestart(lr_scheduler.CosineAnnealingLR): def __init__(self, optimizer, T_max=10, T_mult=2, eta_min=0, last_epoch=-1): self.T_mult = T_mult super().__init__(optimizer, T_max, eta_min, last_epoch) def get_lr(self): if self.last_epoch == self.T_max: self.last_epoch = 0 self.T_max *= self.T_mult return [ self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2 for base_lr in self.base_lrs ] def warm_restart(scheduler, T_mult=2): if scheduler.last_epoch == scheduler.T_max: scheduler.last_epoch = -1 scheduler.T_max *= T_mult return scheduler . Model . model = timm.create_model(&#39;seresnext50_32x4d&#39;, pretrained=True, num_classes=10) model.fc = nn.Linear(in_features=2048, out_features=10, bias=True) x = torch.randn(1, 3, 224, 224) model(x).shape . Training . for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print(&#39;Finished Training&#39;) . NameError Traceback (most recent call last) &lt;ipython-input-1-8e82031cfffb&gt; in &lt;module&gt; 2 3 running_loss = 0.0 -&gt; 4 for i, data in enumerate(trainloader, 0): 5 # get the inputs; data is a list of [inputs, labels] 6 inputs, labels = data NameError: name &#39;trainloader&#39; is not defined . for epoch in range(2): # loop over the dataset multiple times . running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 . print(&#39;Finished Training&#39;) . &lt;/div&gt; .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/03/15/RANZCRCLIP.html",
            "relUrl": "/deep%20learning/2021/03/15/RANZCRCLIP.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastai with a Pytorch Training Loop",
            "content": "This post is inspired by Zach Mueller&#39;s blog on Pytorch to Fastai, Bridging the Gap. I read that post and thought it was a fantastic illustration of how you can switch out components of fastai with pytorch as needed so you can do low level customization anytime you want. I am writing this post to expand on Zach&#39;s work. . Often a complaint about fastai is &quot;I need access to the training loop and fastai doesn&#39;t allow this&quot;. I want to show that that statement is just not true. Zach&#39;s post demonstrated how to use pytorch components in a fastai training loop seamlessly. This post is going to demonstrate the inverse is also just as easy. Said another way, this post will demonstrate creating a fastai learner (without any pytorch) and then using that in a pure pytorch training loop. . Of course, most of the time you would use callbacks to modify the training loop. But this isn&#39;t a fastai requirement and fastai doesn&#39;t force you to do so. There may be some instances where that&#39;s not desirable. For example, I was working with CycleGAN models which has 2 generators and 2 discriminators. While you could find a way to use callbacks to make that work in the fastai training loop, I found it easier to just have a custom pytorch training loop. As with every other piece of fastai, switching the fastai bits out for pytorch is seamless so that&#39;s no problem. . from fastai.vision.all import * . Create the Fastai Learner . This will create everything needed for a model. The scheduler/optimizer/loss function isn&#39;t specified so we are using fastai&#39;s defaults. Of course, you can specify them with fastai components, or reference Zach&#39;s blog to learn how to switch any of these pieces out with pytorch. . path = untar_data(URLs.CIFAR) . dls = ImageDataLoaders.from_folder(path,valid_pct=0.2) dls.show_batch(max_n=3) . Create the fastai learner. The optimizer, loss function, scheduler, are all being created by fastai using fastai defaults. We&#39;ll use all of that fastai goodness in the pytorch training loop. . learn = cnn_learner(dls,resnet18) . Training . Great, we&#39;ve got all we need to train a model using fastai. So let&#39;s replace the fastai training loop with pytorch. This code follows extrememly closely to the training loop in the pytorch tutorial. For more details on it check there. Naturally you can add anything custom here just as you would in pytorch. . For example, learn.opt is the optimizer in the fastai learner. learn.loss_func is the loss function in the fastai learner. learn.model is the model in the fastai learner. All that was defined by fastai above. . Note: To use the fastai training loop you would do use something like learn.fine_tune or learn.fit_one_cycle. This is doing a pytorch training loop instead. . learn.model.to(&#39;cuda&#39;) for epoch in range(2): running_loss = 0.0 for i, data in enumerate(learn.dls.train, 0): inputs, labels = data learn.opt.zero_grad() outputs = learn.model(inputs) loss = learn.loss_func(outputs, labels) loss.backward() learn.opt.step() # print statistics running_loss += loss.item() if i % 100 == 99: # print every 2000 mini-batches print(&#39;[%d, %5d] loss: %.3f&#39; % (epoch + 1, i + 1, running_loss / 100)) running_loss = 0.0 . [1, 100] loss: 1.827 [1, 200] loss: 1.383 [1, 300] loss: 1.247 [1, 400] loss: 1.120 [1, 500] loss: 1.040 [1, 600] loss: 1.010 [1, 700] loss: 0.984 [2, 100] loss: 0.867 [2, 200] loss: 0.899 [2, 300] loss: 0.910 [2, 400] loss: 0.879 [2, 500] loss: 0.860 [2, 600] loss: 0.851 [2, 700] loss: 0.831 . Conclusion . So really if you need to something custom pure pytorch thing in fastai you can. It really doesn&#39;t matter if that custom thing is in the training loop, dataloaders, optimizers, loss function, or something else - it all integrates with pytorch seamlessly. . Now, for most things I reccomend getting comfortable and using the powerful fastai callback system. But in situations when you need or want pytorch - go for it! .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/03/15/FastaiPytorchTrainingLoop.html",
            "relUrl": "/deep%20learning/2021/03/15/FastaiPytorchTrainingLoop.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Stylegan Components",
            "content": "Intro . In this post I will cover several components needed for style GAN and build a basic one using those blocks. I am not going to train it, or try to make a state of the art one. Things will be simplified to the simplest form possible to understand the concept. If you want to understand some of the key techniques used in modern SoTA GANs, this is the post for you! . A future post will be actually building a real StyleGAN model to produce high def images to show practical GANs in full size networks to create actual images. Reading this post first is highly recommended! . Inspiration for this post came from the deeplearning.ai GAN specialization. For more complete information on GANs in a structured course, check that course out! . Truncated Noise . The first is an easy one to get us warmed up! This is not something used during training, but rather a technique you can use after training to control the diversity-quality trade-off when you generate images. . Generators work by taking in random noise. The random noise can be thought of a a random seed that the generators create images from. Normally we sample from the normal distribution. If you look at the normal distribution graph below you will realize that some values will be selected a lot, while others will be selected pretty rarely. If a value is in the tail, it will be selected much less frequently than a value close to the mean. So what does that mean? It means that for those particular values there will have been fewer examples to train with and will thus probably will result in lower quality images. . In order to deal with this issue we can truncate the normal distribution to sample from only the higher frequency areas. The reason this is a trade-off is because if we have fewer possible values (starting points), that mean means fewer possible images can be generated. In other words we will have less diverse outputs. . So the key things to know are: . Truncated Normal Distribution just cuts off values on each each based on some set parameter | Left graph shows normal distribution - right graphs show different levels of truncation | There is a diversity/quality trade-off that this technique allows you to make | Graphs are most diversity in output images to least diversity from left to right | Graphs are lowest quality images to highest quality images from left to right | . Noise to Weight Mapping . The next component is a noise to weight mapping. A generator gets a noise vector of random values from the normal distribution. This may be problematic. Not all of our features will follow the normal distribution - so trying to map normal distribution values to various features that follow other distributions gets messy. . This is especially problematic because we want to be able to independently control features in the output image. I don&#39;t want to modify the direction the eyes are looking and have that also change facial features. I want to be able to tweak components without having a tangled mess of mappings. . To fix this we learn the distributions that are ideal for the noise vector. So random noise comes in, it gets passed through a Mapping Network and we end with a weight matrix w. Since a neural network can approximate any function, that means it can approximate any distribution so this should work. This lets your model learn represent things in a cleaner way and makes your mapping much less tangled so you can control features much easier. . The mapping network in StyleGAN is composed of 8 layers - but here&#39;s a simplified version just to get the idea that it&#39;s just a normal neural network that is mapping the noise vector (z) to the weights vector (w). . class MappingNetwork(nn.Module): def __init__(self, z_dim, hidden_dim, w_dim): super().__init__() self.mapping = nn.Sequential( nn.Linear(z_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, w_dim)) def forward(self, noise): return self.mapping(noise) MappingNetwork(100,200,75) . MappingNetwork( (mapping): Sequential( (0): Linear(in_features=100, out_features=200, bias=True) (1): ReLU() (2): Linear(in_features=200, out_features=200, bias=True) (3): ReLU() (4): Linear(in_features=200, out_features=75, bias=True) ) ) . Noise Injection . Next, we need a process for injecting random noise in various parts of the network. This is different than the weight vector we created above. We inject this additional noise to increase diversity. The way this works: . Create 1 weight for each channel (learned) | Create a noise tensor of random numbers the same size as your image, but with only 1 channel (random) | Multiply noise tensor by each of those values so you end with something same dimension as image and add this to the image | That outputs the new image that includes the noise can continue down the network. Nothing special needs to happen other than this because we didn&#39;t change any dimensions. Really it&#39;s just a linear layer with random noise in it. You can see below that then image shape and the final shape are identical. . This happens in many places in the network before every AdaIN layer. So let&#39;s see what the AdaIN layer is. . class InjectNoise(nn.Module): def __init__(self, channels): super().__init__() self.weight = nn.Parameter(torch.randn(channels)[None, :, None, None]) def forward(self, image): noise_shape = (image.shape[0],1,image.shape[2],image.shape[3]) noise = torch.randn(noise_shape, device=image.device) out = image + self.weight * noise print(f&#39;Image (input) {image.shape}&#39;) print(f&#39;Weight (step 1): {self.weight.shape}&#39;) print(f&#39;Noise (step 2): {noise.shape}&#39;) print(f&#39;weight * noise + image (ouput): {out.shape}&#39;) return out tmp = InjectNoise(512)(torch.randn(32,512,4,4)) . Image (input) torch.Size([32, 512, 4, 4]) Weight (step 1): torch.Size([1, 512, 1, 1]) Noise (step 2): torch.Size([32, 1, 4, 4]) weight * noise + image (ouput): torch.Size([32, 512, 4, 4]) . Adaptive Instance Normalization (AdaIN) . To recap what we have so far: . An image that has random noise injected into it from the Noise Injection Step | A transformed noise matrix from our mapping network w | . We need to combine these and we need some sort of normalization. That is what this Adaptive Instance normalization is going to do. Just like the noise injection happens in many places in the network. . As previously mentioned, injecting w rather than just normally distributed noise gives us more control over the images generated. We are going to take our image after normalization, multiply it by a scale from the weight matrix and add a shift also from the weight matrix. Put another way, another linear layer. So in summary what we need to do is: . Normalize the image | Use a linear layer to map w to 1 value per channel to give us a scale tensor | Use a linear layer to map w to 1 value per channel to give us a shift tensor | output style_tensor * normalized_image + shift_tensor | Take a look below at some the code for what it does and the shapes to understand the inputs and outputs. Input and output size is the same, but with normalization and injected weight tensor! . class AdaIN(nn.Module): def __init__(self, channels, w_dim): super().__init__() self.instance_norm = nn.InstanceNorm2d(channels) self.scale_transform = nn.Linear(w_dim, channels) self.shift_transform = nn.Linear(w_dim, channels) def forward(self, image, w): normalized_image = self.instance_norm(image) scale_tensor = self.scale_transform(w)[:, :, None, None] shift_tensor = self.shift_transform(w)[:, :, None, None] transformed_image = scale_tensor * normalized_image + shift_tensor print(f&#39;Image (input) {image.shape}&#39;) print(f&#39;normalized_image (step 1) {normalized_image.shape}&#39;) print(f&#39;w (input) {w.shape}&#39;) print(f&#39;scale (step 2): {scale_tensor.shape}&#39;) print(f&#39;shift (step 3): {shift_tensor.shape}&#39;) print(f&#39;scale * norm_image + shift (ouput): {transformed_image.shape}&#39;) return transformed_image tmp = AdaIN(512,256)(torch.randn(32,512,4,4),torch.randn(32,256)) . Image (input) torch.Size([32, 512, 4, 4]) normalized_image (step 1) torch.Size([32, 512, 4, 4]) w (input) torch.Size([32, 256]) scale (step 2): torch.Size([32, 512, 1, 1]) shift (step 3): torch.Size([32, 512, 1, 1]) scale * norm_image + shift (ouput): torch.Size([32, 512, 4, 4]) . Progressive Growing . Now there&#39;s one last piece we need to understand the main components of styleGAN. Progessive growing is just what it sounds like. The generator will create a small image and progressively grow the size. It doubles the image in size until getting the image to the required size. This allows for higher quality and resolution photos. . Intuitively this makes sense. It&#39;d be much harder to generate an entire picture all at once that all meshes well together. Instead we put basic structures and build on it slowly by filling in more and more fine details over time as the image you are generating increases in size. . So let&#39;s jump into it. Let&#39;s create a re-usable block to implement this using the other components as well. Here&#39;s what we need: . An upsampling layer (for progressive growing) | A convolutional layer (standard for image problems) | Random noise injection (we created that above) | An AdaIN layer (we created that above) | An activation (just like all neural networks need) | class MinifiedStyleGANGeneratorBlock(nn.Module): def __init__(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True): super().__init__() self.use_upsample = use_upsample if self.use_upsample: self.upsample = nn.Upsample((starting_size), mode=&#39;bilinear&#39;) self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding=1) self.inject_noise = InjectNoise(out_chan) self.adain = AdaIN(out_chan, w_dim) self.activation = nn.LeakyReLU(0.2) def forward(self, x, w): if self.use_upsample: x = self.upsample(x) # upsample (step 1) x = self.conv(x) # conv layer (step 2) x = self.inject_noise(x) # noise injection (step 3) x = self.activation(x) # activation (step 4) x = self.adain(x, w) # AdaIN (step 5) return x . Now, you can implement progressive growing and put it all together. Let&#39;s see how that works in StyleGAN. As you can see we move from an 8x8 image to a 16x16 image. StyleGAn will do this many times. . Keep in mind all of this is simplified and scaled down from what is in StyleGAN. The purpose of this blog was to communicate the core concepts and techniques used in StyleGAN, not necessarily show the practical applications. Stay tuned for a blog that shows practical application of these concepts! . class MinifiedStyleGANGenerator(nn.Module): def __init__(self, z_dim, map_hidden_dim, w_dim, in_chan, out_chan, kernel_size, hidden_chan): super().__init__() self.map = MappingNetwork(z_dim, map_hidden_dim, w_dim) self.sc = nn.Parameter(torch.randn(1, in_chan, 4, 4)) self.block0 = MinifiedStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, 4) self.block1 = MinifiedStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8) self.block2 = MinifiedStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16) self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1) self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1) def upsample_to_match_size(self, smaller_image, bigger_image): return F.interpolate(smaller_image, size=bigger_image.shape[-2:], mode=&#39;bilinear&#39;) def forward(self, noise, return_intermediate=False): w = self.map(noise) # This is our mapping network going from noise -&gt; w x = self.block0(self.sc, w) # w from mapping network is input here x1 = self.block1(x, w) # w noise from mapping network is input here also image1 = self.block1_to_image(x1) print(f&#39;ImageSize1 {image1.shape}&#39;) x2 = self.block2(x1, w) # w noise from mapping network is input here also image2 = self.block2_to_image(x2) print(f&#39;ImageSize2 {image2.shape}&#39;) x1_upsample = self.upsample_to_match_size(image1, image2) return 0.2 * (image2) + 0.8 * (x1_upsample) tmp = MinifiedStyleGANGenerator(z_dim=128, map_hidden_dim=1024,w_dim=496,in_chan=512,out_chan=3, kernel_size=3, hidden_chan=256)(get_truncated_noise(10, 128, 0.7)) . ImageSize1 torch.Size([10, 3, 8, 8]) ImageSize2 torch.Size([10, 3, 16, 16]) .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/03/01/StyleGanComponents.html",
            "relUrl": "/deep%20learning/2021/03/01/StyleGanComponents.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "GAN Basics",
            "content": "Purpose . This blog is tagetted to people wanting a general intuition about GANs. This is not going to cover how to code one or build one, but will talk about the most basic high level concepts to understand. . You do not need to understand GANs prior to reading this post. I do assume that you generally are familiar with modeling. . What is a GAN and what can it do? . A GAN is a Generative Adverserial Network. They excel in creating data. Let&#39;s take a look at a few examples: . A GAN can enhance images . Google Brain did research to show how GANs can be used to enhance images. The left super blurry unrecognizable pictures were given to a GAN. The Middle column is what the GAN made when enhancing the image. The right column is what the image should look like if the GAN was perfect. . . A GAN can change image style . We can also transfer images from one style to another. Whether that&#39;s changing video of a horse to a zerbra or combining photos with art, this medium aricle shows a cool example! . . A GAN can create new images . In the paper Progressive Growing of GANs for Improved Quality, Stability, and Variation, NVIDIA showed the capability of GANs to create realistic super resolution photos of people that do not exist. These are fictional people made up by the GAN. . . A GAN can help you draw . NVIDIA again shows a really cool video of how basic sketches cna be turned into realistic photos. I can imagine how this could help people create art, visualize designs, and more! . A GAN can compose music . Another example is this song that was composed by AI. The lyrics is a person, but the instrumentation is AI - a great example of Machine-Human collaboration. You can see the GAN understood basic musical phrasing, hits, understood it can build to hits and go quiet for a couple beats before a large hit to add impact. If I didn&#39;t know, I wouldn&#39;t have realized is was using AI . How Does it Work? . This is more complex than your average Neural Network because it is relying on 2 (or more) Neural Networks training in conjunction. You have 2 models with different roles: . The Critic is the quality gauge on the Generator while the generator is what&#39;s actually producing the data. Let&#39;s look at a summary of each of those. . . How they train together . There is a big loop where they pass information back and forth an dlearn. Here&#39;s generally how it works . . First Challenges . Co-learning . As these models learn together they need to be evenly match in terms of skill. This can be especially challenging because the critic has a much easier job. Think about it. You paint a fake Monet and I will determine whether it&#39;s a real monet or a fake. Who do you think will be more competent at their task? Clearly painting the monet is the much harder job. . So what can we do about it? The siimplest 2 appraoches are: . Set how many times the generator gets updated vs the critic. | Set the learning rates different for the generator vs the critic | Mode Collapse . Mode collapse happens when the generator finds a weakness in the crtic and exploits it. For example, the generator might do really well with golden retrievers - so rather than making all types of dogs is just learns to make lots of golden retrievors. . Improvements &amp; Tweaks . What I have covered above is simple data generation in an unsupervised manner. There&#39;s several modification that can be made to let these GANs do fancier things - and Ill briefly touch on two of those here. . Conditional GANs . A conditional GAN is where you can tell it what kind of image you want. For example if you are generating different dog breeds, you tell the gan you want a specific breed (ie Golden Retriever). The way this works: . The Generator is given a specific class to generate data for. | The Critic determins whether is is real or fake data for that class. For example rather than &quot;Predict if this is a real dog or not&quot; it&#39;s &quot;Predict if this is a real golden retriever or not&quot;. In order to fool the critic, the generator now has to not just create a dog - but the right species of dog. The generator could predict a perfect image of a pitbull, but it woul dbe easy for the critic to determine that it&#39;s a not a real golden retriever as pitbulls look completely different! | Controllable GANs . A Controllable GAN allows you to control different aspects of the image. For example, I want to be able to take an image and tell it to generate the same image but add a beard. Or generate the same image but make the person look older. . A bit of background and how it&#39;s accomplished: A generator creates data from random noise vectors. These random noise vectors can be thought of as random seeds in a sense. If I give the generator the exact same vector of random numbers, it will generate the exact same data. So those random number translate to output features in the data, so you can figure out how they map and then tweak away! . Now, there&#39;s a lot of execution details and challenges to making this happen that I am not covering in this post, but feel free to reach out if you&#39;re interested and I can do a follow up post on the details on how to actually accomplish this. . Here&#39;s an example of what Photoshop is working on when it comes to controllable GANs. .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/02/20/GanIntroduction.html",
            "relUrl": "/deep%20learning/2021/02/20/GanIntroduction.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Plant Pathology",
            "content": "This post is looking at the plant pathology kaggle competition and showing how you could create a top 10 kaggle submission with fastai. This is the first of a blog series where I will do this with historical kaggle competitions. Many techniques will come from the winning solution code base by the &quot;Alipay Tian Suan Security Lab Kaggle&quot; team, but I will make a modifications as I see fit. I will explain those as they come up. . The original code was in pytorch lightning. I will be using Fastai, which will allow me to simplify the solution considerably without sacrificing results. . from fastai.vision.all import * from sklearn.model_selection import StratifiedKFold import os, cv2 from albumentations import ( Compose,GaussianBlur,HorizontalFlip,MedianBlur,MotionBlur,OneOf, RandomBrightness,RandomContrast,Resize,ShiftScaleRotate,VerticalFlip ) path = Path(&#39;./data/images&#39;) . Transforms . First, let&#39;s set up our transforms. The solution I am using as a starting point used Albumentations so I will use that as well and will use their transforms. I grabbed the AlbumentationsTransform class from the fastai docs tutorial on albumentations. . Note: As you can see, super easy to mix and match Albumentations with Fastai transforms and use other libraries with fastai even when there is not &quot;built-in&quot; integration with that library. . class AlbumentationsTransform(RandTransform): &quot;A transform handler for multiple `Albumentation` transforms&quot; split_idx,order=None,2 def __init__(self, train_aug, valid_aug): store_attr() def before_call(self, b, split_idx): self.idx = split_idx def encodes(self, img: PILImage): if self.idx == 0: aug_img = self.train_aug(image=np.array(img))[&#39;image&#39;] else: aug_img = self.valid_aug(image=np.array(img))[&#39;image&#39;] return PILImage.create(aug_img) . Now, we can define our transforms as normal. These are from the code base I am working from. I move Normalize to batch transforms and use the fastai normalize instead, which does it in a batch on the GPU in fastai. It&#39;s a small performance boost. With a dataset this small it probably just wasn&#39;t worth the time to set that up on the GPU using pytorch lightning, but it&#39;s free in fastai so may as well. . image_size = [480, 768] def get_train_aug(image_size): return Compose( [ Resize(height=image_size[0], width=image_size[1]), OneOf([RandomBrightness(limit=0.1, p=1), RandomContrast(limit=0.1, p=1)]), #fastai has OneOf([MotionBlur(blur_limit=3), MedianBlur(blur_limit=3), GaussianBlur(blur_limit=3)], p=0.5), VerticalFlip(p=0.5),#Dihedral HorizontalFlip(p=0.5), ShiftScaleRotate( shift_limit=0.2, scale_limit=0.2, rotate_limit=20, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_REFLECT_101, p=1, ), ]) def get_valid_aug(image_size): return Compose( [ Resize(height=image_size[0], width=image_size[1]), ]) . item_tfms = [AlbumentationsTransform(get_train_aug(image_size), get_valid_aug(image_size))] batch_tfms = [Normalize.from_stats(*imagenet_stats)] . Data . Next, let&#39;s load the dataframe with the training labels that was provided. The data was given to us in one hot encoded format, but we&#39;ll add an decoded column for simplicity. Since we will need to do the same processing for the test set, I put it in a function so we can use it later. . Note: I am adding the path to the image folder in the train dataframe. You can also just pass the path to your fasatai dataloaders and do the same thing that way. . def process_df(path): df = pd.read_csv(path) df[&#39;image_id&#39;] = &#39;data/images/&#39; + df.image_id + &#39;.jpg&#39; df[&#39;label&#39;] = df.apply(lambda x: &quot; &quot;.join([cat for cat in df.columns if x[cat] == 1]), axis=1) return df . train = process_df(&#39;data/train.csv&#39;) train.sample(n=5) . image_id healthy multiple_diseases rust scab label . 1390 data/images/Train_1390.jpg | 0 | 1 | 0 | 0 | multiple_diseases | . 783 data/images/Train_783.jpg | 0 | 0 | 0 | 1 | scab | . 1195 data/images/Train_1195.jpg | 1 | 0 | 0 | 0 | healthy | . 1804 data/images/Train_1804.jpg | 0 | 0 | 0 | 1 | scab | . 1622 data/images/Train_1622.jpg | 0 | 0 | 0 | 1 | scab | . Now we are ready to get out data loaded up. I will start with data cleaning. In the training set there are 2 images that are not the same as the others. This can be fixed by a transpose. . The winning solution added the transpose into their dataloader by checking image shapes as they are loaded and transpose as needed. I am going to fix it in the data upfront. This way, I don&#39;t need an if statement in the dataloader to check the image size on every image and if it&#39;s the wrong one transpose. And then repeat that every epoch. If I fix it here I only have to fix it once vs having it fixed every epoch. . for img in train.image_id: img_loaded= Image.open(img) if img_loaded.shape == (1365, 2048): continue print(img,img_loaded.shape) img_loaded.transpose(Image.TRANSPOSE).save(img) . Now, there is nothing special about the data so we can use the high level API to create a dataloaders object. We can take a look at a few images with fastai&#39;s built in visualizations. They look good - so we can feel good that we are looking at the data in the right way and that our transforms aren&#39;t doing something super weird. . dls = ImageDataLoaders.from_df(train,bs=16,seed=2020,item_tfms=item_tfms, batch_tfms=batch_tfms,label_col=5) dls.show_batch(max_n=16) . Loss Function . The codebase I am working off of uses Cross Entropy Loss on one-hot encoded values and I will as well. The one hot encoding will be important later for soft labeling. Using pure pytorch is completely compatible with fastai and we can pass this directly to our learner no problem. . class CrossEntropyLossOneHot(nn.Module): def __init__(self): super(CrossEntropyLossOneHot, self).__init__() self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, preds, labels): return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1)) . Optimizer &amp; Scheduler . In the original code base they had to write an Optimizer and Scheduler themselves in pytorch for use in pytorch lightning, but we can skip this. The approach they used is the approach fastai uses by default. It&#39;s a good example of fastai having SoTA defaults that saves time without sacrificing results. . . Model . I will use the winning solution&#39;s se_resnext50_32x4d architecture. Again, pure pytorch is just fine with fastai if you want to do custom stuff - so we don&#39;t need to modify anything from the winner&#39;s model code to make it work with fastai. I am going to go ahead and use the same architecture they used and port is straight into the fastai learner. A fastai model IS a pytorch model - so there&#39;s no problem with this. . import pretrainedmodels def l2_norm(input, axis=1): norm = torch.norm(input, 2, axis, True) output = torch.div(input, norm) return output class BinaryHead(nn.Module): def __init__(self, num_class=4, emb_size=2048, s=16.0): super(BinaryHead, self).__init__() self.s = s self.fc = nn.Sequential(nn.Linear(emb_size, num_class)) def forward(self, fea): fea = l2_norm(fea) logit = self.fc(fea) * self.s return logit class se_resnext50_32x4d(nn.Module): def __init__(self): super(se_resnext50_32x4d, self).__init__() self.model_ft = nn.Sequential( *list(pretrainedmodels.__dict__[&quot;se_resnext50_32x4d&quot;](num_classes=1000, pretrained=&quot;imagenet&quot;).children())[ :-2 ] ) self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) self.model_ft.last_linear = None self.fea_bn = nn.BatchNorm1d(2048) self.fea_bn.bias.requires_grad_(False) self.binary_head = BinaryHead(4, emb_size=2048, s=1) self.dropout = nn.Dropout(p=0.2) def forward(self, x): img_feature = self.model_ft(x) img_feature = self.avg_pool(img_feature) img_feature = img_feature.view(img_feature.size(0), -1) fea = self.fea_bn(img_feature) # fea = self.dropout(fea) output = self.binary_head(fea) return output . Soft Label Callback . Now, I write a callback to grab the one-hot encoded labels instead of the encoded column in our training loop. This won&#39;t do anything until we have our distiilled labels, but I will set the model up this way now so I don&#39;t have to modify anything later . class SoftLabelCB(Callback): def before_train(self): self.imgs_list = L(o for o in self.dl.items.iloc[:,0].values) # get list of images in the order they are drawn this epoch self.df = self.dl.items.set_index(&#39;image_id&#39;) def before_validate(self): self.imgs_list = L(o for o in self.dl.items.iloc[:,0].values) # get list of images in the order they are drawn this epoch self.df = self.dl.items.set_index(&#39;image_id&#39;) def before_batch(self): df = self.df imgs = self.imgs_list[self.dl._DataLoader__idxs[self.iter*self.dl.bs:self.iter*self.dl.bs+self.dl.bs]] one_hot_yb = df.loc[imgs,df.columns[:-1]].values self.learn.yb = (Tensor(one_hot_yb).cuda(),) . When I train I will be doing folds to generate new labels for the training set for knowledge distillation. I might as well set up inference and make predictions for each of the folds. I&#39;m also adding 4 fold TTA - it&#39;s no effort for me to do it in fastai (learn.get_preds vs learn.tta.) . Inference . def test_predict(cnt,msg): # Create Test Dataloaders test = process_df(&#39;data/sample_submission.csv&#39;) test_dl = dls.test_dl(test) # predict with test time augmentation preds, _ = learn.tta(dl=test_dl) p = preds.softmax(axis=1) # format submission file test = pd.read_csv(&#39;data/sample_submission.csv&#39;)[&#39;image_id&#39;] out_a = pd.concat([test,pd.DataFrame(p,columns = learn.dls.vocab)],axis=1)[[&#39;image_id&#39;,&#39;healthy&#39;,&#39;multiple_diseases&#39;,&#39;rust&#39;,&#39;scab&#39;]] # write to csv and submit to kaggle out_a.to_csv(f&#39;submission{cnt}.csv&#39;,index=False) os.system(f&#39;&#39;&#39;kaggle competitions submit -c plant-pathology-2020-fgvc7 -f submission{cnt}.csv -m &quot;{msg}&quot;&#39;&#39;&#39;) . Training . That&#39;s all I need for the model. Here&#39;s what&#39;s going on: . I am doing a stratified k fold instead of a normal k fold | In the learner I pass in all the stuff defined above, some of which was fastai stuff and others from pytorch. Super easy to mix and match as needed. dataloaders (dls) that was defined using the fastai api | Model that was defined using pretrainedmodels and pytorch | Loss function that is pytorch | Callbacks for gradient clipping as well as what will be soft labeling (for now it just uses the one hot encoded labels) | to_fp16 does the half precision for me | . | fine_tune takes care of the training for me. Ill go with the default freeze_epochs, learning rate, optimizer, and scheduler. | Instead of generating a normal predictions, I am doing a 4 count tta. As you can see, it&#39;s very easy to do. | . Lets pass the data (dls) the model (se_resnext50_32x4d), the loss function (CrossEntropyLossOneHot), the cbs (GradientClip and SoftLabelCB), and convert that to fp16. This will generate our distilled labels, which is just predictions on the training set. . Note: I am skipping picking a learning rate. Fastai&#8217;s default is pretty good most of the time and is plenty good to demonstrate top tier results in this instance for the blog post, but the lr_find in fastai is reccomended to find an appropriate learning rate when you are looking for the best results possible. I am also not going to do early stopping like in the winning solution. If the model overfits I would rather retrain from scratch with fewer epochs due to the nature of cyclic learning rates. That said you can add the EarlyStoppingCallback in about 2 seconds if you want to. Here&#39;s the docs for it . Getting Distilled (soft) labels . Here&#39;s the 5 fold to get predictions on our entire training set. . skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1) splits, preds, targs, preds_c, = [],[],[],[] true = pd.DataFrame(columns = L(o for o in train.columns)) pred = pd.DataFrame(columns = L(o for o in train.columns)) i=0 for _, val_idx in skf.split(train.image_id,train.label): splitter = IndexSplitter(val_idx) # Create dataloaders splittin on indexes defined by StratifiedKFold db = DataBlock(blocks=(ImageBlock,CategoryBlock), get_x=ColReader(0),get_y=ColReader(5), item_tfms=item_tfms,batch_tfms=batch_tfms, splitter=splitter) dls = db.dataloaders(train,bs=24) #train model with fastai dataloaders, pytorch model, pytorch loss function, fastai gradient clipping, custom callback, on fp16 precision learn = Learner(dls,se_resnext50_32x4d(),loss_func=CrossEntropyLossOneHot(),cbs=[GradientClip,SoftLabelCB()]).to_fp16() learn.fine_tune(80,reset_opt=True) # Train freeze epoch then unfreeze for 80 epochs p, _ = learn.tta() # test time augmentation p=p.softmax(axis=1) # Convert to probabilities # Format dataframe to save items_pred=pd.DataFrame(p,columns=dls.vocab) items_pred[&#39;label&#39;] = [dls.vocab[int(o)] for o in p.argmax(dim=1)] items_pred[&#39;image_id&#39;] = dls.valid.items.image_id.values items_pred = items_pred[train.columns] true = pd.concat([true,dls.valid.items]) pred = pd.concat([pred,items_pred]) # predict and submit to kaggle test_predict(i,f&#39;distilling labels fold count {i}&#39;) i+=1 . pred.to_csv(&#39;distilled_labels.csv&#39;,index=False) . Great! So that got us to third place with the best model already! Here&#39;s the fold scores. Our second place model as well as an average of all 5 fold predictions were both also in the top 10. We successfully got into the top 10 already with 3 submissions. It is also helpful to note that the most reasonable 2 models to pick would be the average of all 5 fold predictions as well as the 3rd place submission based on the public leaderboard (as when choosing which to use in an active competition you wouldn&#39;t be able to see the private leaderboard score). Mission accomplished! . First place in the competition had a Private Leaderboard score of 0.98445 with 10th place had 0.97883 for reference. . As you can see, fastai is plenty capable to compete on the cutting edge at the top of kaggle competitions. . . Training with Soft Labeling . I am not going to go through the rest of the implementation, but the key technique in the rest is soft labeling. Since this is not a super commonly known technique I am going to explain how to do it in fastai. . Soft labeling is taking a weighted average of a predicted label and a truth label and training your model on that. This tends to be a good thing to try when some of your labels are wrong. Generally labels that the model get wrong are more likely to be the mislabeled ones, and so when you take a weighted average those get label smoothing applies which makes your model react less to those outliers. . So how to do it within fastai? Well we already had the callback set up that allows our model to take one hot encoded labels, so all we need to do is set the weights in our dataframe and re-run what we did before. At this point the fastai implementation is done (the callback), and all I need to do is pass it the soft labeled values as the targets instead of the one hot encoded I was using before. . To demonstrate: . Load training labels and datat | . train = process_df(&#39;data/train.csv&#39;) train = train.sort_values(&#39;image_id&#39;) . Load predictions from previous k-fold | . distilled_labels = pd.read_csv(&#39;distilled_labels.csv&#39;) distilled_labels = distilled_labels.sort_values(&#39;image_id&#39;); # Get one hot encoded labels (zeros and ones) distilled_labels.iloc[:,1:-1] = pd.get_dummies(distilled_labels.label) . Weight as you see fit (winner did 7:3 ratio) | . assert (train.image_id.values==distilled_labels.image_id.values).all() distilled_labels.reset_index(drop=True,inplace=True); train.reset_index(drop=True,inplace=True); # get soft labels train.iloc[:,1:-1] = distilled_labels.iloc[:,1:-1] * .3 + train.iloc[:,1:-1] * .7 train.loc[train.healthy == 0.3][:5] . image_id healthy multiple_diseases rust scab label . 33 data/images/Train_1027.jpg | 0.3 | 0.0 | 0.0 | 0.7 | scab | . 47 data/images/Train_104.jpg | 0.3 | 0.7 | 0.0 | 0.0 | multiple_diseases | . 146 data/images/Train_1129.jpg | 0.3 | 0.7 | 0.0 | 0.0 | multiple_diseases | . 392 data/images/Train_1350.jpg | 0.3 | 0.0 | 0.0 | 0.7 | scab | . 574 data/images/Train_1514.jpg | 0.3 | 0.7 | 0.0 | 0.0 | multiple_diseases | . train.head() . image_id healthy multiple_diseases rust scab label . 0 data/images/Train_0.jpg | 0.0 | 0.0 | 0.0 | 1.0 | scab | . 1 data/images/Train_1.jpg | 0.0 | 1.0 | 0.0 | 0.0 | multiple_diseases | . 2 data/images/Train_10.jpg | 0.0 | 0.0 | 1.0 | 0.0 | rust | . 3 data/images/Train_100.jpg | 1.0 | 0.0 | 0.0 | 0.0 | healthy | . 4 data/images/Train_1000.jpg | 0.0 | 0.0 | 1.0 | 0.0 | rust | . Great - that&#39;s exactly what we want. We were already using the SoftLabelCB above - so that&#39;s it. You can rerun the same kfold code above or train a single model. Feel free to check out the repository linked above to see the exact implementation and details if you want to do a recreation of their results and the rest of their solution! . Just remember If you got the results you are looking for in a significantly easier way than you have thought - This is a good thing and a testament to the power of the library (fastai). This not an indication that you are using a beginner tool. Fastai takes care of all the normal SoTA best practices for you so you can focus on bleeding edge implementations or problem specific challenges. Sometimes that means when you go to implement something or work a problem it &quot;just works&quot; with lot less effort than you were expecting and you are left thinking &quot;That&#39;s it?&quot;. How could that possibly be a reasonable reason not to use the library or to think it&#39;s not an effective library? I remember, a few weeks after being done with @fastdotai course, implementing a new edge detection architecture in pure Pytorch. Then totally handing it off to Fastai for training and working seamlessly. I remember yelling at my computer “That’s it???”. Took me only a few days... . &mdash; Antonin SUMNER (@BBrainkite) February 9, 2021 .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/02/15/PlantPathology.html",
            "relUrl": "/deep%20learning/2021/02/15/PlantPathology.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Hierarchical Architecture",
            "content": "This post will cover an architecture modification for training models when there is a hierarchy with the data. We will be exploring this in the pets dataset and we see significant improvement in accuracy using this architecture: . The general approach will be: . Train a model to predict pet breeds | Train a model to predict pet species | Cut off the heads of both models and feed both into a shared head to predict pet breeds. | . We will be comparing this to a single model predicting pet breeds. 1 Epoch of a normal model is equivalent to 3 epochs in this new architecture and the comparisons we show represent it as such. I&#39;ll cover why when we get there. . Credit:tyoc213 came up with this idea in discord as we were chatting. It sounded interesting so I decided to try it out and it worked amazing! Lets get started! . from fastai.vision.all import * seed = 42 path = untar_data(URLs.PETS) img = (path/&#39;images&#39;).ls()[0] . Get Dataloaders . We will start with creating a model for pet breeds and a model for pet species, so we will need dataloaders for each of those models. I will not cover how this works, but if you would like more details I recommend looking at the fastai datablock tutorial on their documentation page. . We are going to be using really small images (56x56) to make this problem hard enough to experiment with. . def label_func(fname): return &quot;cat&quot; if str(fname.name[0]).isupper() else &quot;dog&quot; def get_dls(get_y): pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= get_y, item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=56) ) return pets.dataloaders(path/&quot;images&quot;,bs=64) . . dls_breed = get_dls(using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;)) dls_breed.show_batch(max_n=3) . dls_species = get_dls(label_func) dls_species.show_batch(max_n=3) . It&#39;s crucial that these dataloaders show the same images in their training and test sets. If they don&#39;t, we would likely have a data leakage issue where each model has some of the others validation set in them. This would be really problematic as we will be combining these models together at the end. . Lets add some quick tests to make sure that the valid imgs and the train images are the same. . breed_val = pd.DataFrame(dls_breed.valid.items,columns=[&#39;breed_imgs&#39;]).sort_values(&#39;breed_imgs&#39;) species_val = pd.DataFrame(dls_species.valid.items,columns=[&#39;species_imgs&#39;]).sort_values(&#39;species_imgs&#39;) assert (breed_val.breed_imgs==species_val.species_imgs).all() . breed_train = pd.DataFrame(dls_breed.train.items,columns=[&#39;breed_imgs&#39;]).sort_values(&#39;breed_imgs&#39;) species_train = pd.DataFrame(dls_species.train.items,columns=[&#39;species_imgs&#39;]).sort_values(&#39;species_imgs&#39;) assert (breed_train.breed_imgs==species_train.species_imgs).all() . Create New Architecture . Now that we have our data let&#39;s break down the steps we need to do: . Train both individual models (1 for breed and 1 for species) | Create combiner architecture that takes both of those models and combines them to predict pet breed | Train final combined model | . Train Individual Models . So now, we put them into a learner and train each of the models for 10 epochs. Nothing special yet. I will be collapsing the outputs of the training cells - we will look at the results at the end. But feel free to expand them out and look if you want. . Clearly these models will learn different things. After all one is predicting 37 classes and one is predicting only 2. . learn_breed = cnn_learner(dls_breed,resnet18,metrics=accuracy) learn_breed.fine_tune(9,reset_opt=True) . epoch train_loss valid_loss accuracy time . 0 | 4.230235 | 3.032757 | 0.211096 | 00:15 | . epoch train_loss valid_loss accuracy time . 0 | 3.340759 | 2.687151 | 0.286198 | 00:16 | . 1 | 2.841087 | 2.352612 | 0.353857 | 00:16 | . 2 | 2.367480 | 2.068192 | 0.412043 | 00:16 | . 3 | 2.069410 | 1.894270 | 0.468200 | 00:15 | . 4 | 1.765054 | 1.723395 | 0.500677 | 00:16 | . 5 | 1.548497 | 1.665506 | 0.522327 | 00:15 | . 6 | 1.398346 | 1.618048 | 0.529093 | 00:16 | . 7 | 1.295849 | 1.587031 | 0.530447 | 00:15 | . 8 | 1.202776 | 1.583355 | 0.535859 | 00:16 | . learn_species = cnn_learner(dls_species,resnet18,metrics=accuracy) learn_species.fine_tune(9,reset_opt=True) . epoch train_loss valid_loss accuracy time . 0 | 0.841909 | 0.564811 | 0.734777 | 00:14 | . epoch train_loss valid_loss accuracy time . 0 | 0.582848 | 0.424087 | 0.801759 | 00:16 | . 1 | 0.443838 | 0.372058 | 0.835589 | 00:15 | . 2 | 0.374449 | 0.319897 | 0.864005 | 00:16 | . 3 | 0.311494 | 0.264328 | 0.888363 | 00:16 | . 4 | 0.256848 | 0.261497 | 0.890392 | 00:16 | . 5 | 0.201610 | 0.237422 | 0.907307 | 00:16 | . 6 | 0.175206 | 0.237565 | 0.907307 | 00:16 | . 7 | 0.168828 | 0.237170 | 0.903248 | 00:16 | . 8 | 0.150706 | 0.234563 | 0.904601 | 00:16 | . Combiner Architecture . In this section there&#39;s a few things to do: . &quot;Cut&quot; each of the individual models above to eliminate the final layer group | Create a model head that takes the features from the (new) last layer of the cut models and combines them | Split the new model into layers so we can tell our model the appropriate layers to freeze for transfer learning. . Note: If any of this doesn&#8217;t make sense to you, I recommend reading Chapter 15 of the Deep Learning with Fastai and Pytorch book. That chapter has all the concepts needed to understand what&#8217;s going on here. | . class PetsModel(Module): def __init__(self, encoder_species,encoder_breed,head): self.body_species = encoder_species[:-1] # cut species model self.body_breed = encoder_breed[:-1] # cut species model self.head = head def forward(self, x): # concatenate the outputs of the cut species and cut breed models together ftrs = torch.cat([self.body_species(x), self.body_breed(x)], dim=1) # Feed the concatenaded outputs to the model head return self.head(ftrs) . Great! So we now have the general architecture. It cuts the models, each model predicts on the same image, it gets concatenated together and then passed through the model head. Let&#39;s initialize out actual model! We can see we need 3 things: . encoder_species: We can grab the model for our species model about by grabbing learn_species.model | encoder_breed: learn_breed.model gives us the model we trained above for breeds | head: We will use the create_head function in fastai to generate a default head. The arguments are pretty straightforward We need 512*4 input features - This is found by looking at the outputs of the cut encoders | For outputs we need 1 output per class we are trying to predict | . | . m = PetsModel(learn_species.model,learn_breed.model,create_head(512*4,len(learn_breed.dls.vocab))) . Now we define the layer splits. For simplicity let&#39;s have 3 layers. 1 for each of the base models, and 1 for the head. . def layer_splitter(model): return [params(model.body_species), params(model.body_breed), params(model.head)] . Now we can pass all this to a Learner and fine tune the last layer. freeze freezes the last model group, which we defined above as the model head. This is perfect, because the other models have already been trained. . learn = Learner(dls_breed,m,metrics=accuracy,splitter=layer_splitter,loss_fun=CrossEntropyLossFlat()) learn.freeze() learn.fit_one_cycle(1,reset_opt=True) . epoch train_loss valid_loss accuracy time . 0 | 1.829848 | 1.660312 | 0.497970 | 00:18 | . Finally, we unfreeze all the parameters and train the whole model for 10 epochs. . learn.unfreeze() learn.fit_one_cycle(9) . epoch train_loss valid_loss accuracy time . 0 | 1.464251 | 1.707697 | 0.504060 | 00:20 | . 1 | 1.869895 | 2.174934 | 0.389039 | 00:19 | . 2 | 1.844630 | 2.140581 | 0.403248 | 00:20 | . 3 | 1.697882 | 1.810330 | 0.458051 | 00:19 | . 4 | 1.453276 | 1.585714 | 0.525710 | 00:20 | . 5 | 1.198111 | 1.477474 | 0.576455 | 00:21 | . 6 | 0.954434 | 1.234098 | 0.624493 | 00:19 | . 7 | 0.718380 | 1.102403 | 0.673884 | 00:19 | . 8 | 0.606312 | 1.091611 | 0.673884 | 00:20 | . Baseline . We need to compare this to something so we know if it is actually helpful. We will train 30 epochs so it&#39;s comparable. After this, we will be ready to compare results! . Note: The more complex model trained for about 30 epochs which is why we are doing that here. 10 epochs went to the species model, 10 epochs on the breed model, then 10 epochs for the final model. . learn_breed_only = cnn_learner(dls_breed,resnet18,metrics=accuracy,loss_fun=CrossEntropyLossFlat()) learn_breed_only.fine_tune(29,reset_opt=True) . epoch train_loss valid_loss accuracy time . 0 | 4.250142 | 3.012516 | 0.232070 | 00:15 | . epoch train_loss valid_loss accuracy time . 0 | 3.287548 | 2.739659 | 0.274696 | 00:16 | . 1 | 2.980861 | 2.542027 | 0.314614 | 00:16 | . 2 | 2.698554 | 2.325055 | 0.355886 | 00:15 | . 3 | 2.419496 | 2.152232 | 0.399865 | 00:15 | . 4 | 2.137550 | 1.984688 | 0.433694 | 00:15 | . 5 | 1.932442 | 1.844241 | 0.471583 | 00:15 | . 6 | 1.733463 | 1.774089 | 0.493234 | 00:15 | . 7 | 1.595882 | 1.635988 | 0.533830 | 00:15 | . 8 | 1.424889 | 1.598016 | 0.536536 | 00:16 | . 9 | 1.304143 | 1.622368 | 0.531123 | 00:17 | . 10 | 1.133940 | 1.571498 | 0.548038 | 00:16 | . 11 | 1.036994 | 1.559505 | 0.552097 | 00:16 | . 12 | 0.963816 | 1.584645 | 0.565629 | 00:16 | . 13 | 0.883272 | 1.571560 | 0.569012 | 00:16 | . 14 | 0.788957 | 1.623814 | 0.562246 | 00:16 | . 15 | 0.700083 | 1.599169 | 0.562246 | 00:16 | . 16 | 0.647424 | 1.614497 | 0.586604 | 00:17 | . 17 | 0.577235 | 1.629573 | 0.573072 | 00:16 | . 18 | 0.519236 | 1.626069 | 0.587280 | 00:16 | . 19 | 0.458696 | 1.630924 | 0.588633 | 00:16 | . 20 | 0.418169 | 1.636246 | 0.587280 | 00:16 | . 21 | 0.385525 | 1.662767 | 0.586604 | 00:16 | . 22 | 0.374188 | 1.631083 | 0.598782 | 00:16 | . 23 | 0.350734 | 1.631033 | 0.600135 | 00:16 | . 24 | 0.333111 | 1.618445 | 0.601488 | 00:16 | . 25 | 0.316757 | 1.648176 | 0.600812 | 00:16 | . 26 | 0.302132 | 1.629723 | 0.605548 | 00:16 | . 27 | 0.294292 | 1.622839 | 0.602165 | 00:16 | . 28 | 0.302467 | 1.626161 | 0.602165 | 00:16 | . Results . Now, we graph the values from our models. We can get all the values we need in a dataframe using the learn.recorder object so that we can plot them easy. The code isn&#39;t all that interesting, but feel free to look if you would like. . The really interesting thing is the graph. We see great trends all around: . Training loss decreasing | Validation loss decreases faster and more than baseline | Accuracy is 7 percentage points higher than baseline . Note: The combined model architecture line looks choppier than it is, because there&#8217;s 1/3 the points. If you remember 1 epoch of the combined model is roughly equivalent to 3 epochs of the baseline model. | . df = pd.DataFrame(learn.recorder.values) # get results for new architecture df.columns = learn.recorder.metric_names[1:-1] df = pd.concat([df,df,df]) # duplicate 3x so that each epoch graphs as 3 epochs df.sort_index(inplace=True) df.reset_index(inplace=True,drop=True) df_b = pd.DataFrame(learn_breed_only.recorder.values) # get results for baseline model df_b.columns = learn_breed_only.recorder.metric_names[1:-1] . . import matplotlib.pyplot as plt fig, ax = plt.subplots(1,3,figsize=(16,4)) for i in range(0,len(ax)): col = df.columns[i] ax[i].plot(df[col], label=&quot;Custom Architecture&quot;) col_b = df_b.columns[i] ax[i].plot(df_b[col_b], label=&quot;Normal Resnet18&quot;) ax[i].legend() ax[i].set_xlabel(&#39;epoch&#39;) ax[0].set_title(&#39;Train Loss&#39;); ax[1].set_title(&#39;Valid Loss&#39;); ax[2].set_title(&#39;Breed Accuracy&#39;) ax[0].set_ylabel(&#39;Loss&#39;);ax[1].set_ylabel(&#39;Loss&#39;);ax[2].set_ylabel(&#39;Accuracy&#39;) . . Text(0, 0.5, &#39;Accuracy&#39;) .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/02/01/Heirarchical-Architecture.html",
            "relUrl": "/deep%20learning/2021/02/01/Heirarchical-Architecture.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Custom Architecture Research",
            "content": "This post will cover an architecture modification for training models to give several model bodys feeding into a single custom head. We will be exploring this in the pets dataset and we see there are some improvements to be had over resnets in this instance. The goal will be to predict pet breed. . Credit:David Berger read a previous blog post and had some great ideas for better baselines and things to try. This post and several of the experiments were largely inspired by our conversation on the fastai discord. Lets get started! . from fastai.vision.all import * import pickle seed = 42 path = untar_data(URLs.PETS) img = (path/&#39;images&#39;).ls()[0] . The Data . We will start with creating a model for pet breeds and a model for pet species, so we will need dataloaders for each of those models. I will not cover how this works, but if you would like more details I recommend looking at the fastai datablock tutorial on their documentation page. . We are going to be using really small images (64x64) to make this problem hard enough and fast enough to experiment with. . def label_func(fname): return &quot;cat&quot; if str(fname.name[0]).isupper() else &quot;dog&quot; def get_dls(get_y): pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= get_y, item_tfms=Resize(128), batch_tfms=aug_transforms(min_scale = 0.9,size=64) ) return pets.dataloaders(path/&quot;images&quot;,bs=64) . . dls_breed = get_dls(using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;)) dls_breed.show_batch(max_n=3) . dls_species = get_dls(label_func) dls_species.show_batch(max_n=3) . It&#39;s crucial that these dataloaders show the same images in their training and test sets. If they don&#39;t, we would likely have a data leakage issue where each model has some of the others validation set in them. This would be really problematic as we will be combining these models together at the end. . Lets add some quick tests to make sure that the valid imgs and the train images are the same. . breed_val = pd.DataFrame(dls_breed.valid.items,columns=[&#39;breed_imgs&#39;]).sort_values(&#39;breed_imgs&#39;) species_val = pd.DataFrame(dls_species.valid.items,columns=[&#39;species_imgs&#39;]).sort_values(&#39;species_imgs&#39;) assert (breed_val.breed_imgs==species_val.species_imgs).all() . breed_train = pd.DataFrame(dls_breed.train.items,columns=[&#39;breed_imgs&#39;]).sort_values(&#39;breed_imgs&#39;) species_train = pd.DataFrame(dls_species.train.items,columns=[&#39;species_imgs&#39;]).sort_values(&#39;species_imgs&#39;) assert (breed_train.breed_imgs==species_train.species_imgs).all() . The Architecture . Now that we have our data let&#39;s talk about what we are going to be testing. . Put simply: We are creating resnet architectures with more than 1 body. We are going to do several variations so we want a flexible wayt to create this. Let&#39;s look at our model class. . I will explain what is going on in it below, as this is central to the whole expirament. . body_list: Body list takes a list of encoders (encoder_list) and cuts the heads off. So now we have a list of bodys using whatever encoders and wieghts were passed, and can do an arbitrary number of bodys using the same class. | self.head: This cretes a head that can take all the bodies and combine them. You will wee that the inputs to the head is dependent on the number of resnet bodies we have. | self.split: This just breaks out each body + head into their own parameter groups. This is important so it is easy to freeze the head that is random weights to train that first. | forward: In the forward you see how it is used. We start by passing the x (image) into each of the bodys and contatenating it together. That then is passed to the head. Exactly what we want! . Note: If any of this doesn&#8217;t make sense to you, I recommend reading Chapter 15 of the Deep Learning with Fastai and Pytorch book. That chapter has all the concepts needed to understand what&#8217;s going on here. | . class PetsModel(Module): def __init__(self, encoder_list,vocab): self.body_list = [encoder[:-1] for encoder in encoder_list] self.head = create_head(512*len(self.body_list)*2,len(vocab)) self.split = [params(body) for body in self.body_list] + [params(self.head)] def layer_splitter(self,model): return self.split def forward(self, x): # concatenate the outputs of the cut species and cut breed models together ftrs = torch.cat([body(x) for body in self.body_list], dim=1) # Feed the concatenaded outputs to the model head return self.head(ftrs) . The Experiment . Now that we understand this custom architecture, let&#39;s go over what all the variations we want to compare. . resnet18, resnet34, resnet50: If this custom architecture isn&#39;t better than one of these in some way (speed to train, validation accuracy, etc), then we may as well just use a resnet. So we need to include those for comparison. | 2UntrainedBodies, 3UntrainedBodies: Uses 2 or 3 bodies respectively | Bodys are each resnet18 | Uses pretrained weights with no seperate training | . | 2TrainedBodies, 3TrainedBodies: Uses 2 or 3 bodies respectively | Bodys are each resnet18 | Takes pretrained resnet18&#39;s and seperately trains them on the task first | . | 2TrainedDupeBodies,3TrainedDupeBodies: Uses 2 or 3 bodies respectively | Bodys are each resnet18 | Takes pretrained resnet18, train it first, then have bodys duplicates of the trained model | . | 2HeirarchyBodies: Uses 2 bodys | Bodys are each resnet18 | Body1 is a resnet18 trained to predict species (higher level in heirarchy) | Body2 is a resnet18 trained to predict breed (lower level in heirarchy) | . | The Results . I will be graphing some key findings to look at, and only graphing the best models. . Let&#39;s start with with a review of the 3 best models: . 2UnTrainedBodies: Had 2 resnet18 bodies, both using pytorch pretrained model weights. | 3UnTrainedBodies: Had 3 resnet18 bodies, each using pytorch pretrained model weights. | 2TrainedDupeBodies: Fine tuned 1 resnet18, then used the weights from that model for both bodies | . Let&#39;s start by looking at validation accuracy. Key notes: . This looks at the best 3 models only | We can see that our best model models are between a resnet 34 and resnet 50 in terms of accuracy consistently in the last 20 epochs | Based on these results, there isn&#39;t really any value in training the bodies seperately before combining them. It&#39;s more beneficial to train the system as a whole immediately. | . Now the graph above does a great job of showing the consistency in the final epochs, but it does hide some of the early training instability. Here&#39;s a fuller view . Great! So this seems really promising. But the real question now is why use this approach over a resnet50? To do that let&#39;s look at the time to process (below). . Below we see a few things: . The resnet50 (which was the most accurate) takes the longest to train | The resnet34 takes slightly more time to train as the 2TrainedDupeBodies and the 2UntrainedBodies model, even though the resnet34 is less accurate. | The 3UntrainedBodies model is not quite as accurate as the resnet50 but also doesn&#39;t take as long to train. . Note: This is really important because 2 of the custom architectures have a better accuracy and trains in the slightly less time than a resnet34. That&#8217;s great news! We will look at a zoomed in view so the differentiations are clearer. Please be aware the Y axis does not start at 0. | .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/02/01/CustomArchitectureResearch.html",
            "relUrl": "/deep%20learning/2021/02/01/CustomArchitectureResearch.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Soft Labeling",
            "content": "This post will walk through how to do use soft labeling in fastai, and demonstrate how it helps with noisy labels to improve training and your metrics. . This post was inspired by a 1st place kaggle submission (not mine), so we know it&#39;s a good idea! The repo for that is here which is done in pytorch lightning. This post will use fastai. . Let&#39;s get started! . Imports . from fastai.vision.all import * path = untar_data(URLs.IMAGEWOOF) from sklearn.model_selection import StratifiedKFold from numpy.random import default_rng . Get Noisy Data . I am using the noisy datasets repo that was hugely inspired by the noisy imagenette repository to get noisy labels for the imagewoof dataset. . First we get the noisy imagewoof csv, then use that to build the dataloaders. . #this code is taken from the noisy imagenette github repo linked above with slight modifications def get_dls(size, woof, pct_noise, bs, splitter=ColSplitter()): path = untar_data(URLs.IMAGEWOOF) df = pd.read_csv(&#39;https://raw.githubusercontent.com/Isaac-Flath/noisy_datasets/main/noisy_imagewoof.csv&#39;) df = df.loc[df.is_valid==False] batch_tfms = [Normalize.from_stats(*imagenet_stats)] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=splitter, get_x=ColReader(&#39;path&#39;, pref=path), get_y=ColReader(f&#39;noisy_labels_{pct_noise}&#39;), item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)], batch_tfms=batch_tfms) return dblock.dataloaders(df, bs=bs) . . dls = get_dls(224,woof=True,pct_noise=5,bs=16) dls.show_batch() . . Create Crossfold, Train, and Predict . The reason I am doing cross folds is to get predicted labels on the training set. The predicted labels on the training set are using labels each model was not trained on. . Note: I am doing this with a 2 fold, but you may want to use a 5-fold or more folds. This cross-fold code was mostly supplied by Zach Mueller, with minor modifications by me for this dataset and tutorial. There is also a tutorial he wrote with more details here . df = pd.read_csv(&#39;https://raw.githubusercontent.com/Isaac-Flath/noisy_datasets/main/noisy_imagewoof.csv&#39;) train_df = df.loc[df.is_valid==False] df.head(3) . path truth noisy_labels_1 noisy_labels_5 noisy_labels_25 noisy_labels_50 is_valid . 0 train/n02111889/n02111889_5826.JPEG | n02111889 | n02111889 | n02111889 | n02111889 | n02111889 | False | . 1 train/n02111889/n02111889_1944.JPEG | n02111889 | n02111889 | n02111889 | n02111889 | n02086240 | False | . 2 train/n02111889/n02111889_17657.JPEG | n02111889 | n02111889 | n02111889 | n02111889 | n02111889 | False | . skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1) splits, preds, targs, preds_c, = [],[],[],[] items = pd.DataFrame(columns = [&#39;path&#39;, &#39;noisy_labels_1&#39;, &#39;noisy_labels_5&#39;, &#39;noisy_labels_25&#39;,&#39;noisy_labels_50&#39;, &#39;is_valid&#39;]) for _, val_idx in skf.split(train_df.path,train_df.noisy_labels_5): splitter = IndexSplitter(val_idx) splits.append(val_idx) dls = get_dls(224,woof=True,pct_noise=5,bs=16,splitter=splitter) learn = cnn_learner(dls,resnet18,metrics=[accuracy,RocAuc()]) learn.fine_tune(10,reset_opt=True) # store predictions p, t, c = learn.get_preds(ds_idx=1,with_decoded=True) preds.append(p); targs.append(t); preds_c.append(c); items = pd.concat([items,dls.valid.items]) . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.036113 | 0.780265 | 0.841569 | 0.963737 | 00:49 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 0.820277 | 0.695468 | 0.868823 | 0.967281 | 00:58 | . 1 | 0.912045 | 0.729069 | 0.846444 | 0.965334 | 01:01 | . 2 | 0.728870 | 0.716678 | 0.848659 | 0.964523 | 00:59 | . 3 | 0.640925 | 0.717469 | 0.848659 | 0.964562 | 00:59 | . 4 | 0.620054 | 0.712924 | 0.839575 | 0.963071 | 01:00 | . 5 | 0.502611 | 0.703821 | 0.850210 | 0.964302 | 00:59 | . 6 | 0.352605 | 0.727077 | 0.858631 | 0.965233 | 00:59 | . 7 | 0.304586 | 0.729460 | 0.864392 | 0.964620 | 00:59 | . 8 | 0.249052 | 0.723166 | 0.858631 | 0.965410 | 00:56 | . 9 | 0.180612 | 0.732588 | 0.859739 | 0.965431 | 00:52 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.166885 | 0.712075 | 0.841090 | 0.969210 | 00:32 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 0.853930 | 0.630505 | 0.868351 | 0.971000 | 00:42 | . 1 | 0.841145 | 0.655725 | 0.860372 | 0.968649 | 00:42 | . 2 | 0.834104 | 0.692984 | 0.839539 | 0.967488 | 00:42 | . 3 | 0.658782 | 0.686378 | 0.854167 | 0.968132 | 00:42 | . 4 | 0.606825 | 0.703417 | 0.846853 | 0.966847 | 00:42 | . 5 | 0.503965 | 0.687867 | 0.843528 | 0.966903 | 00:42 | . 6 | 0.409177 | 0.686660 | 0.857713 | 0.967467 | 00:41 | . 7 | 0.340657 | 0.690113 | 0.858821 | 0.967676 | 00:42 | . 8 | 0.237057 | 0.685794 | 0.866356 | 0.967809 | 00:42 | . 9 | 0.222322 | 0.681753 | 0.865470 | 0.968221 | 00:42 | . Look at Predictions . Lets throw it all in a dataframe so we can look at what we have a little easier. First, let&#39;s break out our different pieces of information. . imgs = L(o for o in items.path.values) y_true = L(o for o in items.noisy_labels_5.values) # Labels from dataset y_targ = L(dls.vocab[o] for o in torch.cat(targs)) # Labels from out predictions y_pred = L(dls.vocab[o] for o in torch.cat(preds_c)) # predicted labels or &quot;pseudo labels&quot; p_max = torch.cat(preds).max(dim=1)[0] # max model score for row . We can double check we are matching things up correctly by checking that the labels line up from the predictions and the original data. Throwing some simple assert statements in is nice because it takes no time and it will let you know if you screw something up later as you are tinkering with things. . assert (y_true == y_targ) # test we matched these up correct . Put it in a dataframe and see what we have. . res = pd.DataFrame({&#39;imgs&#39;:imgs,&#39;y_true&#39;:y_true,&#39;y_pred&#39;:y_pred}).set_index(&#39;imgs&#39;) print(res.shape) print(df.shape) res.sample(5) . (9025, 2) (12954, 7) . y_true y_pred . imgs . train/n02086240/n02086240_6323.JPEG n02086240 | n02086240 | . train/n02093754/n02093754_696.JPEG n02093754 | n02093754 | . train/n02089973/n02089973_12157.JPEG n02089973 | n02089973 | . train/n02096294/n02096294_4188.JPEG n02096294 | n02096294 | . train/n02086240/n02086240_6595.JPEG n02086240 | n02115641 | . Soft Labeling Setup . Now, we have all the data we need to train a model with soft labels. To recap we have: . Dataloaders with noisy labels | Dataframe with img path, y_true, and y_pred (pseudo labels we generated in the cross-fold above) | Now, we will need to convert things to one-hot encoding, so let&#39;s do that for our dataframe . res = pd.get_dummies(res,columns=[&#39;y_true&#39;,&#39;y_pred&#39;]) . Now, lets change the Loss Function and Metric to support one hot encoded targets . class CrossEntropyLossOneHot(nn.Module): def __init__(self): super(CrossEntropyLossOneHot, self).__init__() self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, preds, labels): return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1)) def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred,targ = flatten_check(inp.argmax(dim=axis), targ.argmax(dim=axis)) return (pred == targ).float().mean() . Soft Labeling CallBack . Finally, lets write the callback that does the Soft Labeling. . There&#39;s a few components to this. To put in english what is happening below in each section: . before_train and before_validate: This is grabbing the list of images for the entire dataloader. We don&#39;t need to do this every batch, so it fits well here; | before_batch: This filters the list of images that was defined down to only the images in our batch. From there, it one hot encodes the y variable, and if it&#39;s a training batch it does y_true 0.7 + y_pred 0.3. We don&#39;t want to smooth the validation set as we want a good representation of what the metrics would be on a separate test set. This is the core of soft labeling. | . The intuition for this is that the labels that the model in the crossfold got wrong above have a higher chance of just being incorrect labels. So we smooth those out to punish incorrect classifications less. . Note: You can set thresholds for soft labeling to smooth more or less based on the confidence your predicted labels have. I don&#8217;t have that built into this callback, but it is something you can experiment with! This Callback a collaboration: . Zach Mueller got me started with the callback system in fastai, particularly around dataloader batch indexing in fastai | Kevin H. and I collaborated on this. We were working on this for a joint project, and we were both running experiments to get it to work right and perform. | . class SoftLabelCB(Callback): def __init__(self, df_preds,y_true_weight = 0.7): &#39;&#39;&#39;df_preds is a pandas dataframe where index is image paths Must have y_true and y_pred one hot encoded columns (ie y_true_0, y_true_1) &#39;&#39;&#39; self.y_true_weight = y_true_weight self.y_pred_weight = 1 - y_true_weight self.df = df_preds def before_train(self): if type(self.dl.items)==type(pd.DataFrame()): self.imgs_list = L(o for o in self.dl.items.iloc[:,0].values) if is_listy(self.dl.items): self.imgs_list = L(self.dl.items) def before_validate(self): if type(self.dl.items)==type(pd.DataFrame()): self.imgs_list = L(o for o in self.dl.items.iloc[:,0].values) if is_listy(self.dl.items): self.imgs_list = L(self.dl.items) def before_batch(self): # get the images&#39; names for the current batch imgs = self.imgs_list[self.dl._DataLoader__idxs[self.iter*self.dl.bs:self.iter*self.dl.bs+self.dl.bs]] # get soft labels df = self.df soft_labels = df.loc[imgs,df.columns.str.startswith(&#39;y_true&#39;)].values if self.training: soft_labels = soft_labels*self.y_true_weight + df.loc[imgs,df.columns.str.startswith(&#39;y_pred&#39;)].values*self.y_pred_weight self.learn.yb = (Tensor(soft_labels).cuda(),) . Train the Model and Results . Then we put the callback and our one hot metric and loss function into a learning and fine tune it. As you can see, we get a small bump in both accuracy and roc_auc score. . This is training on the same data that the last of the crossfolds was, so it&#39;s a good comparison. . without soft labeling: max accuracy was 86.8%, which was hit very early on and then did not see improvements for 8 more epochs. | With soft labeling: max accuracy was 88%, over 1% higher than without soft labeling. In addition, the last 4 epochs showed epoch over epoch improvements to the metric and loss with the last epoch being the highest accuracy. We can almost certainly train longer to see even better benefits. | . learn = cnn_learner(dls,resnet18,metrics=[accuracy,RocAuc()],loss_func=CrossEntropyLossOneHot(),cbs=SoftLabelCB(res)) . learn.fine_tune(10,reset_opt=True) . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 1.045338 | 0.761228 | 0.843528 | 0.968725 | 00:33 | . epoch train_loss valid_loss accuracy roc_auc_score time . 0 | 0.781899 | 0.655288 | 0.869016 | 0.970987 | 00:42 | . 1 | 0.777672 | 0.666892 | 0.860594 | 0.970698 | 00:43 | . 2 | 0.734413 | 0.635256 | 0.852837 | 0.969718 | 00:43 | . 3 | 0.611115 | 0.629129 | 0.860816 | 0.969812 | 00:43 | . 4 | 0.575180 | 0.618686 | 0.863475 | 0.970406 | 00:43 | . 5 | 0.464586 | 0.602176 | 0.871897 | 0.969895 | 00:43 | . 6 | 0.433750 | 0.608785 | 0.867021 | 0.971009 | 00:43 | . 7 | 0.414037 | 0.597265 | 0.873005 | 0.970817 | 00:45 | . 8 | 0.344089 | 0.597751 | 0.875665 | 0.971109 | 00:45 | . 9 | 0.314052 | 0.582850 | 0.880541 | 0.971416 | 00:45 | .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2021/01/18/SoftLabeling.html",
            "relUrl": "/deep%20learning/2021/01/18/SoftLabeling.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Neural Networks and XOR",
            "content": "This article is not an applied post like I usually write, but is more diving into why Neural Networks are so powerful. The goal is to show an example of a problem that a Neural Network can solve easily that stricly linear models cannot solve. We will do this in the simplest example, the XOR. . I will cover what an XOR is in this article, so there aren&#39;t any prerequisites for reading this article. Though if this is your first time hearing of the XOR, you may not understand the implications or the importance of solving the XOR. . Credit . I got the idea to write a post on this from reading the deep learning book. The information covered in this post is also in that book, though the book covers more information and goes into more detail. The primary benefit of this post over the book is that this post is in &#39;python pytorch&#39; notation where the book covers this material in &#39;math&#39; notation. If this post is interesting to you, I would recommend checking out the book. . The book can be found with this information: . Title: Deep Learning | Author: Ian Goodfellow and Yoshua Bengio and Aaron Courville | Publisher: MIT Press | URL: http://www.deeplearningbook.org, | Year: 2016 | . The Problem . In the graph below we see the XOR operator outputs. XOR is similar to OR. If either one of the bits is positive, then the result is positive. The difference is that if both are positive, then the result is negative. . We can see the 2 classes as red and blue dots on the visualization. Try to draw a single line that divides the red dots from the blue dots and you will find that it cannot be done. A linear model simply cannot classify these points accurately . from torch import Tensor import torch import pandas as pd import matplotlib.pyplot as plt . . x = Tensor([[0,0],[0,1],[1,0],[1,1]]) y = [0,1,1,0] out = pd.DataFrame(x,columns = [&#39;x1&#39;,&#39;x2&#39;]); out[&#39;XOR&#39;] = y class1 = out[out.XOR==0].iloc[:,:2]; class2 = out[out.XOR==1].iloc[:,:2] fig, ax = plt.subplots(figsize=(4,4)) ax.scatter(class1.x1,class1.x2,c=&#39;red&#39;); ax.scatter(class2.x1,class2.x2,c=&#39;blue&#39;) ax.set_xlabel(&#39;Input1&#39;); ax.set_ylabel(&#39;Input2&#39;) ax.set_title(&#39;Input Data&#39;) plt.show() . . Neural Network Feature Space . Now that we see a linear model cannot solve the problem, or said another way it&#39;s not linearly seperable, let&#39;s see how a Neural Network would help. . We start by defining the tensors that we need: . x: This shows all the points and are the inputs we are using to predict with. You can verify the points on the graph above. | w: This is the weight matrix. A linear layer is xw + b. | b: This is the bias. A linear layer is xw + b | y: This is the dependent variable we are trying to predict (whether the dot is blue or red, or XOR operator output) . Note: The text and some libraries do transposes to have wx + b, but it&#8217;s the same thing. | . x = Tensor([[0,0],[0,1],[1,0],[1,1]]) y = [0,1,1,0] w = torch.ones(2,2); b = Tensor([0,-1]) . Now we do out linear layer with activation function and store that in h. . x@w + b : This is the linear function | torch.clip : This is replacing any negative values with 0. The fancy term for this is a ReLU or Rectified Linear Unit | . h = torch.clip(x@w + b,0) . Instead of plotting our inputs like we did above (when we saw this problem couldn&#39;t be solved linearly), let&#39;s plot the outputs of layer we just calculated. . As you can see when we plot the output of the first layer it&#39;s trivial to separate the blue and red points with a line. We have created a represenation of the data that makes it very easy to classify the points to solve the XOR problem! . out = pd.DataFrame(h,columns = [&#39;x1&#39;,&#39;x2&#39;]); out[&#39;XOR&#39;] = y class1 = out[out.XOR==0].iloc[:,:2]; class2 = out[out.XOR==1].iloc[:,:2] fig, ax = plt.subplots(figsize=(4,4)) ax.scatter(class1.x1,class1.x2,c=&#39;red&#39;); ax.scatter(class2.x1,class2.x2,c=&#39;blue&#39;) ax.set_xlabel(&#39;Feature1&#39;); ax.set_ylabel(&#39;Feature2&#39;) ax.set_title(&#39;Hidden Features&#39;) plt.show() . . Model Predictions . Now that it&#39;s linearly seperable, we can easily add an output layer to form out predictions. All we do for this is multiply my another Tensor so that we get the correct number of outputs. In this case we have 4 points we want to classify, so we have 4 outputs (1 per point). . We see that the model was able to solve the XOR problem . h = torch.clip(x@w + b,0) @ Tensor([1,-2]) res = pd.DataFrame(x.int().numpy(),columns=[&#39;x1&#39;,&#39;x2&#39;]); res[&#39;preds&#39;]=h.int(); res[&#39;targets&#39;]=y res . x1 x2 preds targets . 0 0 | 0 | 0 | 0 | . 1 0 | 1 | 1 | 1 | . 2 1 | 0 | 1 | 1 | . 3 1 | 1 | 0 | 0 | .",
            "url": "https://isaac-flath.github.io/fastblog/2021/01/09/NeuralNetworksXOR.html",
            "relUrl": "/2021/01/09/NeuralNetworksXOR.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Pseudo Labeling",
            "content": "Intro . Goal: The goal of this article is to provide an understanding of what pseudo labeling is, why you might use it, and how you would go about using it. . What&#39;s Included in this post: The information needed to get started on pseudo labeling on your project. . What is pseudo labeling . Pseudo Labeling is the process of creating new labels for a piece of data. . The general idea can be broken into a few steps: . Create a model | Make predictions on some data with that model | Pretend all (or some) of those predictions are ground truth label | Train a new model with those predictions | We will get into more of the details in the how-to section! . Why would I use pseudo labeling? . There are two main functions pseudo Labeling can be used for. I will step through each and provide a general . Data Cleaning &amp; Noise Reduction . Imagine you have a dataset and all the samples have been hand labeled. You know they can&#39;t all be labeled appropriately because it was manual labeling and you want to improve your labels. You have a few options: . Go through every datapoint again and manually verify them all | Somehow identify the ones that are likely to be wrong and put more focus on those ones. | Pseudo labeling can help with option number 2. By creating a prediction on a datapoint, you can see which labels the model disagrees with. Even better, you can look at the confidence that model has in the prediction. So by looking at datapoints that the model is confident are wrong, you can really narrow your focus on your problem areas quickly. . You then can fix it in 2 ways: . Replace your labels with the predicted labels following some threshold (ie score of .9 or higher). | Manually re-classify these labels if you have the time and domain expertise to do these. | Data Augmentation . This approach can also be used on unlabeled data. Rather than trying to replace bad labels, this approach focuses on creating labels for unlabeled data. This can be used on a kaggle test set for example. The reason this can work is because you are teaching the model the structure of the data. Even if not all labels are correct, a lot can still be learning. . Think about if you were to learn what a new type of object looks like. Maybe a type of furniture you&#39;d never heard of before. Doing a google image search for that name and looking at all the results is really helpful, even if not all of the images that are shown are all correct. . Note: I found out about this Approach from Jeremy Howard. He speaks on this in an old version of the fastai course. Here is a summary that includes a link to the portion of the old course he discusses this approach in. I would highly reccomend checking out the latest course as well here. . How to use pseudo labeling (Noise Reduction) . I will use the validation set for this example because it is a little bit more involved. You can simplify this approach a bit if you are doing this on unlabeled data. . Imports . from fastai.vision.all import * path = untar_data(URLs.MNIST,force_download=True) from sklearn.model_selection import StratifiedKFold from numpy.random import default_rng . Introduce Noise to Data . x = get_image_files(path) y = L(parent_label(o) for o in get_image_files(path)) . Get 10% of the indexes to randomly change . n = len(x) rng = default_rng() noise_idxs = rng.choice(n, size=round(n*0.1), replace=False) len(noise_idxs),noise_idxs[:5] . (7000, array([17419, 48844, 61590, 49810, 26348])) . Randomly change these so we have some bad labels . for i in range(0,len(noise_idxs)): old_path = str(x[noise_idxs[i]]) if &#39;training&#39; in old_path: new_path = str(x[noise_idxs[i]])[:49]+f&#39;{np.random.randint(0,10)}&#39;+str(x[noise_idxs[i]])[50:] elif &#39;testing&#39; in old_path: new_path = str(x[noise_idxs[i]])[:48]+f&#39;{np.random.randint(0,10)}&#39;+str(x[noise_idxs[i]])[49:] os.system(f&#39;mv {old_path} {new_path}&#39;) . Look at Data . Some of our labels are now misclassified, but we don&#39;t know which ones. We could look at every image to find them, but that would take a ton of time. Let&#39;s try to find the misclassified images and correct them using a pseudo labeling approach. . mnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=parent_label) dls = mnist.dataloaders(path,bs=16) dls.show_batch(max_n=36,figsize=(6,6)) . Create Crossfold, Train, and Predict . This step is much simpler if you are generating labels for the test set, as you would train your model as normal and predict as normal. The reason I am doing cross folds is to get predicted labels on the training set. . Note: I am doing this with a 2 fold, but you may want to use a 5-fold or more folds. This cross-fold code was mostly supplied by Zach Mueller, with minor modifications by me for this dataset and tutorial. There is also a tutorial he wrote with more details here . skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1) splits, preds, targs, preds_c, items = [],[],[],[], [] for _, val_idx in skf.split(x,y): splitter = IndexSplitter(val_idx) splits.append(val_idx) mnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=splitter, get_y=parent_label) dls = mnist.dataloaders(path,bs=16) learn = cnn_learner(dls,resnet18,metrics=accuracy) learn.fine_tune(2,reset_opt=True) # store predictions p, t, c = learn.get_preds(ds_idx=1,with_decoded=True) preds.append(p); targs.append(t); preds_c.append(c); items.append(dls.valid.items) . epoch train_loss valid_loss accuracy time . 0 | 1.310182 | 1.102497 | 0.722114 | 01:01 | . epoch train_loss valid_loss accuracy time . 0 | 0.696935 | 0.617736 | 0.886114 | 01:20 | . 1 | 0.631840 | 0.570121 | 0.895343 | 01:21 | . epoch train_loss valid_loss accuracy time . 0 | 1.379348 | 1.098212 | 0.715343 | 01:04 | . epoch train_loss valid_loss accuracy time . 0 | 0.725246 | 0.607686 | 0.888714 | 01:21 | . 1 | 0.625833 | 0.557313 | 0.897943 | 01:21 | . Look at Predictions . Lets throw it all in a dataframe so we can look at what we have a little easier. First, let&#39;s break out our differnt pieces of information. . items_flat = L(itertools.chain.from_iterable(items)) imgs = L(o for o in items_flat) y_true = L(int(parent_label(o)) for o in items_flat) # Labels from dataset y_targ = L(int(o) for o in torch.cat(targs)) # Labels from out predictions y_pred = L(int(o) for o in torch.cat(preds_c)) # predicted labels or &quot;pseudo labels&quot; p_max = torch.cat(preds).max(dim=1)[0] # max model score for row . We can double check we are matching things upp correctly by checking that the labels line up from the predictions and the original data. Throwing some simple assert statements in is nice because it takes no time and it will let you know if you screw something up later as you are tinkering with things. . assert (y_true == y_targ) # test we matched these up correct . Put it in a dataframe and see what we have. . res = pd.DataFrame({&#39;imgs&#39;:imgs,&#39;y_true&#39;:y_true,&#39;y_pred&#39;:y_pred,&#39;p_max&#39;:p_max}) res.head(5) . imgs y_true y_pred p_max . 0 /home/isaacflath/.fastai/data/mnist_png/testing/1/8418.png | 1 | 1 | 0.864995 | . 1 /home/isaacflath/.fastai/data/mnist_png/testing/1/2888.png | 1 | 7 | 0.900654 | . 2 /home/isaacflath/.fastai/data/mnist_png/testing/1/6482.png | 1 | 1 | 0.906335 | . 3 /home/isaacflath/.fastai/data/mnist_png/testing/1/7582.png | 1 | 1 | 0.902999 | . 4 /home/isaacflath/.fastai/data/mnist_png/testing/1/4232.png | 1 | 1 | 0.925955 | . Perfect so lets get a list of our images our model got &#39;wrong&#39; and grab some random ones out of the top 5000 the model was most confident about. The theory is that many of these may be misclassified, and we can reclassify them either using the predicted &#39;pseudo&#39; labels, or with manual classification. . imgs = res[res.y_true != res.y_pred].sort_values(&#39;p_max&#39;,ascending=False)[:5000].sample(frac=1) . And then we plot them and see our predicted labels of these are WAY better than the actual labels. A great way to identify some bad labels. . %matplotlib inline fig, ax = plt.subplots(5,5,figsize=(10,10)) for row in range(0,5): for col in range(0,5): img_path1 = imgs.iloc[row*4+col,0] img_path1 = np.array(Image.open(img_path1)) ax[row,col].imshow(img_path1,cmap=&#39;Greys&#39;) ax[row,col].set_title(f&#39;Label:{parent_label(imgs.iloc[row*4+col,0])} | Pred:{imgs.iloc[row*4+col,2]}&#39;) ax[row,col].get_xaxis().set_visible(False) ax[row,col].get_yaxis().set_visible(False) . What Next? . Now that we have found mislabeled data, we can fix them. We see that in this problem in the top 5000 most confident wrong answers our predicted labels are much better. . So the next step would be to replace the labels with our predicted labels, then train our model on the newly cleaned labels! . Note: This same approach can be used on unlabeled data to get data points the model is confident in to expand the training data. .",
            "url": "https://isaac-flath.github.io/fastblog/deep%20learning/2020/12/15/Pseudo-Labeling.html",
            "relUrl": "/deep%20learning/2020/12/15/Pseudo-Labeling.html",
            "date": " • Dec 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Secrets in NBdev CI",
            "content": "Intro . Goal: The goal of this article is to walk through how to store encrypted secrets in Github Actions for the purpose of using these for NBDev&#39;s CI. From this post you will see how to add secrets to any Github Action, but the focus of this post is on NBDev Continuous Integration . What&#39;s Included in this post: The minimum needed to get secrets stored in a specific repositories Github Action. . Where can I find something more detailed on this topic? Github Docs. This guide is mostly a subset of information that in on that page. . Background . What is NBDEV? NBDev is a development environment for python. It allows for &quot;Real&quot; development in jupyter notebooks that will automatically build documentation, run tests, and build the library based on those notebooks. All code, tests, and documentation is written in Jupyter Notebooks and the rest is automated to convert it into the appropriate formats. . What is a Github Action? A github action is a piece of code that you run on a specific trigger. For example, when updates are committed to Github, NBDev runs all the notebooks (which includes tests) to ensure that all tests still pass. . How to . Follow instructions for either the &quot;Create Encrypted Secrets for Repo&quot; section for individual permissions or &quot;Create Encrypted Secrets for Organization&quot; for organization level secrets. Further sections are the same regardless of which approach you take. . Note: A primary reason to use organization level secrets is that they can be used across the organization in multiple different repositories. This is convenient for minimizing redundancy. . Create Encrypted Secrets for Repo . . In your Repository go to Settings -&gt; Secrets -&gt; Add New Secret and add your secret . . Create Encrypted Secrets for Organization . . Note: You must be an admin in your organization for this section . go to Settings -&gt; Secrets -&gt; Add New Secret and add your secret. . Note: For organization secrets there is an extra drop down for you to manage permissions. This is the main difference between &quot;Repository&quot; secrets and &quot;Organization&quot; secrets. You need to select whether you want secrets to be for public repos, private repos, or specific repositories. . . You can update which repositories have access to the secret by visiting the Settings -&gt; Secrets page (Same place you created the secret at above) and selecting &quot;Update&quot; . Adding to Github Action . . Note: I am demonstrating how to add a secret to the NBDev CI action, though the same thing can be done in any Github Action. In your reposity go to Actions -&gt; CI -&gt; Select most recent result. . . Select &quot;Workflow File and then the edit Symbol. This will give us the main workflow file in our repository for this action and allow us to edit it. . . Add our &quot;SUPERSECRET&quot; to the environment in the &quot;Run Tests&quot; section. We will do this by adding a little bit of code which will store our secret &quot;SUPERSECRET&quot; that we created in the repo earlier, and put that as an environment variable &quot;SUPER_SECRET&quot;. . Note: I could add multiple secrets here instead of just one, with 1 secret per line. Here is what it looks like in the full workflow file. . . Then you can commit this change right in your browser . Accessing in Python . Now, we can access this environment variable in the normal way in python. . import os superSecretKey = os.environ[&#39;SUPER_SECRET&#39;] .",
            "url": "https://isaac-flath.github.io/fastblog/ci/githubactions/nbdev/2020/12/06/GithubActionsSecrets.html",
            "relUrl": "/ci/githubactions/nbdev/2020/12/06/GithubActionsSecrets.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Partial Dependence Plotting",
            "content": "What is it? . Traditional analysis looks at historical data and describes it. The data is fixed and you are looking to make a prediction based on the data. The person is responsible for finding and understanding the interactions. This gets exponentially more difficult in higher dimensional problems. . Partial Dependence Plotting looks at a prediction, and modifies the data to see how it effects the prediction. The model is fixed and you are looking to understand interactions by modifying the data. The model is responsible for finding the interactions and the person just focuses on understanding the interactions the model found. . In this way, you can think of Descriptive Analysis as a method of analysis that focuses on understanding the data, where Partial Dependence Plotting is a method of analysis that focuses on understanding the predictions. They can accomplish similar goals but approach it differently. . Why use it? . One of the biggest challenges of statistics is that is requires a person to make a series of assumptions. Whether you are doing p-values, or correlations, or any other test you are typically making some assumption. Maybe you have to assume the data follows a particular distribution, maybe you have to assume that your variables are independent, or maybe you assume your variables and linearly dependent. Whatever the case may be, you typically have to make some assumption - and if you make the wrong assumptions you can get incorrect findings. It&#39;s for this reason that P-Values are not recommended for use in validating results by the majority of the top statisticians (including the American Statistical Association), despite the fact that most business analysts use them heavily in their analysis and decision making. . This leads me to the first advantage of the partial dependence plotting approach, which is a big differentiator between statistics and data science. With partial dependence plotting you are testing through experimentation rather than through descriptive statistics and statistical testing. For example a Neural Network can approximate any function, whether that is linear, exponential, logistic, or any other shape with any number of interactions. So I can use that, measure the accuracy, and understand the uncertainty of my analysis with no assumptions about distribution, co-linearity, or type of function. . A second advantage is that if you have sufficient data for your analysis, but it&#39;s not all the right type due to some data imbalance you can leverage all the data for the analysis. For example, if you are looking at customer churn you likely have many more accounts that did not churn than accounts that did churn. With descriptive statistics you will look at the few churned accounts and see what they have in common. With a model driven approach, you can look at accounts that did not churn and see what changes you could make to those accounts that makes them more likely to churn. . A third advantage is that the output you are seeing is the prediction. The prediction of the future is often what is of value, so if the goal is to make some decision based on what is likely to happen in the future keeping that in the forefront is ideal. . Examples . Ok enough jibber-jabber. Let&#39;s look at some examples. We&#39;ll start simple and get a litle bit more complicated over time. . Logistic Regression . Let&#39;s create a model on the classic iris dataset and fit a logistic regression to it. When we do this, we see we have a mean accuracy score of about 97%. . from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) clf = LogisticRegression(max_iter=500).fit(X, y) clf.score(X, y) . 0.9733333333333334 . That&#39;s pretty good, but we don&#39;t really know what the important variables are. Let&#39;s experiment a bit. . for our first 2 rows of data we can see the model predicts 0, which is the correct answer. What changes could we make to the data to make it predict something else? . X[0:2, :] . array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2]]) . clf.predict(X[0:2, :]) . array([0, 0]) . Let&#39;s test adding to each of the columns and see if any change the predictions. Our original prediction is [0,0], so anything different tells us something . for i in range(0,X_temp.shape[1]): X_temp = X[0:2, :].copy() X_temp[:,i] = X_temp[:,i]+20 print(&#39;New Values&#39;) print(X_temp) print(&#39;Prediction&#39;) print(clf.predict(X_temp)) . New Values [[25.1 3.5 1.4 0.2] [24.9 3. 1.4 0.2]] Prediction [1 1] New Values [[ 5.1 23.5 1.4 0.2] [ 4.9 23. 1.4 0.2]] Prediction [0 0] New Values [[ 5.1 3.5 21.4 0.2] [ 4.9 3. 21.4 0.2]] Prediction [2 2] New Values [[ 5.1 3.5 1.4 20.2] [ 4.9 3. 1.4 20.2]] Prediction [2 2] . That&#39;s interesting. We can see that adding to the 1st, 3rd, and 4th variable made our model make a different prediction. Since we know our model was about 97% accurate, we know this is meaningful. It is picking up on some trend in the data using each of these columns. Lets hone in on column 1 to see if we can understand this more. . for i in range(-10,10): X_temp = X[0:2, :].copy() X_temp[:,0] = X_temp[:,0]+i if (clf.predict(X_temp) == np.array([0,0])).all(): continue print(&#39;Prediction for adding &#39; +str(i)) print(clf.predict(X_temp)) . Prediction for adding 4 [0 1] Prediction for adding 5 [1 1] Prediction for adding 6 [1 1] Prediction for adding 7 [1 1] Prediction for adding 8 [1 1] Prediction for adding 9 [1 1] . Great, here we see that subtracting from that value does not change the prediction, but once we start adding 4 - 5 to it changes. Our original value for these rows were 5.1 and 4.9, so it seems that when we get to the 9 - 10 range for that value it becomes more likely that it is a different class all other things equal. . This is great insight, and throughout this process we made no assumptions of co-linearity, distribution, or anything else. we just manipulated the data to see the change in predictions of the mode. . Neural Network . Let&#39;s try a slightly more difficult problem and use a Neural Net. I could have used an XGBoost or a random forest, or any other model choice. . The goal of this dataset is to predict whether the person makes more or less than 50K salary. This could be useful for marketing reasons if you are trying to decide what products to market to whom. Let&#39;s first look at the data. . from fastai.tabular.all import * path = untar_data(&#39;ADULT_SAMPLE&#39;) df = pd.read_csv(path/&#39;adult.csv&#39;) df.head() . age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary . 0 49 | Private | 101320 | Assoc-acdm | 12.0 | Married-civ-spouse | NaN | Wife | White | Female | 0 | 1902 | 40 | United-States | &gt;=50k | . 1 44 | Private | 236746 | Masters | 14.0 | Divorced | Exec-managerial | Not-in-family | White | Male | 10520 | 0 | 45 | United-States | &gt;=50k | . 2 38 | Private | 96185 | HS-grad | NaN | Divorced | NaN | Unmarried | Black | Female | 0 | 0 | 32 | United-States | &lt;50k | . 3 38 | Self-emp-inc | 112847 | Prof-school | 15.0 | Married-civ-spouse | Prof-specialty | Husband | Asian-Pac-Islander | Male | 0 | 0 | 40 | United-States | &gt;=50k | . 4 42 | Self-emp-not-inc | 82297 | 7th-8th | NaN | Married-civ-spouse | Other-service | Wife | Black | Female | 0 | 0 | 50 | United-States | &lt;50k | . Ok, so we can see we&#39;ve got a lot of difference variables or both categorical and continuous to look at. Lets format this for a Neural Network and fit the model. . to = TabularPandas(df, procs=[Categorify, FillMissing,Normalize], cat_names = [&#39;workclass&#39;, &#39;education&#39;, &#39;marital-status&#39;, &#39;occupation&#39;, &#39;relationship&#39;, &#39;race&#39;], cont_names = [&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education-num&#39;], y_names=&#39;salary&#39;, splits=RandomSplitter(valid_pct=0.2)(range_of(df))) dls = to.dataloaders(bs=64) . learn = tabular_learner(dls, metrics=accuracy, wd=0.01) print(to.valid.xs.shape) learn.fit_one_cycle(3) . epoch train_loss valid_loss accuracy time . 0 | 0.363723 | 0.374584 | 0.823710 | 00:04 | . 1 | 0.350536 | 0.357585 | 0.832002 | 00:04 | . 2 | 0.342484 | 0.354669 | 0.831081 | 00:04 | . to.y . 21805 0 14537 0 1399 0 8107 0 16255 0 .. 3603 1 15429 0 3551 0 1880 0 30442 0 Name: salary, Length: 32561, dtype: int8 . df.iloc[0] . age 49 workclass Private fnlwgt 101320 education Assoc-acdm education-num 12 marital-status Married-civ-spouse occupation NaN relationship Wife race White sex Female capital-gain 0 capital-loss 1902 hours-per-week 40 native-country United-States salary &gt;=50k Name: 0, dtype: object . Perfect, we have pretty good accuracy, and our validation set has over 6000 data points. Let&#39;s look at some features to see if we can understand what impacts our model&#39;s prediction of the individuals salary. . 0 = &lt;50K, so it correctly predicts the first row is someone the makes more than 50k. Let&#39;s see at what point the prediction switches if I reduce hours worked. . row, clas, probs = learn.predict(df.iloc[0]) clas . tensor(1) . for i in range(-40,0): X_temp = df.iloc[0].copy() X_temp[&#39;hours-per-week&#39;] = X_temp[&#39;hours-per-week&#39;]+i row, clas, probs = learn.predict(X_temp) if clas == tensor(1): continue print(&#39;Prediction for adding &#39; +str(i)) print(clas) from IPython.display import clear_output clear_output() . Interestingly, the model isn&#39;t convinced even if I change hours works to 0, maybe it thinks the money is passive income or comes from the husband. Let&#39;s see if we can figure that out. . for i in df.relationship.unique(): X_temp = df.iloc[0].copy() X_temp[&#39;relationship&#39;] = i row, clas, probs = learn.predict(X_temp) print(&#39;Prediction for adding &#39; +str(i)) print(clas) . Prediction for adding Wife tensor(1) . Prediction for adding Not-in-family tensor(0) . Prediction for adding Unmarried tensor(0) . Prediction for adding Husband tensor(0) . Prediction for adding Own-child tensor(0) . Prediction for adding Other-relative tensor(0) . If we change the relationship to anything else, the model starts thinking she makes less than 50K instead of more. We can continue to experiment with individual rows, or when ready try some larger experiments across the larger dataset. . df[[&#39;salary&#39;,&#39;age&#39;]][:1000].groupby(&#39;salary&#39;).count() . age . salary . &lt;50k 754 | . &gt;=50k 246 | . results = list() for i in df.relationship.unique(): g50k, l50k = (0,0) df_temp = df[:1000].copy() df_temp[&#39;relationship&#39;] = i for rownum in range(0,len(df_temp)): if learn.predict(df_temp.iloc[rownum])[1] == tensor(1): g50k += 1 else: l50k += 1 clear_output() results.append((i,g50k,l50k)) results . [(&#39; Wife&#39;, 232, 768), (&#39; Not-in-family&#39;, 165, 835), (&#39; Unmarried&#39;, 150, 850), (&#39; Husband&#39;, 210, 790), (&#39; Own-child&#39;, 161, 839), (&#39; Other-relative&#39;, 107, 893)] . We see that changing the relationship did impact the predictions across 1000 samples. Married coules, both &quot;Wife&quot; and &quot;Husband&quot; seem to be much better off with making more than 50K per year. I wonder if their income is combined and thus being double counted. &quot;Unmarried&quot; and &quot;Other-Relative&quot; seem to be the relationship types that are least likely to make more than 50K. Keep iterating through experiment like we have been to dig deeper! . You may be asking, how do I know 1000 samples is enough? The answer - I don&#39;t. Try 1500 samples next and if the results show roughly the same thing you know you have a representative sample. This is again, the data science approach vs the statistical approach. . What now? . Now, you keep digging like we have been. We can be completely sure that the model has found meaningful interactions. We saw that in the accuracy score. So rather than me trying to find interactions, I am trying to uncover the interactions that the model has found. Again, I am making no assumptions in regard to distribution, co-linearity, type of relationships between variable, sample size, or anything else so it allows for much safe analysis of complex datasets. .",
            "url": "https://isaac-flath.github.io/fastblog/interpretation/2020/11/20/PartialDependencePlotting.html",
            "relUrl": "/interpretation/2020/11/20/PartialDependencePlotting.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Media Bias Text Generation Transformer",
            "content": "Goals . Project Goals . Gain Competence doing Text Generation using transformers . I wanted to learn a bit more about text generation, and these transformers that are taking over text generation. I chose to work with GPT2, and found that the HuggingFace transformers and library came very highly recommended. . Get a glimpse in differences in bias from different media sources . I have recently been reading news from several media sources, and I am often astounded by the differences. What I wanted to do, is take a pre-trained model and fine tune it on a specific media source. Do this several times on different sources and compare. In what ways will my models be biased by their source? One might hypothesise that this would show some of the biases that people may be nudged toward by only consuming that media source. . Article Goals . The purpose of this article is just to show some of the outputs and get people thinking about how an approach like this could be interesting and useful to study further. The bulk of the project was on setting up and fine-tuning transformers, which is not at all what this article is about. This article is about looking at some of the outputs in order to spur additional thought on the topic . Prompt Examples . Let&#39;s start by looking at a couple prompts to see how we might compare them manually. We will take a look at a coupe of examples. . Scientists . In this case, the models were each given the prompt &quot;The reason Scientists do press briefings is&quot;. Can you spot the outlier? . CNBC: The reason Scientists do press briefings is because they want to know what&#39;s going on in the world,&quot; he said. &quot;If you don&#39;t have a clear understanding of where things are, it can be very difficult for scientists to get their work done.CNN: The reason Scientists do press briefings is because they want to know what&#39;s going on in the world,&quot; he said. &quot;They&#39;re trying to get a sense of where things are.&quot;Fox News: The reason Scientists do press briefings is because they&#39;re trying to find out more about what the Russians are up toThe New York Times: The reason Scientists do press briefings is that it’s a way for them to get the most out of their timeWashington Post: The reason Scientists do press briefings is because they’re the only ones who know what&#39;s going on. . Antifa . Another area where there&#39;s quite a bit of contrast is Antifa. Which one doesn&#39;t match the others? . CNBC: Antifa is a group that has been active in the U.S. for more than 50 years, according to its website. &quot;We are committed to protecting our community from violence and hate,&quot; it says on its website.CNN: Antifa is a group that advocates for the rights of people to peacefully assemble, organize and participate in peaceful protests.Fox News: Antifa is a group that&#39;s trying to take down the president of the United States. And so, I don&#39;t think it&#39;s going to be easy for them to come in and do what they want to do.The New York Times: Antifa is a group that has been active since the beginning of the year, and it&#39;s not clear how many people have joined. Washington Post: Antifa is a group that advocates for the rights of women and girls to self-identify as LGBTQ, according to its website. . Closing . Now it&#39;s fairly entertaining to read through the ones that really demonstrate bias, like this one: . Fox News: The thing we should do with immigrants is make sure that they&#39;re not coming to the United States illegally. (END VIDEO CLIP) INGRAHAM: Well, I don&#39;t know if you have a lot of sympathy for those who are trying to come into this country as unaccompanied minors or people in their 20s and 30s being separated from their parents because it seems like every single one of them has some kind of criminal record which makes us all very worried about these kids crossing our southern border at any given time. . and to read zingers like this one . CNBC: The most hated politician is Donald Trump, and he&#39;s got a lot of problems . and run across snippets like this . &quot;The landmark summit between North Korean leader Kim Jong Un and dictator-in-waiting Donald Trump&quot; . But that&#39;s a pretty biased way of analyzing things. So instead let&#39;s look at overall word counts and see if there is a difference in word choice between these models. . Word Count . Approach . I came up with a bunch of prompts and gave them to each model. Now, I will count the number of times words appear to see if there are any differences in word choice between media sources. . Get word counts for each group . import pandas as pd import numpy as np import matplotlib.pyplot as plt import nltk from nltk.corpus import stopwords from collections import Counter . . The first step is to read in the text file with text generated by the model (Go to that URL if you want to see more). . df = pd.read_csv(&#39;https://raw.githubusercontent.com/Isaac-Flath/MediaBias/main/text_generated.csv&#39;, usecols=[&#39;source&#39;,&#39;prompt&#39;,&#39;text&#39;]) . . We will then convert everything to lowercase and get rid of punctuation . tokenizer = nltk.RegexpTokenizer(r&quot; w+&quot;) df[&#39;text&#39;] = df.text.str.lower().apply(tokenizer.tokenize) . . Finally we count all the words and put it into a pandas data frame for convenience . results = Counter() df[&#39;text&#39;].apply(results.update) results = pd.concat([pd.DataFrame.from_dict(results.keys()),pd.DataFrame.from_dict(results.values())],axis=1) results.columns = [&#39;words&#39;,&#39;all_cnt&#39;] . . Now, we repeat this same thing for each media source so we have word counts for each source (ie CNN) . for s in df.source.unique(): tempdf = df[df.source==s].copy() tempcntr = Counter() tempdf[&#39;text&#39;].apply(tempcntr.update) tempresults = pd.concat([pd.DataFrame.from_dict(tempcntr.keys()),pd.DataFrame.from_dict(tempcntr.values())],axis=1) tempresults.columns = [&#39;words&#39;,s.replace(&#39; &#39;,&#39;&#39;)+&#39;_cnt&#39;] results = pd.merge(results,tempresults, how = &#39;left&#39;, on = &#39;words&#39;) results.fillna(0,inplace=True) . . results.head() . words all_cnt CNBC_cnt CNN_cnt FoxNews_cnt TheNewYorkTimes_cnt WashingtonPost_cnt . 0 blockchain | 7 | 1.0 | 2.0 | 2.0 | 1.0 | 1.0 | . 1 is | 276 | 50.0 | 54.0 | 55.0 | 58.0 | 59.0 | . 2 a | 592 | 125.0 | 138.0 | 91.0 | 141.0 | 97.0 | . 3 technology | 12 | 4.0 | 2.0 | 2.0 | 2.0 | 2.0 | . 4 that | 513 | 92.0 | 105.0 | 92.0 | 106.0 | 118.0 | . Exclude stop words . We don&#39;t really care about some words. For example, if one station uses &quot;the&quot; a bit more that doesn&#39;t mean much to me. So let&#39;s remove stop words. . Conveniently nltk has a list of english stop words already put together. . print(stopwords.words(&#39;english&#39;)[:5]) . [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;] . Now let&#39;s use that to filter out dataframe for all stop words, as well as sort it in descending order so the most common words are at the top. . results = results[~results.words.isin(stopwords.words(&#39;english&#39;))].copy() results.sort_values(&#39;all_cnt&#39;,ascending=False, inplace=True) . Frequency Graphs . Let&#39;s graph the amount the count deviates from the average and see if there is any point looking further. If these are all close to blank graphs (no deviation from average) then that means the text they generated uses almost the same words. . In this case, we can see that they all deviate in different ways on different words. So the media source impacted the models word choice . width, indexes = 0.7, np.arange(len(results)) fig, (ax1,ax2,ax3,ax4,ax5) = plt.subplots(5,figsize=(5,10)) ax_map = [(ax1,results.CNBC_cnt),(ax2,results.FoxNews_cnt),(ax3,results.CNN_cnt), (ax4,results.TheNewYorkTimes_cnt),(ax5,results.WashingtonPost_cnt)] for x in ax_map: x[0].set_title = x[1].name x[0].bar(indexes,x[1]-results.all_cnt/5,width) x[0].set_ylim(-10,10) plt.show() . .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/2020/11/01/MediaBiasTransformer.html",
            "relUrl": "/neural%20networks/2020/11/01/MediaBiasTransformer.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Using the Learning Rate Finder (Beginner)",
            "content": "The importance of a good Learning Rate . Before we get started, there&#39;s a few questions we need to understand. . Why bother selecting a learning rate? Why not use the defaults? . Quite simply, a bad learning rate can mean bad performance. There are 2 ways this can happen. . Learning too slowly: If the learning rate is too small it will take a really long time to train your model. This can mean that to get a model of the same accuracy, you either would need to spend more time or more money. Said another way, it will either take longer to train the model using the same hardware or you will need more expensive hardware (or some combination of the two). . | Learning too quickly: If the learning rate is too large, the steps it takes will be so big it overshoots what is an optimal model. Quite simply your accuracy will just bounce all over the place rather than steadily improving. . | So we need a learning rate that is not too big, but not too small. How can we thread the needle? . Isn&#39;t there some automated way to select a learning rate? . The short answer is no, there isn&#39;t. There are some guidelines available that will be covered, but ultimately there is no sure-fire automated way to automated selectig a learning rate. The best method is to to use the learning rate finder. . The Problem . We will be identifying cats vs dogs. . Note: All details other than selecting a learning rate will be glossed over. Please see https://docs.fast.ai/tutorial.vision for more detail on the actual modeling. . To get our model started, we will import the library. . from fastai.vision.all import * . Then download the data . path = untar_data(URLs.PETS) . Now, let&#39;s organize the data in a dataloader. . files = get_image_files(path/&quot;images&quot;) def label_func(f): return f[0].isupper() dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224)) . Now we can look at the pictures we are looking to classify. We are predicting whether these images are a cat or not. . dls.show_batch(max_n=3) . Now, we can create a learner to do the classification. . learn = cnn_learner(dls, resnet34, metrics=error_rate) . Learning Rate Finder . Finally we can get to the main topic of this tutorial. I have modified the learning rate finder from fastai to add dots at the reccomended locations. We can see a couple of red dots as fast reference points, but it is still on us to pick the value. It&#39;s a bit of an art. . Here&#39;s the new code for the lr_find and plot_lr_find functions, which have been added to Walk With Fastai repo as well. . @patch def plot_lr_find(self:Recorder, suggestions= False,skip_end=5, lr_min=None, lr_steep=None): &quot;Plot the result of an LR Finder test (won&#39;t work if you didn&#39;t do `learn.lr_find()` before)&quot; lrs = self.lrs if skip_end==0 else self.lrs [:-skip_end] losses = self.losses if skip_end==0 else self.losses[:-skip_end] if suggestions: lr_min_index = min(range(len(lrs)), key=lambda i: abs(lrs[i]-lr_min)) lr_steep_index = min(range(len(lrs)), key=lambda i: abs(lrs[i]-lr_steep)) fig, ax = plt.subplots(1,1) ax.plot(lrs, losses) if suggestions: ax.plot(lr_min,L(losses)[lr_min_index],&#39;ro&#39;) ax.plot(lr_steep,L(losses)[lr_steep_index],&#39;ro&#39;) ax.set_ylabel(&quot;Loss&quot;) ax.set_xlabel(&quot;Learning Rate&quot;) ax.set_xscale(&#39;log&#39;) @patch def lr_find(self:Learner, start_lr=1e-7, end_lr=10, num_it=100, stop_div=True, show_plot=True, suggestions=True): &quot;Launch a mock training to find a good learning rate, return lr_min, lr_steep if `suggestions` is True&quot; n_epoch = num_it//len(self.dls.train) + 1 cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div) with self.no_logging(): self.fit(n_epoch, cbs=cb) if suggestions: lrs,losses = tensor(self.recorder.lrs[num_it//10:-5]),tensor(self.recorder.losses[num_it//10:-5]) if len(losses) == 0: return lr_min = lrs[losses.argmin()].item() grads = (losses[1:]-losses[:-1]) / (lrs[1:].log()-lrs[:-1].log()) lr_steep = lrs[grads.argmin()].item() if show_plot: self.recorder.plot_lr_find(suggestions=True, lr_min =lr_min/10., lr_steep=lr_steep) else: if show_plot: self.recorder.plot_lr_find() if suggestions: return SuggestedLRs(lr_min/10.,lr_steep) . What we are looking for is a logical place on the graph where the loss is decreasing. The red dots on the graph indicate the minimum value on the graph divided by 10, as well as the steepest point on the graph. . We can see that in this case, both the dots line up on the curve. Anywhere in that range will be a good guess for a starting learning rate. . learn.lr_find() . SuggestedLRs(lr_min=0.010000000149011612, lr_steep=0.0008317637839354575) . Now we will fine tune the model as a first training step. . learn.fine_tune(1, base_lr = 9e-3) . epoch train_loss valid_loss error_rate time . 0 | 0.087485 | 0.025303 | 0.006766 | 00:15 | . epoch train_loss valid_loss error_rate time . 0 | 0.077561 | 0.028005 | 0.010825 | 00:15 | . Now that we have done some training, we will need to re-run the learning rate finder. As the model changes and trains, we can find a new &#39;best&#39; learning rate. . When we run it below, we see the graph is a bit tricker. We definitely don&#39;t want the point to the far left where the loss is spiking. But we also don&#39;t want the point on the right where the loss is increasing. For this we will find a value between the two on that curve where loss is decreasing and train some mode. . learn.lr_find() . SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=1.0964781722577754e-06) . We end up with a 0.6% error rate. Not bad! . learn.fit_one_cycle(1,9e-7) . epoch train_loss valid_loss error_rate time . 0 | 0.040149 | 0.021365 | 0.006089 | 00:15 | .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/2020/10/20/LR_Finder_Modification.html",
            "relUrl": "/neural%20networks/2020/10/20/LR_Finder_Modification.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "APL - Geometry of Linear Equations (18.06_L1)",
            "content": "Motivation . I am retaking this linear algebra course with 2 intentions. . First, I want to learn Dyalog APL - which is a super cool and powerful way to do linear algebra and numerical transforms. The second is to get a better understanding of Linear Algebra. I would love to achieve &#39;fluency&#39; in linear algebra. . I will be doing everything in APL. So what is APL? I think of it as a better regex for linear algebra. Like regex, it is very condensed with different symbols representing different operations. Like regex, you can combine these symbols in many different ways to achieve almost anything (but in linear algebra instead of text). Unlike regex, APL is it&#39;s own full language in itself and not some string sitting inside of another language. . This blog post will be my notes about Lecture 1 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Matrix Multiplication . Imagine we want to multiply 2 matrices together, a1 and a2. How should we think about it? . + a1 ← 2 2 ⍴ 2 5 1 3 + a2 ← 2 1 ⍴ 1 2 . 2 5 1 3 1 2 The result is a linear combination of the columns of ar1 which we can of course write in APL like this . c1←a1[;1]×a2[1;] ⍝ Column 1 of a1 time row 1 of a2 c2←a1[;2]×a2[2;] ⍝ Column 2 of a1 time row 2 of a2 c1+c2 ⍝ Sum those up as a linear combination . 12 7 A better way to do this in APL is to condense it down using an inner product. . a1+.×a2 . 12 7 So lets walk through the example of +.× and see why we get the correct answer and what it is doing. . What the . does is gives us a framework or structure for the answer. We take combinations of rows columns, which gives us the following equations to calculate our 2 numbers. . (2 ?1? 1) ?2? (5 ?1? 2) . (1 ?1? 1) ?2? (3 ?1? 2) . Then we need to define the 2 unknown operators (?1? and ?2?). These are defined by the operators on each side of the dot. Normal matrix multiplication means ?1? = * and ?2? = +, which is why +.x gives us matrix multiplication. But really we could put any operators in there. Addition, division, subtraction, exponents, min or max, or any other operator you can think of or create. . Another way to think about the same thing is to do this in three steps . Create our inner product matrices (rows * columns from original matrices): . $ begin{bmatrix}?&amp;&amp;1 &amp;?&amp;2 2&amp;5&amp; end{bmatrix}$ $ begin{bmatrix}?&amp;&amp;1 &amp;?&amp;2 1&amp;3&amp; end{bmatrix}$ . Use the final operator (multiplication) to fill in the ? . $ begin{bmatrix}2&amp; &amp;10 end{bmatrix}$ $ begin{bmatrix}1&amp; &amp;6 end{bmatrix}$ . Use the first operator to combine the ? . $10+2 = 12$ . $1+6 = 7$ . Now that we understand that, we can flip our operators and look at ×.+ instead of +.×. We can also do any number of other operators to do lots of different matrix operations. . ⍝ using addition.multiplication (normal matrix multiplication) a1+.×a2 ⍝ using multiplication.addition a1×.+a2 ⍝ using max.min a1⌈.⌊a2 ⍝ using addition.subtraction a1-.+a2 ⍝ using exponent.division a1*.÷a2 ⍝ using factorial.natural_log a1!.⍟a2 . 12 7 21 10 2 2 ¯4 ¯3 5.65685 1 1 0.63093 Calculating Y values . Let&#39;s imaging we have a system of 2 equations and want to graph them. How would we solve for many values of x in APL? Let&#39;s just use the same example we had above . + res ← a1 ÷ 2/a2 ⍝ Isolate function so right side (y) is 1 in all instanced . 2 5 0.5 1.5 ⍝ calculate y values for x is 1 through 10 xs ← ⍳10 (res[;1] ∘.× xs) + ((⍴ xs)/2 1 ⍴ res[;2]) . 7 9 11 13 15 17 19 21 23 25 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 And there you have it! . Elimination . +A ← 3 3 ⍴ 1 2 1 3 8 1 0 4 1 v ← 3 1 ⍴ 2 12 2 E21 ← 3 3 ⍴ 1 0 0 (A[2;1]×¯1÷A[1;1]) 1 0 0 0 1 E31 ← 3 3 ⍴ 1 0 0 0 1 0 ((E21+.×A)[3;1]×¯1÷(E21+.×A)[1;1]) 0 1 E32 ← 3 3 ⍴ 1 0 0 0 1 0 0 ((E31+.×E21+.×A)[3;2]×¯1÷(E31+.×E21+.×A)[2;2]) 1 . 1 2 1 3 8 1 0 4 1 E32+.×E31+.×E21+.×v . 2 6 ¯10",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/09/30/APL-Geometry-of-Linear-Equations-(18.06_L1).html",
            "relUrl": "/linear%20algebra/2020/09/30/APL-Geometry-of-Linear-Equations-(18.06_L1).html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "RNN Foundations",
            "content": "from fastai.text.all import * . Credit Where Credit is Due . The concept and techniques covered in this post are covered in much greater detail in Jeremy Howard and Sylvain Gugger&#39;s book. If you like this post, you should buy the book as you&#39;ll probably like it even more! . https://www.amazon.com/gp/product/1492045527/ref=ppx_yo_dt_b_asin_image_o08_s00?ie=UTF8&amp;psc=1| . Data Setup . Get the Data . path = untar_data(URLs.HUMAN_NUMBERS) lines = L() with open(path/&quot;train.txt&quot;) as f: lines += L(*f.readlines()) with open(path/&quot;valid.txt&quot;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . Tokenization . What is Tokenization? Tokenization is about getting &#39;tokens&#39; of language that have meaning. A word could be a token as it has meaning. A piece of punctuation could be a token as it has meaning. If a work is in all capital letters that could be a token. A portion of a word could be a token (ie dis) because a word beginning with dis has meaning. There are many many ways to tokenize, for this post I will use the most simple approach. That is, I will split based on spaces to make each word a token. . txt = &#39; . &#39;.join([l.strip() for l in lines]) . tokens = L(*txt.split(&#39; &#39;)); tokens . (#63095) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...] . Numericalization . Now that things are split into tokens, we need to start thinking about how to feed it to a Neural Network. Neural Networks rely on multiplication and addition, and we can&#39;t do that with a word. Somehow we need to convert these tokens to numbers. That is what Numericalization is all about. We will do this in a few steps: . Get a unique list of all tokens (v) | Assign a number to each of token (vocab) | Replace tokens with numbers (nums) | v = tokens.unique() # Assign a number to each of token (vocab) vocab = {v:i for i,v in enumerate(v)}; # We can lookup the number associated with a token like this vocab[&#39;fifty&#39;] . 23 . nums = L(vocab[tok] for tok in tokens); nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . Sequence Definition . Now that we have tokens in the form of numbers, we need to create out inputs and outputs to the model. For this we need to organize our data into dependent and independent variables. Let&#39;s use the preceding 3 words to predict the next word. Below, we see the same thing in 2 ways - one with tokens and one with numbers. These are the same thing, just translating the tokens to numbers using the vocab above. . Note: Sequence Length (sl) will be 3, because we are using a sequence of 3 words to predict the next word. . sl = 3 # For example, we will use the tokens &#39;one&#39;,&#39;.&#39;, and &#39;two&#39; to predict &#39;.&#39; L((tokens[i:i+sl], tokens[i+sl]) for i in range(0,len(tokens)-sl-1,sl)) . (#21031) [((#3) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;], &#39;.&#39;),((#3) [&#39;.&#39;,&#39;three&#39;,&#39;.&#39;], &#39;four&#39;),((#3) [&#39;four&#39;,&#39;.&#39;,&#39;five&#39;], &#39;.&#39;),((#3) [&#39;.&#39;,&#39;six&#39;,&#39;.&#39;], &#39;seven&#39;),((#3) [&#39;seven&#39;,&#39;.&#39;,&#39;eight&#39;], &#39;.&#39;),((#3) [&#39;.&#39;,&#39;nine&#39;,&#39;.&#39;], &#39;ten&#39;),((#3) [&#39;ten&#39;,&#39;.&#39;,&#39;eleven&#39;], &#39;.&#39;),((#3) [&#39;.&#39;,&#39;twelve&#39;,&#39;.&#39;], &#39;thirteen&#39;),((#3) [&#39;thirteen&#39;,&#39;.&#39;,&#39;fourteen&#39;], &#39;.&#39;),((#3) [&#39;.&#39;,&#39;fifteen&#39;,&#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+sl]), nums[i+sl]) for i in range(0,len(nums)-sl-1,sl)); seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . Dataloader . Now we need to create out dataloader. The Dataloader is just packaging it into batches, and not doing any tranformations or changes to the data. What we saw above is what will be given to the model. . bs = 128 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut],seqs[cut:],bs=bs, shuffle=False) . dls2 = DataLoader(seqs[:cut],bs=bs, shuffle=False) dls3 = DataLoader(seqs[cut:],bs=bs, shuffle=False) dls4 = DataLoaders(dls3,dls3) . Language Model . Naive Model . First, let&#39;s figure out a baseline for what is the best &#39;non-stupid&#39; model we can come up with. If a model can&#39;t beat this score, then it&#39;s not worth anything. . The approach we will take will be to predict the most common token every time. If we do that we get about a 15% accuracy. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, v[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . RNN Number 1 . Code . We are going to make the simplest RNN we can. Here&#39;s a quick explanation of the code below. . for i in range(sl): Because we are feeding in a number of tokens based on our sequence length, sl, which was defined as 3. We will have 3 steps, 1 per token. . h = h + self.i_h(x[:,i]) For each input token we will run our input to hidden function. We are indexing to grab the column in our embedding matrix that corresponds with the token, and adding that. All this is doing is adding the embedding for the particular token. . h = F.relu(self.h_h(h)) We then run our hidden to hidden function (h_h), which is a linear layer (y = wx + b). We do a ReLu of that, which is just replacing any negative values with 0. . return self.h_o(h) We then run our hidden to output function (h_o), which is another linear layer, but it is outputing the prediction of which word is next. Naturally, this is the size of our vocabulary. . Wrap all that in a class and it looks like the below: . class LM1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(sl): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . Now we can run it below and see that we get almost 50% accuracy before we overfit, which is great considering the most common token only appears 15% of the time. . learn = Learner(dls, LM1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.505863 | 2.136583 | 0.458046 | 00:00 | . 1 | 1.602575 | 1.847033 | 0.480865 | 00:00 | . 2 | 1.503249 | 1.727588 | 0.492275 | 00:00 | . 3 | 1.436492 | 1.771485 | 0.410506 | 00:00 | . Tensors . So what is it REALLY doing? To understand that, I find it helpful to think about the matrix/tensor sizes at each step. . Embeddings . Let&#39;s start with our input_hidden. Our Embedding matrix is has 64 weights (n_hidden) for each token in our vocabulary. So that looks like this: . $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-weights} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }30-tokens$ . Now all the embedding layer does is get the correct columns. So for the first word in the sequence we get the index, then look it up in the embedding matrix. That 1 index location turns into the 64 weights. . $ underbrace{ begin{bmatrix} cdots cdots cdots cdots cdots cdots end{bmatrix}}_{ displaystyle token-idx} left. vphantom{ begin{bmatrix} cdots cdots cdots cdots cdots cdots end{bmatrix}} right }128-bs$ $==$ lookup in embedding matrix $==&gt;$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128$ . Hidden Linear Layer . Next, we have out hidden_hidden. We have our 128x64 matrix from our embedding lookup and we need to do a linear layer. . $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-weights} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128-bs$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }64$ $+$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-bias} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }1$ $=$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-weights} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128-bs$ ===ReLu - Replace all negatives with 0 ===&gt; $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-weights} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128-bs$ . And we do the above for however long our sequence is, in our case 3. So for each token we do the above. We start with 0 on the first loop, and each subsequence loop through we add onto that. . Ouput Linear Layer . We ended with a 128x64 matrix, which isn&#39;t exactly what we want. We have 30 words, so we want to know which one of the 30 is most likely. Specifically for each of the 128 items in our batch, we want 30 scores (1 for each word in our vocab). So we do a similar stepp as our hidden linear layer, but adjust the number of weights so we end up with the matrix of the appropriate size. . $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 64-weights} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128-bs$ $ underbrace{ begin{bmatrix} cdots &amp; cdots cdots &amp; cdots cdots &amp; cdots cdots &amp; cdots end{bmatrix}}_{ displaystyle 30} left. vphantom{ begin{bmatrix} cdots &amp; cdots cdots &amp; cdots cdots &amp; cdots cdots &amp; cdots end{bmatrix}} right }64$ $+$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 30-bias} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots end{bmatrix}} right }1$ $=$ $ underbrace{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}}_{ displaystyle 30-preds} left. vphantom{ begin{bmatrix} cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots cdots &amp; cdots &amp; cdots &amp; cdots end{bmatrix}} right }128-bs$ . RNN Number 2 . Now that we have a simple model, how do we improve it? There are many steps that need to be taken to get to a cutting edge model. We&#39;ll do one improvement, then leave the rest for future blog posts. . One thing that was a bit odd is in the training loop we reset back to 0 every time. What I mean by that, is we would loop through each of the 3 tokens, output our predictions for those, update the weights, then reset back for a new set. This isn&#39;t really how language works. Language has a pattern and a sequence to it. The further back you go the less important, but even things said a couple minutes ago could be important. Could you imagine holding a conversation if you could only remember and respond based on the last 3 words? . So let&#39;s fix this problem. We will move our h=0 up to the initialization of the class, and never reset back to 0. Instead, we will continuously keep adding to it. We will only update the last batch of weights (as if we updated all of them by the 1000th one we would be updating far to many weights to compute). We call this &quot;detaching&quot; it. Ultimately we are left with the same thing, but if has a memory of previous sequences beyond the one we are processing! Let&#39;s see if it makes things better. . class LM2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out . To do this we need to take care that our data is in the appropriate order, so let&#39;s do a few tranformations to make that work. . m = len(seqs)//bs m,bs,len(seqs) def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds . cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . learn = Learner(dls, LM2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) . learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.342321 | 1.897249 | 0.481689 | 00:00 | . 1 | 1.453624 | 1.713581 | 0.449707 | 00:00 | . 2 | 1.154838 | 1.680148 | 0.519775 | 00:00 | . 3 | 1.042766 | 1.566625 | 0.517822 | 00:00 | . 4 | 0.969852 | 1.633654 | 0.542480 | 00:00 | . 5 | 0.937066 | 1.581196 | 0.559570 | 00:00 | . 6 | 0.882712 | 1.660810 | 0.588379 | 00:00 | . 7 | 0.844926 | 1.595611 | 0.597656 | 00:00 | . 8 | 0.808309 | 1.613600 | 0.605225 | 00:00 | . 9 | 0.797358 | 1.621867 | 0.605713 | 00:00 | . And we are up from about 50% accuracy to about 60%! . Conclusion . Hopefully from this post you gained an understanding of the fundamental concepts behind NLP using Neural Networks. While this isn&#39;t cutting edge, the fundamental principles must be understood if you want to gain an intuition about what types of things might work. .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/2020/08/22/RNN_Foundations.html",
            "relUrl": "/neural%20networks/2020/08/22/RNN_Foundations.html",
            "date": " • Aug 22, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "NLP - Tokenization Basics",
            "content": "Intro . In this post we are going to dive into NLP, specifically Tokenization. Tokenization are the foundation of all NLP. . So what is a language model? In short, it is a model that uses the preceding words to predict the next word. We do not need seperate labels, because they are in the text. This is training the model on the nuances of the language you will be working on. If you want to know if a tweet is toxic or not, you will need to be able to read and understand the tweet in order to do that. The language model helps with understanding the tweet - then you can use that model with those weights to tune it for the final task (determining whether the tweet is toxic or not). . For this post, I will be using news articles to show how to tokenize a news article and numericalize it to get ready for deep learning. . Credit Where Credit is Due . The concept and techniques covered in this post are covered in much greater detail in Jeremy Howard and Sylvain Gugger&#39;s book. If you like this post, you should buy the book as you&#39;ll probably like it even more! . https://www.amazon.com/gp/product/1492045527/ref=ppx_yo_dt_b_asin_image_o08_s00?ie=UTF8&amp;psc=1| . The Data . I will be using the &quot;All-the-news&quot; dataset from this site. https://components.one/datasets/all-the-news-2-news-articles-dataset/ . I downloaded then put the csv into a sqlite database for conveniece . import pandas as pd import sqlite3 con = sqlite3.connect(&#39;../../../data/news/all-the-news.db&#39;) pd.read_sql_query(&#39;SELECT publication, min(date),max(date), count(*) from &quot;all-the-news-2-1&quot; group by publication order by max(date) desc limit 5&#39;, con) . publication min(date) max(date) count(*) . 0 Buzzfeed News | 2016-02-19 00:00:00 | 2020-04-02 00:00:00 | 32819 | . 1 The New York Times | 2016-01-01 00:00:00 | 2020-04-01 13:42:08 | 252259 | . 2 Business Insider | 2016-01-01 03:08:00 | 2020-04-01 01:48:46 | 57953 | . 3 Washington Post | 2016-06-10 00:00:00 | 2020-04-01 00:00:00 | 40882 | . 4 TMZ | 2016-01-01 00:00:00 | 2020-04-01 00:00:00 | 49595 | . I am going to pick the 5 most recent New York times Articles. For the final model I will use all of the data, but for simplicity of demonstrating tokenization we will use just 5 articles. Here is an example of the start of one of the articles . df = pd.read_sql_query(&#39;SELECT article from &quot;all-the-news-2-1&quot; where publication = &quot;The New York Times&quot; order by date desc limit 5&#39;, con) ex = df.iloc[1,0]; ex[:162] . &#39;President Trump told of “hard days that lie ahead” as his top scientific advisers released models predicting that the U.S. death toll would be 100,000 to 240,000.&#39; . Tokenization . So how do I turn what I see above (text) into something a neural network can use? The first layer in a neural network is going to do matrix multiplication and addition. How do I multiply &quot;President Trump told of “hard days that lie ahead” as his top scientific advisers released models&quot; by any number? This is the core question we will answer with tokenization. . Note: Tokenization is the method in which we take text and turn them into numbers we can feed into a model . A simple Approach . Let&#39;s start with a simple idea. Let&#39;s treat each word as seperate inputs in the same way that seperate pixels in an image are seperate inputs. We can do this in the english language by splitting our text by spaces/ . ex[:162] . &#39;President Trump told of “hard days that lie ahead” as his top scientific advisers released models predicting that the U.S. death toll would be 100,000 to 240,000.&#39; . import numpy as np tokens = ex.split(sep = &#39; &#39;) tokens[:10] . [&#39;President&#39;, &#39;Trump&#39;, &#39;told&#39;, &#39;of&#39;, &#39;“hard&#39;, &#39;days&#39;, &#39;that&#39;, &#39;lie&#39;, &#39;ahead”&#39;, &#39;as&#39;] . That&#39;s better, now we have distinct data points. But we need them to be numbers in order to multiply and add them. So let&#39;s replace each work with a number. . To do this we will get a unique list of all of the words, then assign a number to each word. . from fastai2.text.all import * vocab = L(tokens).unique() . word2idx = {w:i for i,w in enumerate(vocab)} . We have 20165 words, but only 1545 unique words. Each of those assigned a number in a dictionary. . len(ex),len(vocab) . (21065, 1545) . We can see that each word gets a number. . list(word2idx.items())[:5] . [(&#39;President&#39;, 0), (&#39;Trump&#39;, 1), (&#39;told&#39;, 2), (&#39;of&#39;, 3), (&#39;“hard&#39;, 4)] . Now all we have to do is replace our tokens with the numbers in our word2idx dictionary. Lets take a look at 10 words near the end of our aricle and see what itt looks like as tokens as well as numbers . nums = L(word2idx[i] for i in tokens) nums[3000:3010],L(tokens[3000:3010]) . ((#10) [1359,24,17,943,1360,1361,388,331,77,1362], (#10) [&#39;linked&#39;,&#39;to&#39;,&#39;the&#39;,&#39;coronavirus.&#39;,&#39;Only&#39;,&#39;Italy&#39;,&#39;has&#39;,&#39;recorded&#39;,&#39;a&#39;,&#39;worse&#39;]) . Next Steps . While this is the idea behind tokenization, there are many things that were not considered. Here are some other ideas to consider when choosing a tokenization approach. . What holds meaning other than words in english that we could make into tokens? What about punctuation or a comma? What about the beginning of a sentance or paragraph? | Should &#39;words&#39; and &#39;word&#39; be tokenized as 2 seperate words? Or could we assign &#39;word&#39; and &#39;s&#39; as the tokens because the base of the word has the same meaning? | Is there a better way to break up a sentance other than by words? What if it were just based on common sentance strings. Maybe &#39;of a&#39; could be 1 token rather than 2. could &#39; dis&#39; or &#39;ing&#39; be tokens that can be combined with many different words? | .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/2020/08/19/Tokenization.html",
            "relUrl": "/neural%20networks/2020/08/19/Tokenization.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Mixup Deep Dive",
            "content": "Intro . In this post we are going to dive into what Mixup is. Mixup is a very powerful data augmentation tool that is super helpful tool to have in your toolbox, especially when you don&#39;t have enough data and are overfitting. . The goal of this post is not to show you the intricacies of training a model using mixup - that will be reserved for a future post. The goal of this post is to communicate an intuitive understanding of what mixup is and why it works. If you don&#39;t know what the tool is, it&#39;s impossible to have good intuition on how and when to use it. . We will be using the Pets Dataset to demonstrate this. . Bonus Challenge:As you go through each step, think about what other kinds of data you may be able to apply these concepts to. Could you apply these transformations to NLP Embeddings? Could you apply these transformations to Tabular Data? . Setup . Get Libraries/Data . from fastai2.data.external import * from fastai2.vision.all import * from PIL import Image import matplotlib.pyplot as plt from pylab import rcParams from functools import partial,update_wrapper . seed = 42 # Download and get path for dataseet path = untar_data(URLs.PETS) #Sample dataset from fastai2 path.ls() . (#2) [Path(&#39;/home/isaacflath/.fastai/data/oxford-iiit-pet/annotations&#39;),Path(&#39;/home/isaacflath/.fastai/data/oxford-iiit-pet/images&#39;)] . Helper Functions . def plot_images(imgs): rcParams[&#39;figure.figsize&#39;] = 10, 20 imgs_len = len(imgs) for x in range(0,imgs_len): plt.subplot(1,imgs_len,x+1) plt.imshow(imgs[x]) . Data Setup . Data Blocks and data loaders are convenient ways that fastai helps us manage an load data. There is a lot going on in DataBlock so I am going to break it down piece by piece. . DataBlock . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=224) ) . dataloader . The dataloader is what we will actually interact with. In the DataBlock we defined lots of things we need to do to get and transform images, but not where to get them from. We define that in the dataloade . dls = pets.dataloaders(path/&quot;images&quot;) . Mixup Explained . So what is Mixup really? To undestand that, we are first going to look at a couple pictures and understand what a Neural network would normally see an image then use the same images and apply mixup after that. To say that another way. we want to understand the inputs and the outputs. To say that another way, we want to know our xs and our ys . x: No Mixup . Let&#39;s use 2 images as an example. I have plotted them below. . im1 = tensor(Image.open((path/&#39;images&#39;).ls()[8]).resize((500,371))).float()/255; im2 = tensor(Image.open((path/&#39;images&#39;).ls()[6]).resize((500,371))).float()/255; . plot_images([im1,im2]) . Great, so the inputs are the pictures. What are the outputs? Well the output is going to be what breed they are. Let&#39;s see what breed they are. . (path/&#39;images&#39;).ls()[8],(path/&#39;images&#39;).ls()[6] . (Path(&#39;/home/isaacflath/.fastai/data/oxford-iiit-pet/images/leonberger_20.jpg&#39;), Path(&#39;/home/isaacflath/.fastai/data/oxford-iiit-pet/images/Ragdoll_119.jpg&#39;)) . Ok we can see in the file name that the dog is a leonberger and the cat is a ragdoll. So now we need to translate that into the one-hot encoded matrix for our model to predict. Looking at dls.vocab gives us all the class names. . dls.vocab . (#37) [&#39;Abyssinian&#39;,&#39;Bengal&#39;,&#39;Birman&#39;,&#39;Bombay&#39;,&#39;British_Shorthair&#39;,&#39;Egyptian_Mau&#39;,&#39;Maine_Coon&#39;,&#39;Persian&#39;,&#39;Ragdoll&#39;,&#39;Russian_Blue&#39;...] . y: No Mixup . Let&#39;s define y for these 2 images. In a normal scenario, we have 1 column per class. When looking at the vocab above we saw that there were 37 classes. All of them will be 0 except the target. . Let&#39;s start by figuring out which column is the target (ie leonberger and ragdoll). Then we just need a tensor of length 37 that is all zeros except that position which will be a 1. . list(dls.vocab).index(&#39;leonberger&#39;),list(dls.vocab).index(&#39;Ragdoll&#39;) . (25, 8) . y_leonberger = tensor([0]*25+[1]+[0]*(37-26)) # 37 classes long, all 0 except position 8 which represents Ragdoll and is 1 y_Ragdoll = tensor([0]*8+[1]+[0]*(37-9)) print(y_leonberger) print(y_Ragdoll) . tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . Great! We have our images that go in, and our output we want to predict. This is what a normal neural network is going to try to predict. Let&#39;s see whats different if we use these 2 images with the Mixup data Augmentation . x: Yes Mixup . For the images, we are going to apply an augmentation. What Mixup does, it really mixing up 2 images together. The name is really exactly what it is doing. Let&#39;s apply mixup to an image and see what I mean. . Let&#39;s take a mix of the 2 images. We will take 40% of the first image, and 60% of the second image and plot them. We are doing this by multiplying the actual pixel values. . For example, If the pixel 1 value from image 1 .4 + pixel 1 value from image 2 .6 and that will equal pixel 1 value in my new image. Take a look at the third image and you can see it really does have a bit of each image in there. . im_mixup = im1*.6+im2*.4 . plot_images([im1,im2,im_mixup]) . y: Yes Mixup . So now we have our new augmented image with mixup. Clearly it&#39;s not really fair to call it 100% of either class. In fact it&#39;s 60% of one class and 40% of the other. So how does our Y work? . Well, we already have our ys when they are 100% of either class, so lets just take 60% of one + 40% of the other exactly like we did for our images. That should give us an appropriate label. . y_leonberger = tensor([0]*25+[1]+[0]*(37-26)) # 37 classes long, all 0 except position 8 which represents Ragdoll and is 1 y_Ragdoll = tensor([0]*8+[1]+[0]*(37-9)) y_mixup = y_leonberger*.6+y_Ragdoll*.4 print(y_leonberger) print(y_Ragdoll) print(y_mixup) . tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) . What weights? . Here I took 60% of one image and 40% of the other. In reality you could to a 90/10 split. Or a 99/1 split. Or a 50/50 split. . I picked relatively close weights so it&#39;s easy to see, but these weights are something to play with and tune when building your model. . FastAI mixup . Applying the basic Mixup in fastai is super easy. Here&#39;s how you can create a CNN using Mixup. . learn = cnn_learner(dls,resnet34,cbs=MixUp) .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/2020/08/11/Mixup-Deep-Dive.html",
            "relUrl": "/neural%20networks/2020/08/11/Mixup-Deep-Dive.html",
            "date": " • Aug 11, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "MultiCat Image Classification",
            "content": "Intro . In this post we will be creating a multi-category classifier. This means one image may have 1, 2, or more labels and we want to identify and label them all correctly. Let&#39;s get started! . Get Data . from fastai2.vision.all import * import pandas as pd from functools import partial warnings.filterwarnings(&quot;ignore&quot;) path = untar_data(URLs.PASCAL_2007) . train = pd.read_csv(path/&#39;train.csv&#39;) test = pd.read_csv(path/&#39;test.csv&#39;) . dls = ImageDataLoaders.from_df(train, path, folder=&#39;train&#39;, valid_col=&#39;is_valid&#39;, label_delim=&#39; &#39;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224)) . As you can see from the batch below, images have different number of labels depending on what is in the picture. . dls.show_batch() . Modeling . Loss Function . With this multi-category, we use binary cross-entropy instead of normal cross-entropy. This is because normal cross entropy uses a softmax, which makes all probabilities add to 1. That makes a ton of sense when we only have 1 class, but with multiple it doesn&#39;t. Softmax in the loss function makes it much harder to train as it forces class probabilities to sum to 1, when there may in fact have multiple classes (or no classes). . def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() . Training . We will use a resnet34 architecture, using a pretrained model. The first thing we will want to do is freeze the model and train the last layer, as that is not transferrable from the pretrained model. . learn = cnn_learner(dls,arch = resnet34,metrics = accuracy_multi,loss_func=binary_cross_entropy) learn.freeze() learn.lr_find() . SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.033113110810518265) . learn.fit_one_cycle(4,5e-2) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.445584 | 0.540536 | 0.917350 | 00:13 | . 1 | 0.284971 | 0.207589 | 0.931175 | 00:12 | . 2 | 0.201342 | 0.131654 | 0.950159 | 00:13 | . 3 | 0.156355 | 0.116606 | 0.957350 | 00:13 | . Next we will unfreeze our model, meaning we will train all layers in our model. You will notice I am using discriminative learning rates, which is key. Early layers should not have weights adjusted as fast as later laters. . Note: Discriminative learning rates means we use a smaller learning rate for earlier layers in the model. This is what slice(1e-4/100,1e-4) is doing. . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=9.999999747378752e-06) . learn.fit_one_cycle(4,slice(1e-4/100,1e-4)) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.118245 | 0.115170 | 0.957570 | 00:13 | . 1 | 0.117186 | 0.113712 | 0.958446 | 00:13 | . 2 | 0.114177 | 0.113257 | 0.958048 | 00:13 | . 3 | 0.111450 | 0.113189 | 0.958207 | 00:13 | . For our final steps, I will train the last couple of layers with the earlier layers frozen . learn.freeze_to(-2) learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=9.12010818865383e-07) . learn.fit_one_cycle(2, 1e-5) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.110856 | 0.112854 | 0.958506 | 00:13 | . 1 | 0.110439 | 0.112414 | 0.958566 | 00:13 | . learn.freeze_to(-1) learn.lr_find() . SuggestedLRs(lr_min=7.585775847473997e-08, lr_steep=6.918309736647643e-06) . learn.fit_one_cycle(3,5e-3) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.110587 | 0.116988 | 0.957769 | 00:12 | . 1 | 0.107045 | 0.111408 | 0.959621 | 00:13 | . 2 | 0.099724 | 0.105969 | 0.961375 | 00:13 | . Just over 96% accuracy - Not bad! When we look at the labels over the images, we can see it actually did a pretty good job identiffying what is in the images/ . learn.show_results() . Accuracy Threshhold . One last thing to consider with multi-category is the accuracy threshhold. If a class score is &gt; 0.5, should we consider that class a match? Or should it require a score of &gt; 0.6? I reccomend playing with the threshold to see what works for your application. You can print many metrics during training by passing a list of metrics to the metrics argument. . learn = cnn_learner(dls,arch = resnet34,metrics = [partial(accuracy_multi,thresh=x) for x in np.array(range(1,10))/10]) learn.fine_tune(1) . epoch train_loss valid_loss accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi time . 0 | 0.910183 | 0.608041 | 0.133367 | 0.254004 | 0.405359 | 0.555877 | 0.688267 | 0.793626 | 0.873327 | 0.928267 | 0.956932 | 00:12 | . epoch train_loss valid_loss accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi accuracy_multi time . 0 | 0.663072 | 0.493378 | 0.135498 | 0.284940 | 0.479542 | 0.656992 | 0.791195 | 0.879163 | 0.930239 | 0.954681 | 0.963386 | 00:13 | .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/image%20classification/2020/08/01/Multi-Cat-Image-Classifier.html",
            "relUrl": "/neural%20networks/image%20classification/2020/08/01/Multi-Cat-Image-Classifier.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Heirarchical Loss Function",
            "content": "Intro . For this post, I am going to make a modification to the loss function. The goal will be to weight losses differently. Why should we treat all classes the same in our loss function if they aren&#39;t? . In this blog we are going to do an image classification to take pet pictures and predict the breed. Normally, each class is treated the same when calculating the loss function. In this post I will explore a change to take into account weighting classes differently. For example, If a picture is a maincoon, predicting a sphynx (wrong type of cat) is less egregious of an error than predicting pitbull. I want my loss function to reflect this. . If you think about it intuitively when we teach children - if we show a child an apple and the chile thinks it&#39;s a peach - We probably wouldn&#39;t just tell the child he is wrong. We would probably tell him he is close, but that it&#39;s a different fruit and really it&#39;s an apple. Neural networks benefit from having this extra piece of information which is what this post will look to do in a loss function. . I am going to skim over loading the data and training the model, feel free to see my Fine Grain Image Classifier post from Jun-27-2020 if you want more detail on those aspect. . Setup . Library Import and Dataset Download . from fastai2.vision.all import * import matplotlib.pyplot as plt from pylab import rcParams from torch.nn.functional import nll_loss,log_softmax seed = 42 # Download and get path for dataseet path = untar_data(URLs.PETS) #Sample dataset from fastai2 img = (path/&#39;images&#39;).ls()[0] . Data Prep . Load the Data in a data block, take a look at the classes and scale the images down a bit so the problem isn&#39;t too easy. . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=56) ) . dls = pets.dataloaders(path/&quot;images&quot;,bs=64) dls.show_batch() . /Users/isaac.flath/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Classes . We can see in our dls object the class names in the order our predictions and labels will be in. I went through and found that the first 12 classes are all cats, and the last 25 classes are all dogs. This will be important as we are creating the heirarchy at that level. . dls.vocab . (#37) [&#39;Abyssinian&#39;,&#39;Bengal&#39;,&#39;Birman&#39;,&#39;Bombay&#39;,&#39;British_Shorthair&#39;,&#39;Egyptian_Mau&#39;,&#39;Maine_Coon&#39;,&#39;Persian&#39;,&#39;Ragdoll&#39;,&#39;Russian_Blue&#39;...] . pet_type = [&#39;cat&#39;]*12+[&#39;dog&#39;]*25 . Custom Stuff . Metrics . I need to measure the accuracy at the 2 levels in the heirarchy (breed and species levels). The goal is that I can impact them differently by modifying the weights in my loss function. Ultimately what I care about is the accuracy not whatever the loss function says, so I need to define the metrics for each level. These will be the metrics to determine is this approach is working or not. . def accuracy_breed(inp, targ, axis=-1): # Compute Accuracy as normal - function straight from fastai library pred,targ = flatten_check(inp.argmax(dim=axis), targ) return (pred == targ).float().mean() def accuracy_species(inp, targ, axis=-1): #convert input (35 classes of breeds) into new_input (cat or dog) temp = [torch.argmax(x) for x in inp] new_inp = tensor([(x &gt; 11).int() for x in temp]) # conver target (35 classes of breeds) into new_target (cat or dog) new_targ = tensor([(x &gt; 11).int() for x in targ]) return (new_inp == new_targ).float().mean() . Loss Function . The loss functions are what will actually be optimized. The loss function is what the model will calculate the gradients off of to update our weights. I am doing a linear combination of cross entropy loss at the 2 levels of the heirarchy. I have a weight $w$ which I can change to change the proportion of these. use a weight to change the proportion of which level I use. . def cross_entropy_species(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): # softmax to convert scores to probabilities input_p = torch.softmax(input,dim=-1) return nll_loss(torch.log(input_p), target, None, None, ignore_index, None, reduction) def cross_entropy_breed(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): # softmax to convert scores to probabilities input_p = torch.softmax(input,dim=1) # changes from many classes down to 2 classes for both input and target # Sum the probabilities for all the cat breeds to get probability it&#39;s a cat. Same for dog cats = torch.sum(input_p[:,0:12],dim=1).view(input_p.shape[0],1) dogs = torch.sum(input_p[:,12:37],dim=1).view(input_p.shape[0],1) # format new inputs and new targets for 2 classes new_input = torch.cat([cats,dogs],-1) new_target = TensorCategory((target &gt; 11).long(),device=&#39;cuda:0&#39;) # Finish calculation for cross-entropy using new inputs and targets return nll_loss(torch.log(new_input), new_target, None, None, ignore_index, None, reduction) def final_loss(input, target, w=1, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;): ce_species = cross_entropy_species(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;) ce_breed = cross_entropy_breed(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;) # Linear combination of the cross-entropy scores at the 2 levels in heirarchy. return w*ce_species+(1-w)*ce_breed . Experiments . Here I wanted to run tests at different weights to see how they performed. I ran one with 3 epochs and one with 10 epochs. I did multiple runs at the same weights so I could see if I was getting stuck in different local minimums due to the change. . I found that in general, this can help some. Using a slight amount of weight toward the species helps the neural network optimize better. This makes sense as we are giving it some more information. We are telling the neural network more than right class vs wrong class. We are now giving it a third option (right class vs wrong class but close vs totally wrong class). Having this extra piece of information is helpful for training your model and is what we are building into the loss function below. . from functools import partial results = pd.DataFrame() id_num = 1 # weights between 0 and 1, incrementing by 0.05 weights = np.array(range(0,21))/20 for w in weights: for i in range(0,5): print(w) # Take my loss function and sets the default weight to my new weight tmp_loss = partial(final_loss,w=w) #Create basic CNN with smart defaults learn = cnn_learner(dls, resnet18, metrics=[accuracy_breed,accuracy_species], pretrained=True, loss_func = tmp_loss) # Fine tune take pretrained model, trains the final layers, unfreezes, then trains the full model learn.fine_tune(3) # Store the results df = pd.DataFrame(learn.recorder.values) df.columns = learn.recorder.metric_names[1:-1] df[&#39;w&#39;] = w df[&#39;id&#39;] = id_num results = results.append(df) id_num = id_num + 1 learn = None torch.cuda.empty_cache() . results.to_csv(&#39;results.csv&#39;) . from functools import partial weights = np.array(range(0,11))/10 results = pd.DataFrame() id_num = 1 for w in weights: for i in range(0,5): print(w) tmp_loss = partial(final_loss,w=w) learn = cnn_learner(dls, resnet18, metrics=[accuracy_breed,accuracy_species], pretrained=True, loss_func = tmp_loss) learn.fine_tune(10) df = pd.DataFrame(learn.recorder.values) df.columns = learn.recorder.metric_names[1:-1] df[&#39;w&#39;] = w df[&#39;id&#39;] = id_num results = results.append(df) id_num = id_num + 1 learn = None torch.cuda.empty_cache() . results.to_csv(&#39;results2.csv&#39;) . Results . Overall both over 3 and 10 epochs showed the same thing, which is weighting the Breed level (higher level then our task) between 20% and 40% seems to improve performance for both breed as well as species. I suspect that this should be treated as another thing to tune and not something you can arbitrarily set for all datasets. . This seems to indicate that when you have an obvious heirarchy in your data, using this methodology (and possibly extending it for multi-level heirarchies) can help overall performance of the model. Graphs of results below . def graph_results(df): rcParams[&#39;figure.figsize&#39;] = 20, 10 graph1 = df[[&#39;accuracy_breed&#39;,&#39;accuracy_species&#39;,&#39;w&#39;]].groupby(&#39;w&#39;).max().reset_index() graph2 = df[df.epochs == max(df.epochs)][[&#39;accuracy_breed&#39;,&#39;accuracy_species&#39;,&#39;w&#39;]].groupby(&#39;w&#39;).mean().reset_index() plt.suptitle(&quot;Weight * BreedLoss + (1 - Wieght) * SpeciesLoss&quot;, fontsize=14) plt.subplot(2, 2, 1) plt.plot(&#39;w&#39;, &#39;accuracy_breed&#39;, data=graph1, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=12, color=&#39;skyblue&#39;, linewidth=4) plt.title(&#39;Max Accuracy (Breed)&#39;) plt.xlabel(&#39;Weight&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.subplot(2, 2, 2) plt.plot(&#39;w&#39;, &#39;accuracy_species&#39;, data=graph1, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=12, color=&#39;skyblue&#39;, linewidth=4) plt.title(&#39;Max Accuracy (Species)&#39;) plt.xlabel(&#39;Weight&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.subplot(2, 2, 3) plt.plot(&#39;w&#39;, &#39;accuracy_breed&#39;, data=graph2, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=12, color=&#39;skyblue&#39;, linewidth=4) plt.title(&#39;Avg Final Epoch Accuracy (Breed)&#39;) plt.xlabel(&#39;Weight&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.subplot(2, 2, 4) plt.plot(&#39;w&#39;, &#39;accuracy_species&#39;, data=graph2, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=12, color=&#39;skyblue&#39;, linewidth=4) plt.title(&#39;Avg Final Epoch Accuracy (Species)&#39;) plt.xlabel(&#39;Weight&#39;) plt.ylabel(&#39;Accuracy&#39;) . . 3 epochs . df = pd.read_csv(&quot;results.csv&quot;, skiprows=1, names = [&#39;epochs&#39;, &#39;train_loss&#39;, &#39;valid_loss&#39;, &#39;accuracy_breed&#39;,&#39;accuracy_species&#39;, &#39;w&#39;, &#39;id&#39;]) graph_results(df) . 10 epochs . df = pd.read_csv(&quot;results2.csv&quot;, skiprows=1, names = [&#39;epochs&#39;, &#39;train_loss&#39;, &#39;valid_loss&#39;, &#39;accuracy_breed&#39;,&#39;accuracy_species&#39;, &#39;w&#39;, &#39;id&#39;]) graph_results(df) .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/image%20classification/2020/07/21/Heirarchical-Loss.html",
            "relUrl": "/neural%20networks/image%20classification/2020/07/21/Heirarchical-Loss.html",
            "date": " • Jul 21, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Terraform Basics Part 1",
            "content": "Intro . In this blog we are going to take a look at Terraform. Terraform is tool that enables you to manage your cloud infrastructure in an auditable and programmatic way. In this initial post we are going to go through the most basic application, creating and spinning up an EC2 instance. Future posts will build on this for other applications . Setup . You will need to install Terraform in your terminal. I am not going to cover installation in this post, but you can check out terraform.io and install if from there. . What is Terraform . Many people think Terraform is just a bunch of config files, but rather than thinking of it as config files it is more helpful to think of it as a programming language. You can define lists, maps (similar to python dictionaries), strings, and perform actions using values in those variables. . Variables . For example, Here&#39;s how I could define a map, where I may store different ami&#39;s for different region. . variable &quot;AMIS&quot; { type = map(string) default = { us-east-1 = &quot;ami-0ac80df6eff0e70b5&quot;, us-east-2 = &quot;ami-31432143214213435&quot; } } . Then in terrarorm console get that value using: . var.AMIS.us-east-1 . So this is how we can get and store different configuration options. But Terraform is not just variables. You also have ways of running your code to deploy things. For example terraform plan shows you what it would do if you deployed. terraform apply deploys your options and terraform destroy destroys all your infrastructure defined. . But what is it applying? So far all we have done is define a variable. Let&#39;s define an EC2 instance. . Actions . resource &quot;aws_instance&quot; &quot;web&quot; { ami = &quot;${lookup(var.AMIS, var.AWS_REGION)}&quot; instance_type = &quot;t2.micro&quot; } . From the above we can see we have most of what we need to launch an EC2 instance. We define an AMI by looking up the AWS_Region variable from the AMIS variable we defined above. . Full EC2 setup . You may have noticed, there&#39;s a few things missing from the above. For example, how does it connect to AWS? How do you give it the credentials without putting them in github? We will create 4 files that gets a good organized setup for the full thing. . instance.tf: This will be the actual command that defined the EC2 instance we want to create | vars.tf: We can define all the variables we will need here | provider.tf: This will have the command needed to connect to our AWS account | terraform.tfvars: This will store our aws credentials | . vars.tf . First we will define all the variables we need. You may notice I do not put any values in the the credentials. This is on purpose - best practice is to keep access keys out of github. We will define these in a different file. . You can see we also set a default AWS_REGION as well as AMIs to use. I am only setting 1, but you can make as complicated of a map variable as you want with multiple regions, different types of instances (maybe a Deep learning ami, and a random forest ami, and a generally compute ami). . variable &quot;AWS_ACCESS_KEY&quot; {} variable &quot;AWS_SECRET_KEY&quot; {} variable &quot;AWS_REGION&quot; { default = &quot;us-east-1&quot; } variable &quot;AMIS&quot; { type = map(string) default = { us-east-1 = &quot;ami-0ac80df6eff0e70b5&quot; } } . terraform.tfvars . This is a pretty simple file where we define the values for our credentials. The important thing is to add this file to the gitignore so that it does not get put in the github reposity (for security reason). It just holds your credentials, for example it may be. . AWS_ACCESS_KEY = &quot;ABCDE12345FGHI6789&quot; AWS_SECRET_KEY = &quot;abc123def456+789ghi983klm . provider.tf . Next. we will connect to aws using the keys we defined in the terraform.tfvars and vars.tf files. The provider.tf file looks like this: . provider &quot;aws&quot; { access_key = &quot;${var.AWS_ACCESS_KEY}&quot; secret_key = &quot;${var.AWS_SECRET_KEY}&quot; region = &quot;${var.AWS_REGION}&quot; } . instance.tf . Now finally we are ready to launch our instance. We have our region, our credentials, and are connected to aws. Naturally we could have 100 different instances defined in this same way for different regions and instance types and it will automatically create them all, but for now we will just create 1 and stay within the free-tier. . resource &quot;aws_instance&quot; &quot;web&quot; { ami = &quot;${lookup(var.AMIS, var.AWS_REGION)}&quot; instance_type = &quot;t2.micro&quot; } . You will notice I am using the lookup funciton. This is looking up the var.AWS_REGION from the var.AMIS variable. We set the default region to us-east-1 in the vars.tf file. And it uses that to lookup that key in the AMIS variable, which is a map of strings. . Apply the Changes . Now all you need to do is run terraform apply in your terminal and your t2.micro ec2 instance will automatically be created! If it is a new folder for terraform, you will need to run terraform init in the folder before it will work. .",
            "url": "https://isaac-flath.github.io/fastblog/aws/infrastructure/terraform/devops/2020/07/15/Terraform-Part-1.html",
            "relUrl": "/aws/infrastructure/terraform/devops/2020/07/15/Terraform-Part-1.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Fine Grain Image Classification",
            "content": "Intro . In this blog we are going to do an image classification to take dog pictures and predict the breed. This is considered &#39;fine grain&#39; because the difference between classes is fairly minimal. Classifying between breeds of dogs is fine grain, classifying whether something is a dog or a cup is not. . To do this we are going to use fastaiv2, which is the new version of fastai that will come out in July. The purpose of this post is to introduce a few key concepts that will be useful as we move into harder problems . Setup . Library Import and Dataset Download . from fastai2.vision.all import * seed = 42 # Download and get path for dataseet path = untar_data(URLs.PETS) #Sample dataset from fastai2 path.ls() . (#2) [Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images&#39;),Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/annotations&#39;)] . img = (path/&#39;images&#39;).ls()[0] img . Path(&#39;/home/ubuntu/.fastai/data/oxford-iiit-pet/images/British_Shorthair_154.jpg&#39;) . Data Setup . Data Blocks and data loaders are convenient ways that fastai helps us manage an load data. There is a lot going on in DataBlock so I am going to break it down piece by piece. . DataBlock . Blocks: What is our data? x is our images (ImageBlock) and y is our categories (CategoryBlock). In this case each image will get a dog breed as the category. . | get_items: How do we get our data (x)? We use the predefined get_image_files for this, though we can give it something custom if needed. . | splitter: How should we create the validation set? The splitter splits our data into test and validation sets. The default 20% validation set is just fine, but we define a seed so it is reproducable. . | get_y: How do we get our dependent variable (y)? In this care we are going to get it from the file name. With using_attr, we can apply a function to an attribute of the file (name). So here we are using regex on the file name to get y. . | item_tfms: What transformations do we need to do before packing things up to be sent to the GPU? In this case resizing it to 460. . | batch_tfms: What transformations do we want to do in batches on the GPU? There are many default transforms, and we are specifying a few ourselves (min_scale and size). . | . pets = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter= RandomSplitter(valid_pct = 0.2, seed=seed), get_y= using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(min_scale = 0.9,size=224) ) . dataloader . The dataloader is what we will actually interact with. In the DataBlock we defined lots of things we need to do to get and transform images, but not where to get them from. We define that in the dataloade . dls = pets.dataloaders(path/&quot;images&quot;) . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . dls.show_batch() . Training the Model . Get a basic model working . In 2 lines of code I am going to create and train a basic model. There&#39;s a couple things to note: . I am using the dls from the previous step. This is where we defined how to load the data, how to label it, data augmentation, training/validation split, etc. | I can also pass standard architectures. &quot;Resnet&quot; is a common architecture for image classification. &quot;34&quot; signifies that it has 34 layers. If you wish to understand what a layer is, please check out the Image Classifier Basics blog posts that build a simple 1 layer net. | I set a metric, but use the default loss metric. . Note: Loss is what the model uses to train on. Error rate is just for our reference. Accuracy and error rates make very poor choices for loss function because they have either 0 or infinite slope, so calculating the gradient/path value/derivative is not meaningful. This is a prime example of why good functions for computers to understand what&#8217;s going on and good functions for peope to understand what&#8217;s going on can be very different things. | learn = cnn_learner(dls, resnet34, metrics=error_rate) . Next we are going to fine tune the model. . If we were starting from scratch when training a model we will train every layer. Fine tuning is about training the final layer(s) and leaving the rest intact. Previous layers were set using weights via transfer learning. What this means is that a model was trained to be able to detect and classify a bunch of different objects. The earlier layers of the neural network detect things that are common to lots of images (ie circles, edges, corners, etc.). These don&#39;t need to change much generally. The last layer is predicting the specific thing, in our case pet breeds. This is what we need to change. . Note: A fun thought expirament on understanding why transfer learning works is to think about elements that you need to identify basically everything that you take for granted, and try to imagininng the world and objects around you if you were missing some basic concepts. For example, what if you did not have the ability to tell the diference vs a surface and and edge? Or what if you couldn&#8217;t tell the difference between something that is straight and curved? Or what if circular shaped, square shaped objects, and traingular shaped objects all looked the same to you? What if you could not recognize any pattern - what would you think of a tile floor if you had no ability to comprehend that the tiles are a pattern? How could you function if you couldn&#8217;t see corners? . learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.519997 | 0.311546 | 0.106901 | 01:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.472248 | 0.341488 | 0.105548 | 01:23 | . 1 | 0.374747 | 0.241689 | 0.076455 | 01:22 | . 2 | 0.230108 | 0.202105 | 0.066306 | 01:22 | . learn.recorder.plot_loss() . We see out validation loss improves significantly with our error rate. We will use this error rate at our baseline. . Note: A common misconception is when training loss is lower than validation loss. This is not the case. You cannot be overfitting as unless your validation scores get worse. In a well tuned model, training loss will almost always be lower than validation loss. Let&#39;s take a look at our model, then see if we can improve it! . Look at results . First lets look at some pictures. I think it&#39;s always good to actually loook at some prediciton the model is making. . learn.show_results() . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Many times in classification we look at a confusion matrix. As you can see, when we start having a lot of classes, it is really hard to make anything meaningful out of the confusion matrix. There&#39;s just too many classes. . Next, we will take a look at some more specif data. Let&#39;s start with a high level confusion matrix. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Instead, we look at top losses to see where our model was most wrong. We also look at the &#39;most confused&#39; function whichprints which classes it gets confused on most often. . interp.plot_top_losses(9, figsize=(15,10)) . interp.most_confused(min_val=3) . [(&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 4), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 4), (&#39;beagle&#39;, &#39;basset_hound&#39;, 4), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 3), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 3), (&#39;havanese&#39;, &#39;yorkshire_terrier&#39;, 3)] . Make a Better Model . Now that we have a baseline using the defaults, let&#39;s see what we can do to improve it. We will talk about a few main topics. . Freezing and Unfreezing for training | Learning Rate Finder | Discriminate learning rrate | Architecture | . Learning Rate Finder . The Learning Rate Finder is very important because setting a good learning rate can make or break a model. We will use it multiple times, and it will come up in essentially every deep learning project. It is good to spend some time to understand what it is showing and expirament. . The learning rate finder (lr_find) gives us 3 things: . lr_min: This is the learning rate that gives us the minimum loss in our graph. 1 common rule of thumb is to divide this by 10 and use that as your learning rate. | lr_steep: This is the steepest point on our graph. Another rule of thumb is to make this your learning rate. Interestingly enough, these 2 rules of thumb often end up with very similar results. | graph: The graph is really what i use when determining a learning rate. At the beginning of the graph we see very little reduction in loss. At the end of the graph we see loss spiking. Obviously nether of those are good. In reality we want our learning rate to be somewhere in the middle-ish of that steep decline. This is in line with our 2 rule of thumbs. | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 3.63e-03 . Now that we have our graph, let&#39;s train our model with this learning rate. . What&#39;s the difference between fine tune and fit one cycle? . learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.108581 | 0.351311 | 0.108931 | 01:03 | . 1 | 0.510538 | 0.250211 | 0.078484 | 01:04 | . 2 | 0.329532 | 0.207013 | 0.063599 | 01:04 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . Unfreezing . Unfreezing a model is about training the full model. We spoke earlier about fine_tune only training that last layers. This is a great start and we want to train the last layers more than the early layers, but we still want to train the early layers. Unfreeze allows us to do this. Now that we unfroze the model and are going to train the model more, we will need to do our learning rate finder to pick a good learning rate again. . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=1.584893179824576e-05, lr_steep=5.754399353463668e-06) . Discriminative Learning Rates . Discriminative Learning Rates means that we are going to use a different learning rate for each layer. We have previously really been training the final layers of the model. We are now ready to udate all the weights and biases, including the ones that were set by our transfer learning. While we do want to train the whole model, we don&#39;t want to train it at the same speed. We want more changes in the end (ie figuring out exactly which breed it is) and less changes in the early layers (ie identifying lines). This isn&#39;t so different than how people work. Learning a new thing (ie a type of dog breed) is much easier than improving my fundamental understanding of the world around me. . We will fit this now for 6 epochs. The first layers will have 1e-6 learning rate. The final layers will have 1e-4. Middle layers will be between those 2 numbers. We can see we get down to just under a 6% error rate. 94% accuracy - not bad! . learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 0.260102 | 0.199174 | 0.064953 | 01:22 | . 1 | 0.243101 | 0.191330 | 0.062246 | 01:23 | . 2 | 0.211765 | 0.187266 | 0.059540 | 01:22 | . 3 | 0.188451 | 0.184359 | 0.063599 | 01:22 | . 4 | 0.181025 | 0.180899 | 0.060217 | 01:22 | . 5 | 0.175231 | 0.181373 | 0.059540 | 01:22 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . learn.recorder.plot_loss() . Architecture . Another simple lever is to increase the number of layers in the neural network. The more layers, the higher capacity to learn from data. This also means the higher capacity that you overfit, so more layers does not always mean better! . Note: Overfitting is defined as continued training increases your validation loss. Many people feel that overfitting is when your training loss is less than validation loss, but in reality almost every well tuned model will have a lower training loss than validation loss. If the prediction accuracy on things the model haven&#8217;t seen keeps getting better, how can you be overfitting? Lets see what the impact can be from using a different architecture. A few comments: . We were using resnet34 before, and now we are using resnet101. the number represents the number of layers | We added a to_fp16. We are actually decreasing the precision of our calculations a bit for 2 reasons Faster to train | Helps combat overfitting | . | We are using fine_tune with freeze_epochs. All we are doing is training with the earlier layers frozen for 3 epochs, then training unfreezing, then training for 6 epochs. Take a look through the code of the fine_tune method at the end and you will see that I am not summarizing, that&#39;s just exactly what it is doing. (self.freeze -&gt; self.fit_one_cycle -&gt; divide learning rate -&gt; self.unfreeze -&gt; self.fit_one_cycle). | . With the resnet101 we are down to less that 5% error rate. Even better! . learn = cnn_learner(dls, resnet101, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.371693 | 0.247931 | 0.079161 | 02:43 | . 1 | 0.551731 | 0.246441 | 0.077131 | 02:39 | . 2 | 0.371089 | 0.233275 | 0.080514 | 02:39 | . /home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(&#34;The default behavior for interpolate/upsample with float scale_factor will change &#34; . epoch train_loss valid_loss error_rate time . 0 | 0.258692 | 0.246271 | 0.081191 | 03:36 | . 1 | 0.310040 | 0.321208 | 0.091340 | 03:33 | . 2 | 0.266156 | 0.289912 | 0.081867 | 03:34 | . 3 | 0.149828 | 0.213094 | 0.060893 | 03:34 | . 4 | 0.082449 | 0.183762 | 0.056834 | 03:35 | . 5 | 0.048759 | 0.173572 | 0.049391 | 03:34 | . learn.recorder.plot_loss() . ??learn.fine_tune . Signature: learn.fine_tune( epochs, base_lr=0.002, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, lr_max=None, div_final=100000.0, wd=None, moms=None, cbs=None, reset_opt=False, ) Source: @patch @log_args(but_as=Learner.fit) @delegates(Learner.fit_one_cycle) def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100, pct_start=0.3, div=5.0, **kwargs): &#34;Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR&#34; self.freeze() self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs) base_lr /= 2 self.unfreeze() self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs) File: ~/anaconda3/lib/python3.7/site-packages/fastai2/callback/schedule.py Type: method .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/image%20classification/2020/06/27/Fine-Grain-Image-Classifier.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/27/Fine-Grain-Image-Classifier.html",
            "date": " • Jun 27, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Connecting Jupyter to EC2",
            "content": "Intro . The goal of this is to run a jupyter notebook as if it were locally but have all of it running in the backend. I have tried sagemaker and other packaged products, and I prefer just using Jupyter-lab. This guide is how I connect. . This is mostly a reference guide to refer back to until you memorize the steps. If this is your first time using AWS, EC2, or you aren&#39;t already fairly familiar with the process then I recommend checking out a guide that explains things a bit more. This is meant to be be a good place to use as a reminder of what to do while you do it the first 10 or 20 times, rather than a step-by-step guide on how to do it. . A great place (and what I used initially) for getting instructions to do this for the first time would be: https://fzr72725.github.io/2018/01/14/How-to-Connect-to-Jupyter-Notebook-Server-on-AWS-EC2-from-Your-Local-Machine.html . Launch an EC2 Instance . First, launch an EC2 instance. I use p2.xlarge for almost everything. You will be prompted to create or use an existing key pair. You will need this private key, so download it and store it somewhere. If you lose it, you lose the EC2 instance. I have a copy of mine in S3 . Connect to your EC2 Instance . In the AWS console, you can click connect. If your EC2 instance isn&#39;t started then start it. It will give you instructions for SSHing in. There&#39;s 2 steps that are important here. . chmod 400 &lt;key.pem&gt; . and . ssh -i &quot;key.pem&quot; username@whatever_it_tells_you_in_aws_console.compute-1.amazonaws.com . Launch Jupyter . In the EC2 I just ssh into I run . jupyter-lab --no-browser --port=8889 . I stored my key in the directory ~/.aws, but that should be replaced with wherever. I open a new console and run . ssh -i ~/.aws/key.pem -L 8000:localhost:8889 username@whatever_it_tells_you_in_aws_console.compute-1.amazonaws.com . From there open localhost:8000 in your browser and you are good to go. If it asks for a token you can find it in the terminal output where you ran jupyter-lab! .",
            "url": "https://isaac-flath.github.io/fastblog/aws/2020/06/25/JupyterEC2.html",
            "relUrl": "/aws/2020/06/25/JupyterEC2.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Neural Network Basics (Part 2)",
            "content": "from fastai2.vision.all import * from fastai2.data.external import * from PIL import Image import math . Intro . Today we will be working with the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict what number it is. We will be building a Neural Network to do this. This is building off of the Image Classifier Basics post where we classified into 3s and 7s. If anything in this post is confusing, I reccomend heading over to part 1. I am assuming that the content covered in Part 1 is understood. . If you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Load the Data . Naturally, the first step is to get and load the data. We&#39;ll look at it a bit along tohe way to make sure it was loaded properly as well. We will be using fastai&#39;s built in dataset feature rather than sourcing it ourself. I am skimming over this quickly as this was covered in part 1. . path = untar_data(URLs.MNIST) # This takes that path from above, and get the path for training and validation training = [x.ls() for x in (path/&#39;training&#39;).ls().sorted()] validation = [x.ls() for x in (path/&#39;testing&#39;).ls().sorted()] . Let&#39;s take a look at an image. The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with. . im3 = Image.open(training[6][1]) im3 . tensor(im3).shape . torch.Size([28, 28]) . Linear Equation . We are looking to do wx + b = y. It seems that a Neural network should use some super fancy equation in it&#39;s layers, but that&#39;s all it is. In a single class classifier, y has 1 column as it is predicting 1 thing. In a multiclass classifier y has &quot;however-many-classes-you-have&quot; columns. . Tensor Setup . The first thing I will do is get my xs and ys in tensors in the right format. . training_t = list() for x in range(0,len(training)): # For each class, stack them together. Divide by 255 so all numbers are between 0 and 1 training_t.append(torch.stack([tensor(Image.open(i)) for i in training[x]]).float()/255) validation_t = list() for x in range(0,len(validation)): # For each class, stack them together. Divide by 255 so all numbers are between 0 and 1 validation_t.append(torch.stack([tensor(Image.open(i)) for i in validation[x]]).float()/255) . training_t[1][1].shape . torch.Size([28, 28]) . Let&#39;s do a simple average of one of our images. It&#39;s a nice sanity check to see that we did things ok. We can see that after averaging, we get a recognizable number. That&#39;s a good sign. . show_image(training_t[5].mean(0)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6b6ba44250&gt; . x = torch.cat([x for x in training_t]).view(-1, 28*28) valid_x = torch.cat([x for x in validation_t]).view(-1, 28*28) # Defining Y. I am starting with a tensor of all 0. # This tensor has 1 row per image, and 1 column per class y = tensor([[0]*len(training_t)]*len(x)) valid_y = tensor([[0]*len(validation_t)]*len(valid_x)) # Column 0 = 1 when the digit is a 0, 0 when the digit is not a 0 # Column 1 = 1 when the digit is a 1, 0 when the digit is not a 1 # Column 2 = 1 when the digit is a 2, 0 when the digit is not a 2 # etc. j=0 for colnum in range(0,len(training_t)): y[j:j+len(training_t[colnum]):,colnum] = 1 j = j + len(training[colnum]) j=0 for colnum in range(0,len(validation_t)): valid_y[j:j+len(validation_t[colnum]):,colnum] = 1 j = j + len(validation[colnum]) # Combine by xs and ys into 1 dataset for convenience. dset = list(zip(x,y)) valid_dset = list(zip(valid_x,valid_y)) # Inspect the shape of our tensors x.shape,y.shape,valid_x.shape,valid_y.shape . (torch.Size([60000, 784]), torch.Size([60000, 10]), torch.Size([10000, 784]), torch.Size([10000, 10])) . Perfect. We have exactly what we need and defined above. 60,000 images x 784 pixels for my x. A 60,000 images x 10 classes for the predictions. . 10,000 images make up the validation set. . Calculate wx + b . Let&#39;s initialize our weights and biases, then do the matrix multiplication and make sure the output is the expected shape (60,000 images x 10 classes). . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() # Initializze w and b weight tensors w = init_params((28*28,10)) b = init_params(10) . (x@w+b).shape,(valid_x@w+b).shape . (torch.Size([60000, 10]), torch.Size([10000, 10])) . Great, we have the right number of predictions. Obviously the predictions are no good. There a couple things left to do. The first thing we need to do is turn our Linear Equation into a Neural Network. To do that we need to do this twice, with a ReLu inbetween. . Neural Network . . Note: You can check out the first Image Classifier blog post, which explains does this in a simpler problem (single class classifier) and assumes less pre-requisite knowledge. I am assuming that the information in Part 1 is understood. If you understand Part 1, you are ready for Part2! . # This can have more layers by duplicating the patten seen below, this is just the fewest layers for demonstration. def simple_net(xb): # Linear Equation from above res = xb@w1 + b1 #Linear # Replace any negative values with 0. This is called a ReLu. res = res.max(tensor(0.0)) #ReLu # Do another Linear Equation res = res@w2 + b2 #Linear # return the predictions return res . # The number 30 here can be adjusted for more or less model complexity. multipliers = 30 w1 = init_params((28*28,multipliers)) b1 = init_params(multipliers) w2 = init_params((multipliers,10)) b2 = init_params(10) . simple_net(x).shape # 60,000 images with 10 predictions per class (one per digit) . torch.Size([60000, 10]) . Improving Weights and Biases . We have predictions with random weights and biases. What we need to do is to get these weights and biases to be the right numbers rather than random numbers. To do this we need to use Gradient Descent to improve the weights. Here&#39;s roughly what we need to do: . Create a loss function to measure how close (or far) off we are | Calculate the gradient (slope) so we know which direction to step | Adjust our values in that direction | Repeat many times | . The first thing we need to use gradient descent is we need a loss function. Let&#39;s use something simple, how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5. We will do this for every class . The one addition is that we will add something called a sigmoid. All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges. . Note: If you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivilant to the &quot;Path Value&quot; . Loss Function . def mnist_loss(predictions, targets): # make all prediction between 0 and 1 predictions = predictions.sigmoid() # Difference between predictions and target return torch.where(targets==1, 1-predictions, predictions).mean() . mnist_loss(simple_net(x),y),mnist_loss(simple_net(valid_x),valid_y) . (tensor(0.4779, grad_fn=&lt;MeanBackward0&gt;), tensor(0.4793, grad_fn=&lt;MeanBackward0&gt;)) . Calculate Gradient . Now we have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let&#39;s create a function to change our weights. . First, we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches. . . Note: This is the same from part 1 . dl = DataLoader(dset, batch_size=1000, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=1000) # Example for how to get the first batch xb,yb = first(dl) valid_xb,valid_yb = first(valid_dl) . def calc_grad(xb, yb, model): # calculate predictions preds = model(xb) # calculate loss loss = mnist_loss(preds, yb) # Adjust weights based on gradients loss.backward() . Train the Model . . Note: This is the same from part 1 . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Measure Accuracy on Batch . def batch_accuracy(xb, yb): # this is checking for each row, which column has the highest score. # p_inds, y_inds gives the index highest score, which is our prediction. p_out, p_inds = torch.max(xb,dim=1) y_out, y_inds = torch.max(yb,dim=1) # Compre predictions with actual correct = p_inds == y_inds # average how often we are right (accuracy) return correct.float().mean() . Measure Accuracy on All . . Note: This is the same from part 1 . def validate_epoch(model): # Calculate accuracy on the entire validation set accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] # Combine accuracy from each batch and round return round(torch.stack(accs).mean().item(), 4) . Initialize weights and biases . # With this problem being much harder, I will give it more weights to work with complexity = 500 w1 = init_params((28*28,complexity)) b1 = init_params(complexity) w2 = init_params((complexity,10)) b2 = init_params(10) params = w1,b1,w2,b2 . Train the Model . Below we will actually train our model. . lr = 50 # epoch means # of passes through our data (60,000 images) epochs = 30 loss_old = 9999999 for i in range(epochs): train_epoch(simple_net, lr, params) # Print Accuracy metric every 10 iterations if (i % 10 == 0) or (i == epochs - 1): print(&#39;Accuracy:&#39;+ str(round(validate_epoch(simple_net)*100,2))+&#39;%&#39;) loss_new = mnist_loss(simple_net(x),y) loss_old = loss_new . Accuracy:22.67% Accuracy:32.56% Accuracy:33.12% Accuracy:34.25% . Results . A few key points: . The Loss is not the same as the metric (Accuracy). Loss is what the models use, Accuracy is more meaningful to us humans. | We see that our loss slowly decreases each epoch. Our accuracy is getting better over time as well. | . We start at about 10% accuracy, which makes sense. With random weights we predict correctly 1/10 times. With 10 digits that sounds like a random guess. Over time, we slowly decrease the loss and after 30 epochs we are around 36% accuracy. Much better! We could keep training more to keep improving accuracy, but I think we see the idea. . This Model vs SOTA . What is different about this model than a best practice model? . This model is only 1 layer. State of the art for image recognitions will use more layers. Resnet 34 and Resnet 50 are common (34 and 50 layers). This would just mean we would alternate between the ReLu and linear layers and duplicate what we are doing with more weights and biases. | More weights and Biases. The Weights and Biases I used are fairly small - I ran this extremely quickly on a CPU. With the appropriate size weight and biases tensors, it would make way more sense to use a GPU. | Matrix Multiplication is replaced with Convolutions for image recognition. A Convolution can be thought of as matrix multiplication if you averaged some of the pixels together. This intuitively makes sense as 1 pixel in itself is meaningless without the context of other pixels. So we tie them together some. | Dropout would make our model less likely to overfit and less dependent on specific pixels. It would do this by randomly ignoring different pixels so it cannot rely on them. It&#39;s very similar to how decision trees randomly ignore variables for their splits. | Discriminate learning rates means that the learning rates are not the same for all levels of the neural network. With only 1 layer, naturally we don&#39;t worry about this. | Gradient Descent - we can adjust our learning rate based on our loss to speed up the process | Transfer learning - we can optimize our weights on a similar task so when we start trying to optimize weights on digits we aren&#39;t starting from random variables. | Keep training for as many epochs as we see our validation loss decrease | . As you can see, these are not completely different models. These are small tweaks to what we have done above that make improvements - the combination of these small tweaks and a few other tricks are what elevate these models. There are many &#39;advanced&#39; variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing without all the jargon - they are pretty simple concepts. I&#39;ll cover them in future blog posts. .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/image%20classification/2020/06/21/Image-Classifier-Basics-MultiClass.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/21/Image-Classifier-Basics-MultiClass.html",
            "date": " • Jun 21, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Neural Network Basics (Part 1)",
            "content": "from fastai.vision.all import * from fastai.data.external import * from PIL import Image import math torch.manual_seed(100) . &lt;torch._C.Generator at 0x7f38fc038470&gt; . Intro . Today we will be working with a subset of the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict whether it is a 3 or a 7. We will be building a Neural Network to do this. . If you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book. They have some awesome courses on their fast.ai website as well. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Load the Data . Naturally, the first step is to get and load the data. We&#39;ll look at it a bit along tohe way to make sure it was loaded properly and we understand it. We will be using fastai&#39;s built in dataset feature rather than sourcing it ourself. . path = untar_data(URLs.MNIST_TINY) # This takes that path from above, and get the path for the threes and the sevens threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() . Let&#39;s take a look at an image. The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it. This is not just for memory concerns, but you want to generally know some basics about whatever you are working with. . im3 = Image.open(threes[1]) im3 . tensor(im3).shape . torch.Size([28, 28]) . What I am going to do below is put the tensor into a dataframe, and color the pixels based on the value in each place. We can clearly see that this is a 3 just from the values in the tensor. This should give a good idea for the data we are working with and how an image can be worked with. . pd.DataFrame(tensor(im3)).loc[3:24,6:20].style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 77 | 181 | 254 | 255 | 95 | 88 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 3 | 97 | 242 | 253 | 253 | 253 | 253 | 251 | 117 | 15 | 0 | 0 | 0 | 0 | . 6 0 | 20 | 198 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 239 | 59 | 0 | 0 | 0 | . 7 0 | 0 | 108 | 248 | 253 | 244 | 220 | 231 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 110 | 129 | 176 | 0 | 83 | 253 | 253 | 253 | 194 | 24 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 26 | 0 | 83 | 253 | 253 | 253 | 253 | 48 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 253 | 253 | 253 | 189 | 22 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 83 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 183 | 253 | 253 | 253 | 138 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 65 | 246 | 253 | 253 | 253 | 175 | 4 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 172 | 253 | 253 | 253 | 253 | 70 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 66 | 253 | 253 | 253 | 253 | 238 | 54 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 17 | 65 | 232 | 253 | 253 | 253 | 149 | 5 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 45 | 141 | 253 | 253 | 253 | 123 | 0 | 0 | . 18 0 | 0 | 0 | 41 | 205 | 205 | 205 | 33 | 2 | 128 | 253 | 253 | 245 | 99 | 0 | . 19 0 | 0 | 0 | 50 | 253 | 253 | 253 | 213 | 131 | 179 | 253 | 253 | 231 | 59 | 0 | . 20 0 | 0 | 0 | 50 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 212 | 0 | 0 | . 21 0 | 0 | 0 | 21 | 187 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 212 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 9 | 58 | 179 | 251 | 253 | 253 | 253 | 219 | 44 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 139 | 253 | 253 | 130 | 49 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Defining our Linear Equation . We are looking to do xw + b = y. It seems that a Neural network should use some super fancy equation in it&#39;s layers, but that&#39;s all it is. Basically the same equation everyone learns that defines a line (mx+b) . Setup . We will need a wieght matrix with 1 weight per pixel, meaning this will be a 784 row by 1 column matrix. In order to multiply this by our 784 pixels we need to frormat that in 1 row x 784 column matrix. Then we can do matrix multiplication. We are also going to add b, so let&#39;s initialize that as well. Since we haven&#39;t solved the problem yet, we don&#39;t know what good values for w and b are so we will make them random to start with. . Note: When we checked the shape above, we saw our images were 28 x 28 pixels, which is 784 total pixels. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() w = init_params((28*28,1)) b = init_params(1) . max(w) . tensor([2.8108], grad_fn=&lt;SelectBackward&gt;) . Now we just need x and y. A 784x1 matrix times a 1x784 matrix. We want all values to be between 0 and 1, so we divide by the max pixel value (255) . threes_t = [tensor(Image.open(i)) for i in threes] sevens_t = [tensor(Image.open(i)) for i in sevens] # Get my list of tensors and &quot;stack&quot; them. Also dividing by 255 so all values are between 0 and 1 threes_s = torch.stack(threes_t).float()/255 sevens_s = torch.stack(sevens_t).float()/255 #Verify max and min pixel values torch.min(threes_s), torch.max(threes_s), torch.min(sevens_s), torch.max(sevens_s) . (tensor(0.), tensor(1.), tensor(0.), tensor(1.)) . Let&#39;s do a simple average of all our threes together and see what we get. It&#39;s a nice sanity check to see that we did things ok. We can see that after averaging, we pretty much get a three! . show_image(threes_s.mean(0)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9fdda2a410&gt; . Perfect, lets finish definining our x. We want x to have both threes and sevens, but right now they are seperate. We will use torch.cat to combine them, and .view to change the format of the matrix to the right format. Y is being defined as a long matrix with 1 row per image (prediction) and 1 column. . x = torch.cat([threes_s, sevens_s]).view(-1, 28*28) # Set my y, or dependent variable matrix. A three will be 1, and seven will be 0. So we will be prediction 0 or 1. y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) # Combine by xs and ys into 1 dataset for convenience. dset = list(zip(x,y)) x_0,y_0 = dset[0] x_0.shape,y_0 . (torch.Size([784]), tensor([1])) . Perfect. We have exactly what we need and defined above. A 784 x 1 matrix times a 1 x 784 matrix + a constanct = our prediction. Let&#39;s take a look to verify things are the right shape, and if we actually multiply these things together we get 1 prediction per image. . w.shape,x_0.shape,b.shape,y_0.shape . (torch.Size([784, 1]), torch.Size([784]), torch.Size([1]), torch.Size([1])) . print((x@w+b).shape) (x@w+b)[1:10] . torch.Size([709, 1]) . tensor([[ 3.3164], [ 5.2035], [-3.7491], [ 1.2665], [ 2.2916], [ 1.3741], [-7.6092], [ 1.3464], [ 2.7644]], grad_fn=&lt;SliceBackward&gt;) . Great, we have the right number of predictions. Obviosly the predictions are no good at predictions 0 or 1. After all, our weights and biases are all random. Let&#39;s do something about that. . We will need to do all this on our validation set as well, so let&#39;s do that here. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . . Loss Function . We need to improve our weights and biases (w and b) and we do that using gradient descent. I have a few posts on gradient descent, feel free to check those out if you want details on how it works. Here we will use the built-in pytorch functionality. . The first thing we need to use gradient descent is we need a loss function. Let&#39;s use something simple, how far off we were. If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5. . The one addition is that we will add something called a sigmoid. All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1. We never want to predict anything outside of these ranges as those are our 2 categories. . def mnist_loss(predictions, targets): # make all prediction between 0 and 1 predictions = predictions.sigmoid() # Difference between predictions and target return torch.where(targets==1, 1-predictions, predictions).mean() . Gradient Descent . Background and Setup . predict | calculate loss | calculate gradient | subtract from weights and bias | . Ok, now we have a function we need to optimize and a loss function to tell us our error. We are ready for gradient descent. Let&#39;s create a function to change our weights. . Note: If you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this. We will be calculating a gradient - which are equivilant to the &quot;Path Value&quot; . def linear1(xb): return xb@weights + bias # Here is how we will initialize paramaters. This is just giving me random numbers. def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . First, we will make sure our datasets are in a DataLoader. This is convenience class that helps manage our data and get batches.. . dl = DataLoader(dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256) . We are now going to get the first batch out. We&#39;ll use a batch size of 256, but feel free to change that based on your memory. You can see that we can just simply call first dl, and it creates our shuffled batch for us. . xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . Let&#39;s Initialize our paramaters we will need. . weights = init_params((28*28,1)) bias = init_params(1) . Calculate the Gradient . We now have our batch of x and y, and we have our weights and biases. The next step is to make a prediction. Since our batch size is 256, we see 256x1 tensor. . preds = linear1(xb) preds.shape, preds[:5] . (torch.Size([256, 1]), tensor([[-17.7688], [ -3.9593], [ -4.0014], [ 1.4874], [ 0.5148]], grad_fn=&lt;SliceBackward&gt;)) . Now we calculate our loss to see how we did. Probably not well considering all our weights at this point are random. . loss = mnist_loss(preds, yb) loss . tensor(0.5678, grad_fn=&lt;MeanBackward0&gt;) . Let&#39;s calculate our Gradients . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0011), tensor([-0.0071])) . Since we are going to want to calculate gradients every since step, let&#39;s create a function that we can call that does these three steps above. Let&#39;s put all that in a function. From here on out, we will use this function. . Note: It&#8217;s always a good idea to periodically reviewing and trying to simplify re-usable code. I reccomend doing following the above approach, make something that works - then simplify. It often wastes a lot of time trying to write things in the most perfect way from the start. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(xb, yb, linear1) weights.grad.mean(),bias.grad . (tensor(-0.0022), tensor([-0.0142])) . weights.grad.zero_() bias.grad.zero_(); . Training the Model . Now we are ready to create a function that trains for 1 epoch. . Note: Epoch is just a fancy way of saying 1 pass through all our data. . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Naturally, we want to be able to measure accuracy. It&#39;s hard for us to gauge how well the model is doing from our loss function. We will create an accuracy metric to look at accuracy for that batch. . Note: A loss function is designed to be good for gradient descent. A Metric is designed to be good for human understanding. This is why they are different sometimes. . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(xb), yb) . tensor(0.4180) . Looking at accuracy of our batch is great, but we also want to look at our accuracy for the validation set. This is our way to do that using the accuracy funcion we just defined. . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.4138 . Awesome! Let&#39;s throw this all in a loop and see what we get. . params = weights,bias lr = 1. for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.5249 0.6097 0.7038 0.7628 0.8165 0.8439 0.8685 0.8839 0.8922 0.8984 0.9071 0.9097 0.9136 0.9162 0.9176 0.9202 0.9215 0.9267 0.9311 0.9319 . Linear Recap . Lots of stuff, but let&#39;s recap really what we did: . Make a prediction | Measure how we did | Change our weights so we do slightly better next time | Print out accuracy metrics along the way so we can see how we are doing | . This is a huge start, but what we have is a linear model. Now we need to add non-linearities so that we can have a true Neural Network. . ReLu . What is it? . First, a ReLu is our non-linearity in a neural network. A neural network is just alternating linear and nonlinear layers. We defined the Linear layer above, here we will talk about the non-linear ones. So what exactly does the non-linear layer do? It&#39;s actually much simpler than people like to believe, it&#39;s just a max. . For example, I am going to apply a ReLu to a matrix. . $ begin{bmatrix}-1&amp;1 -1&amp;-1 0&amp;0 0&amp;1 1&amp;-1 1&amp;0 -1&amp;0 -1&amp;-1 1&amp;1 end{bmatrix}$ $=&gt;$ $ begin{bmatrix}0&amp;1 0&amp;0 0&amp;0 0&amp;1 1&amp;0 1&amp;0 0&amp;0 0&amp;0 1&amp;1 end{bmatrix}$ . As you will see I just took max(x,0). Another way of saying that is I replaced any negative values with 0. That&#39;s all a ReLu is. . In a Neural Network . These ReLus go between our linear layers. Here&#39;s what a simple Neural Net looks like. . # The number 30 here can be adjusted for more a less model complexity. Future posts will talk more about what that is. w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . # This can have more layers by duplicating the patten seen below, this is just the simplest model. def simple_net(xb): res = xb@w1 + b1 #Linear res = res.max(tensor(0.0)) #ReLu res = res@w2 + b2 #Linear return res . Train the Full Neural Network . So we have our new model with new weights. It&#39;s more than just the linear model, so how do we use gradient descent? We now have 4 weights (w1,w2,b1,b2). . Turns our it&#39;s exactly what we already did. Let&#39;s add the new paramaters to our paramsm, and change out the linear model with the simple_net we just defined. We end up with a pretty decent accuracy! . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . params = w1,b1,w2,b2 lr = 1 for i in range(20): train_epoch(simple_net, lr, params) print(validate_epoch(simple_net), end=&#39; &#39;) . 0.6711 0.8028 0.8865 0.9198 0.9245 0.9225 0.9291 0.9223 0.9402 0.9433 0.9343 0.9433 0.9392 0.9392 0.9463 0.9431 0.9476 0.9489 0.9457 0.9528 . Recap of Tensor Shapes . Understanding the shapes of these tensors and how the network works is crucial. Here&#39;s the network we built. You can see how each layer can fit into the next layer. . x.shape,w1.shape,b1.shape,w2.shape,b2.shape . (torch.Size([709, 784]), torch.Size([784, 30]), torch.Size([30]), torch.Size([30, 1]), torch.Size([1])) . . What More? . This is a Neural Network. Now we can do tweaks to enhance performance. I will talk about those in future posts, but here&#39;s a few concepts. . Instead of a Linear Layer, A ReLu, then a linear layer - Can we add more layers to have a deeper net? | What if we average some of the pixels in our image together before dong matrix multiplication (ie a convolutions)? | Can we randomly ignore pixels to prevent overfitting (ie dropout)? | . There are many &#39;advanced&#39; variations of Neural Networks, but the concepts are typically along the lines of above. If you boil them down to what they are really doing - they are pretty simple concepts. .",
            "url": "https://isaac-flath.github.io/fastblog/neural%20networks/image%20classification/2020/06/19/Image-Classifier-Basics.html",
            "relUrl": "/neural%20networks/image%20classification/2020/06/19/Image-Classifier-Basics.html",
            "date": " • Jun 19, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Gradient Descent for Linear Regression (Part 2)",
            "content": "Why part 2? . I have done a couple blog posts on Gradient Descent for linear regression focusing on the basic algorithm. In this post, I will be covering some more advanced gradient descent algorithms. I will post as I complete a section rather than waiting until I have every variation posted. This is partially to show some popular ones, but the more important thing to understand from this post is that all these advanced algorithms are really just minor tweaks on the basic algorithm. . Goal Recap . The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. This approach will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit. . This post will cover the algorithms. Part 4 of this series will focus on scaling this up to larger datasets. One of the primary tools of scaling is using stochastic gradient descent, which is just a fancy way to say &quot;just use a subset of the points instead of all of them&quot;. . Background . Our goal is to define the equation $y= theta_0+ theta_1x$. This is the same thing as $y=mx+b$. For this post I will use $y=mx+b$ language with $m$ being the slope and $b$ being the y intercept. . Note: In order to adjust $m$, I will take $m$ - &lt;$m$ PathValue&gt; * &lt;adj $ alpha$&gt;. . . Note: In order to adjust $b$, I will take $b$ - &lt;$b$ PathValue&gt; * &lt;adj $ alpha$&gt;. Each of these advanced algorithms either change the Path Value, or change $ alpha$. I will show what the calculation for each is for each algorthm is, have a written explanation, and python code that illustrates it. . Setup . Here is where I load libraries, define my dataset, and create a graphing function. . import matplotlib.pyplot as plt import pandas as pd import numpy as np import math from IPython.display import clear_output from time import sleep np.random.seed(44) xs = np.random.randint(-100,100,100) ys = xs * np.random.randint(-10,10) + 100 # + np.random.randint(-200,200,50) cycles = 50 def graph_gradient_descent(values,cycles, figures,step): plt.figure(figsize=(20,10)) cols = 3 rows = math.ceil(figures/3) for x in range(1,figures+1): plt.subplot(rows,cols,x) plt.scatter(values[&#39;x&#39;],values[&#39;y&#39;]) plt.scatter(values[&#39;x&#39;],values[&#39;cycle0&#39;]) plt.scatter(values[&#39;x&#39;],values[&#39;cycle&#39;+str(x*step)]) labels = [&#39;y&#39;,&#39;initial&#39;,&#39;cycle&#39;+str(x*step)] plt.legend(labels) plt.ylim(-1000,1000) plt.xlim(-100,100) . . Basic Gradient Descent . This is the basic gradient descent algorithm that all others are based on. If you are not clear on how gradient descent works, please refer to the background section for a review or Gradient Descent Part 1 Blog Post for more details. I will use this same format below for each algorithm, and change only what is necessary for easier comparison. . Inputs . $ alpha = learning rate$ . n = number of data points . New Formulas . $PathValue_m = PathValue_bx$ . $PathValue_b = y_{pred}-y_{obs}$ . Each variation after this does 1 of 3 things to modify this algorithm: . Adjusts $ alpha$ by some amount | Adjust $PathValue$ by some amount. | Adjust both $ alpha$ and $PathValue$. | . Really logically speaking, what else can you do? These are the only values that are used to adjust our values, so any tweaks must involve those. We can modify number through addition, subtraction, multiplication, and division: If you get stuck, try to get back to those basics. . Python Function . alpha = 0.0005 def gradient_descent(xs,ys,alpha,cycles): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvb&#39;,&#39;pvm&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) new_m = weights[p_cycle_name].m - path_value_m * adj_alpha new_b = weights[p_cycle_name].b - path_value_b * adj_alpha weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent(xs,ys,alpha,cycles) graph_gradient_descent(values,cycles,12,2) . Momentum . Concept . The idea of momentum is to use the previous path values to influence the new path value. It&#39;s taking a weighted average of the previous path value and the new calculation. This is referred to as momentum because it is using the momentum from previous points to change the size of the step to take. To control what kind of weighted average is used, we define $ beta$. . This is useful and effective because we want to have very large steps early on, but the closer we get to the optimal values the lower we want our learning rate to be. This allows us to do that, and if we overshoot then it will average with previous path values and lower the step size. This allows for larger steps while minimizing the risk of our gradient descent going out of control. If you overshoot the optimal weights the weighted average will decrease the step size and keep going, eventually settling on the minimum. A very handy feature! . What is different . What is different: $PathValue$ has changed and is using a new input $ beta$ . If you look at $PathValue_b$ you will notice a change in this formula. $PathValue_m$ multiplies $PathValue_b$ by our x value for that point, so it is effected as well. . New Inputs . $ beta = 0.9$ . New Formulas . $ alpha_{adj} = frac{1}{n} alpha$ . $PathValue_m$ = $PathValue_bx$ . $PathValue_b = ( beta)(PathValue_{b_{previous}}) + (1 - beta)(y_{pred}-y_{obs})$ . Python Function . alpha = 0.0001 beta = 0.9 def gradient_descent_momentum(xs,ys,alpha,cycles,beta): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) if cycle &gt; 1: path_value_b = (beta) * weights[p_cycle_name].pvb + (1-beta) * path_value_b path_value_m = (beta) * weights[p_cycle_name].pvm + (1-beta) * path_value_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha new_b = weights[p_cycle_name].b - path_value_b * adj_alpha weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta) graph_gradient_descent(values,cycles,12,3) . RMSProp . Concept . The idea of RMS prop is that we will adjust out learning rate based on how large our error rate it. If we have a very large error, we will take a larger step. With a smaller error, we will take a smaller step. This minimizes the changces that we overshoot the ideal weights. This is accomplished by diving the learning rate by an weighted exponential average of the previous path values. To control what kind of weighted average is used, we define $ beta$. . This is useful and effective because we want to have very large steps early on with a big learning rate, but the closer we get to the optimal values the lower we want our learning rate to be. This is exactly what RMS prop does - adjust out learning rate based on our error rate. This allows for larger steps with a bigger learning rate while minimizing the risk of our gradient descent going out of control. It has similar benefits of momentum, but approaches it by modifying the learning rath rather than the path value. . What is different . What is different: we have an alpha_multiplier for each variable that is calculated each cycle. When calculating the new value, we divide our learning rate $ alpha$ by the square root of this alpha multiplier. The alpha multiplier uses a new input, $ beta$ . Our alpha multiplier is calculated with this formula. . $alphamultiplier_b = ( beta)(alphamultiplier_{b_{previous}}) + (1 - beta)((y_{pred}-y_{obs})^2)$ . $alphamultiplier_m = ( beta)(alphamultiplier_{m_{previous}}) + (1 - beta)(x(y_{pred}-y_{obs})^2)$ . Here&#39;s the path value for our Regular Gradient Descent | . $new_m = m - PathValue_m * frac{ alpha}{n}$ . $new_b = b - PathValue_b * frac{ alpha}{n}$ . Here&#39;s the path value for our RMS Prop | . $new_m = m - PathValue_m * frac{ alpha}{n sqrt{alphamultiplier_m}}$ . $new_b = b - PathValue_b * frac{ alpha}{n sqrt{alphamultiplier_b}}$ . Python Function . beta = 0.9 alpha = 500 def gradient_descent_momentum(xs,ys,alpha,cycles,beta): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;,&#39;am_m&#39;,&#39;am_b&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) alpha_multiplier_b = abs(path_value_b)**2 alpha_multiplier_m = abs(path_value_m)**2 if cycle &gt; 1: alpha_multiplier_b = (beta) * weights[p_cycle_name].am_b + (1-beta) * alpha_multiplier_b alpha_multiplier_m = (beta) * weights[p_cycle_name].am_m + (1-beta) * alpha_multiplier_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m) new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b) weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b, alpha_multiplier_m, alpha_multiplier_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta) graph_gradient_descent(values,cycles,15,2) . Adam . Concept . The idea of Adam is that there are really nice properties to RMS Prop as well as momentum, so why not do both at the same time. We will modify our path value using the momentum formula and we will modify our learning rate using RMSProp formula. To control what kind of weighted average is used, we define beta_rmsprop and beta_momentum. We can have a pretty big learning rate without overshooting. . This is useful and effective because we want the ability to pick up speed like momentum does, but also want to minimize overshooting. Basically we pick up momementum when we are far from the minimum, but we slow down as we get close before we overshoot. . beta_rmsprop = 0.9 beta_momentum = 0.7 alpha = 200 def gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum): n = len(xs) adj_alpha = (1/n)*alpha values = pd.DataFrame({&#39;x&#39;:xs,&#39;y&#39;:ys}) weights = pd.DataFrame({&#39;cycle0&#39;:[1,1,0,0,0,0]},index=[&#39;m&#39;,&#39;b&#39;,&#39;pvm&#39;,&#39;pvb&#39;,&#39;am_m&#39;,&#39;am_b&#39;]) values[&#39;cycle0&#39;] = weights[&#39;cycle0&#39;].m*values[&#39;x&#39;] + weights[&#39;cycle0&#39;].b for cycle in range(1,cycles+1): p_cycle_name = &#39;cycle&#39;+str(cycle-1) c_cycle_name = &#39;cycle&#39;+str(cycle) error = values[p_cycle_name]-values[&#39;y&#39;] path_value_b = sum(error) path_value_m = sum(error * values[&#39;x&#39;]) if cycle &gt; 1: path_value_b = (beta_momentum) * weights[p_cycle_name].pvb + (1-beta_momentum) * path_value_b path_value_m = (beta_momentum) * weights[p_cycle_name].pvm + (1-beta_momentum) * path_value_m alpha_multiplier_b = abs(path_value_b)**2 alpha_multiplier_m = abs(path_value_m)**2 if cycle &gt; 1: alpha_multiplier_b = (beta_rmsprop) * weights[p_cycle_name].am_b + (1-beta_rmsprop) * alpha_multiplier_b alpha_multiplier_m = (beta_rmsprop) * weights[p_cycle_name].am_m + (1-beta_rmsprop) * alpha_multiplier_m new_m = weights[p_cycle_name].m - path_value_m * adj_alpha / math.sqrt(alpha_multiplier_m) new_b = weights[p_cycle_name].b - path_value_b * adj_alpha / math.sqrt(alpha_multiplier_b) weights[c_cycle_name] = [new_m, new_b, path_value_m, path_value_b, alpha_multiplier_m, alpha_multiplier_b] y_pred = weights[c_cycle_name].m*values[&#39;x&#39;] + weights[c_cycle_name].b values[c_cycle_name] = y_pred return weights,values weights, values = gradient_descent_momentum(xs,ys,alpha,cycles,beta_rmsprop,beta_momentum) graph_gradient_descent(values,cycles,15,2) .",
            "url": "https://isaac-flath.github.io/fastblog/gradient%20descent/2020/06/11/GradientDescentforLinearRegression-P2.html",
            "relUrl": "/gradient%20descent/2020/06/11/GradientDescentforLinearRegression-P2.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Solving $Ax=b$ - The Complete Solution (18.06_L8)",
            "content": "Ax=b . $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b1 2&amp;4&amp;6&amp;8&amp;b2 3&amp;6&amp;8&amp;10&amp;b3 end{array} right]$ . Note: You may notice not all rows and columns are independent Let&#39;s do elimination to start solving. . $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 2&amp;4&amp;6&amp;8&amp;b_2 3&amp;6&amp;8&amp;10&amp;b_3 end{array} right]$ $=&gt;$ $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 0&amp;0&amp;2&amp;4&amp;b_2-2b_1 0&amp;0&amp;2&amp;4&amp;b_3-3b_1 end{array} right]$ $=&gt;$ $ left[ begin{array}{cccc|c} 1&amp;2&amp;2&amp;2&amp;b_1 0&amp;0&amp;2&amp;4&amp;b_2-2b_1 0&amp;0&amp;0&amp;0&amp;b_3-b_2-b_1 end{array} right]$ . Well that makes sense. We can see intuitively we that the last row - the second - the first gives us 0. . Solvability condition on b . Solvable when b is in $C(A)$ . Note: Column space of A $C(A)$ is all linear combinations of columns of A | If a combination of A gives zero row, then the same combinations of entries of b must give 0 | . Find Complete Solution . Find $x_{particular}$ . Set all free variables to 0 | Solve $Ax=b$ for pivot variables | Set all free variables to 0 . Free variable are rows that do not have a pivot column. These free variables can be set to anything, so we set them to the most convenient thing 0 and do back substitution. . $ left[ begin{array}{cccc} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{array} right]$ $ left[ begin{array}{cccc} x_1 0 x_3 0 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 3 0 end{array} right]$ . Solve $Ax-b$ for pivot variables. . $x_1 + 2x_3 = 1$ $2x_3 = 3$ . This means . $x_3 = 3/2$ and $x_1 = -2$ . Put those into out x and we have. . $x_p =$ $ left[ begin{array}{cccc} -2 0 3/2 0 end{array} right]$ . Find all Solutions . Find anything in $x_{nullspace}$ | $X=X_p + X_n$ . Note: The nullspace are any values of x that solve for 0. So naturally the x particular solution + any combination of 0 is still the solution. $x_{complete} = $ $ left[ begin{array}{cccc} -2 0 3/2 0 end{array} right]$ $+c_1$ $ left[ begin{array}{cccc} -2 1 0 0 end{array} right]$ $+c_1$ $ left[ begin{array}{cccc} 2 0 -2 1 end{array} right]$ |",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/06/06/18.06_8_Solving-Ax=b-row-reduced-form-R.html",
            "relUrl": "/linear%20algebra/2020/06/06/18.06_8_Solving-Ax=b-row-reduced-form-R.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Gradient Descent for Linear Regression (Part 1B)",
            "content": "Why part 1B? . I have been getting questions about the initial Gradient Descent Post. The questions boil down to &quot;So with 2 points I can define a line, but I could already do that. What I need is to fit a line where points aren&#39;t perfect and I have lots of them. How do I use gradient descent in a more complicated problem? . This post will quickly recap the initial Gradient Descent for Linear Regression post, show that methodology applied in a google sheet so you can see each calculation, and then scale that methodology to more points. . Goal Recap . The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. . I will use the sum of squares cost function to take a predicted line and slowly change the regression coefficients until the line is a line of best fit. Here&#39;s what it looks like before and after 24 iterations of gradient descent. As you can see, after 24 iterations our predicted points are getting pretty close to a best fit. You will be able to use the method defined here to scale this to as many points as you have. . . In the first blog we showed this table for how we calculate out prediction. Because we are talking about a linear problem, y = mx + b is the equation, or in calculus terms $y = theta_0+ theta_1x$. We could take this table and expand it down to include $x_3$ all the way through $x_n$ to represent our dataset. . . The tree below from the first blog illustrates how to solve for cost as well as how to improve the values of $ theta$ to minimize cost in the 2 point problem defined there. So the question is, how would we modify this tree for more points? Well, with more data points there would be more edges originating at $J$, and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. Specifically, here is what I would change for a more complicated example with more features: . First, we have a branch for $x^1$ and a branch for $x^2$. These branches are almost identical, other than it being for the 2nd point vs the 1st point. So the first step is to add a branch off of $J = A^1 + a^2$ for $x^3$ all the way to $x^n$. | The second step is to take our formula $1/2 * (y_{pred} - y_{obs})^2$ and change $1/2$ to 1 over &lt;# of data points&gt;. This isn&#39;t strictly neccesary, but it makes the values of J we see a bit more intiutive and scaled. | The third thing is to multiply our path values by 1 over &lt;# of data points&gt;. Again, this isn&#39;t strictly neccesary but it makes setting a learning rate much more intuitive rather than having to do something more complicated to scale our learning rate based on the number of points we have. As a refresher, the path value for theta 1 was $ theta_1 ; path ;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)$, which by multiplying values from the edges in the chart together. The path value for theta 1 will now be $ theta_1 ; path ;value = (x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)) * frac{1}{&lt; # features&gt;)}$. We will do that for the path value formula for $ theta_0$ as well. | . Just like with 2 points, we will multiply the path value by $ alpha$, and subtract that from that $ theta$ to improve our predictions . See this actually work . I have created a google sheet that walks through these cauculations. I strongly reccomend walking through each cell calculation and matching it up to the chart above. Star with the 2Points_Linear_Scalable tab. You can then go to the More_Points_Linear tab and see that it&#39;s the exact same formulas and calculations. . Click here for the Google Sheet . For bonus points, you can start to see what a more advanced gradient descent algorithm is on the Momentum tab. If you look through all the formulas, you will see it&#39;s almost the same thing - but instead of using just the new path value we are doing a weighted average of the path value with the previous path value. .",
            "url": "https://isaac-flath.github.io/fastblog/gradient%20descent/2020/06/01/GradientDescentforLinearRegression-P1B.html",
            "relUrl": "/gradient%20descent/2020/06/01/GradientDescentforLinearRegression-P1B.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Column Space and Null Space (18.06_L7)",
            "content": "Ax=0 . $A =$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ . Note: You may notice not all rows and columns are independent Let&#39;s do elimination to start solving. . $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;2&amp;4 end{bmatrix}$ . Uh oh. Now I see a 0 in the pivot position and I cannot do a row exchange. Really that&#39;s just telling me that the now is just a combination of earlier columns. It&#39;s depending on earlier columns. Let&#39;s just go on and use the next column as the pivot for that row. . $A=$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 2&amp;4&amp;6&amp;8 3&amp;6&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;2&amp;4 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=U$ . Note: We have 2 pivots (aka rank 2). Row 1, Column 1 and Row 2, Column 3. . Special Solutions. . Let&#39;s write out $U$ in matrix in equation form to see what we have. . $x_1+2x_2+2x_3+2x_4=0$ . $2x_3+4x_4=0$ . Let&#39;s call the columns with a pivot a pivot column, and ones without a free column. So we have 2 pivot columns and 2 free columns. . Let&#39;s assign some values to the free columns, then solve for the pivot columns using back substitution. We&#39;ll call these special solutions. . $x=$ $ begin{bmatrix} 1 0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ $=&gt;$ $C$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ . Let&#39;s put 1 in the other free variable. . $x=$ $ begin{bmatrix} 0 1 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ $=&gt;$ $C$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ . Great. Our entire nullspace is a linear combination of these 2 special solutions. There will be 1 special solution for each free variable. In this case there were 2 (free columns, rank, free variables, etc). . $C$ $ begin{bmatrix} -2 1 0 0 end{bmatrix}$ $+d$ $ begin{bmatrix} 2 0 -2 1 end{bmatrix}$ . Reduced Row Echelon Form ($R$) . Reduced row echelon form has zeros above and below pivots, and the pivots are = 1. Let&#39;s do this using elimination. . $U=$ $ begin{bmatrix} 1&amp;2&amp;2&amp;2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;0&amp;-2 0&amp;0&amp;2&amp;4 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;0&amp;-2 0&amp;0&amp;1&amp;2 0&amp;0&amp;0&amp;0 end{bmatrix}$ $=R$ . Note: This row of 0&#8217;s was created because elimination identified that row 3 is just a combination of rows 1 and 2. Why this form? . We see in the pivot columns we see $I$ $ begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix}$ . In the free columns we see the opposite of the nullspace definintions we found above $ begin{bmatrix}2&amp;-2 0&amp;2 end{bmatrix}$ . Solve the transpose . $A=$ $ begin{bmatrix} 1&amp;2&amp;3 2&amp;4&amp;6 2&amp;6&amp;8 2&amp;8&amp;10 end{bmatrix}$ . Elimination from $A$ -&gt; $U$ . $ begin{bmatrix} 1&amp;2&amp;3 2&amp;4&amp;6 2&amp;6&amp;8 2&amp;8&amp;10 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;3 0&amp;0&amp;0 0&amp;2&amp;2 0&amp;4&amp;4 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;2&amp;3 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ . Note: Notice that I did a row exchange to do this elimination and ended with a rank 2, which was the same as with the transpose. . Solve for Nullspace . There&#39;s 1 free column, so you fill in a 1 in the free column. $x=$ $ begin{bmatrix} 1 end{bmatrix}$ Then with back substution solve for the pivot columns. The solution is any linear combination of this, which is a line in the null space. . $$x=C begin{bmatrix}-1 -1 1 end{bmatrix}$$ . Row Reduction from $U$ -&gt; $R$ . Row Echelon Form . $ begin{bmatrix} 1&amp;2&amp;3 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;0&amp;1 0&amp;2&amp;2 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=&gt;$ $ begin{bmatrix} 1&amp;0&amp;1 0&amp;1&amp;1 0&amp;0&amp;0 0&amp;0&amp;0 end{bmatrix}$ $=R$ . We see the identity matrix in the pivot columns, and the opposite of our pivot nullspace varibales in the free column. Perfect! .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/17/18.06_7_pivot_variables_special_solutions.html",
            "relUrl": "/linear%20algebra/2020/05/17/18.06_7_pivot_variables_special_solutions.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Column Space and Null Space (18.06_L6)",
            "content": "Background . We started on this topic in the last post, so much this section is review - but critical for the upcoming discussion. . Vector Spaces . To have a vector space you need to be able to: . Add any vectors and still be within the space | Multiply by any number and stay within space | Take any linear combination and stay within space | Examples . $R^2$ is a vector space. This is all real number vectors with 2 components. Here&#39;s a few examples. . $ begin{bmatrix}3 2 end{bmatrix}$, $ begin{bmatrix}0 0 end{bmatrix}$, $ begin{bmatrix} pi e end{bmatrix}$ . $R^3$ is another vector space, but it contains all real 3D vectors. Here&#39;s a few examples. . $ begin{bmatrix}3 2 5 end{bmatrix}$, $ begin{bmatrix}3 2 0 end{bmatrix}$, $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix} pi e 5 end{bmatrix}$ . $R^n$ is another with all column vectors with $n$ real components. . Subspaces . What if we just want a piece of $R^n$? We still need to be able to add vectors and multiply it together. So what are some subspaces? . A line through $R^n$ that goes through the origin. Any multiple of a point on a line is still on the line. We need the origin because you can mutliply any vector by 0, which would give you the origin. | Just the Origin is a subspace. You can add it to itelf, multiply it by anything, and take any combination and you will stil lhave the origin. | In $R^3$ and above, a plane through the origin is also a subspace. | Combinations of Subspaces . If I have 2 subspaces, then all vectors in P or L or both does not make a subspace. I can&#39;t add all together. In $R^3$ I would have to make a cube which would be all of $R^3$. . However, if you all points that are in both subspaces does make a subspace it is a subspace. If you think about it, by definition 2 Vectors in the intersection are for the subspace 1 and they are both in subspace 2. We already know that if you add 2 vectors in subspace 1 you get another vector in subspace 1. We also know the same thing about subspace 2. So really, any linear combination has to be in both subspaces (probably a smaller subspace). . Subspace from a matrix . Column Space . $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ . Columns are in $R^4$ and all their linear combinations are a subspace. So we have 3 subspaces, 1 for each column. So the full subspace for this matrix must have a linear combination of all columns, or all 3 subspaces. With 3 columns do we get all of $R^4$? What exactly do we get? How big is this subspace? Does $Ax=b$ have a solution for every $b$? Which $b$ values are ok? . We know it doesn&#39;t, because we have 4 equations but only 3 unknowns. In the proper form we see. . $Ax=b$ $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ $ begin{bmatrix}x_1 x_2 x_3 end{bmatrix}$ $=$ $ begin{bmatrix}b_1 b_2 b_3 b_4 end{bmatrix}$ . Now, just because we can&#39;t solve for every b doesn&#39;t mean we can&#39;t solve it for any b. So which $b$&#39;s allow this to be solved? Obviously we can solve for the 0 vector. Obviously we can solve for (1,2,3,4) or any other column as we can have $x_1 = 1$ with the other x&#39;s being 0. We could also solve for multiples of any column because $x_2$ could equal 2 or 3 with other uknowns being 0. . Note: To Summarize, Ax=b can be solved with $b$ is in $C(A)$, the column space of A. Now column 3 and column 1 are redundant, so we can throw away column 3. Column 1 + column 2 = column 3. So really linear combinations of the first 2 columns gives the same subspace as linear combinations of all 3 columns. So in all practicality, we have 2 equations with 4 unknowns with 2 of the columns sitting on the same plane. . Nullspace . $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ . Nullspace of $A$ = all solutions of $x$ where $Ax=0$. Because there are 3 unknowns, it&#39;s a subspace in $R^3$. This is opposed to the column space that is in $R^4$. Naturally the 0 vector always satisfies this so is always in the nullspace. Here&#39;s a few others . $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix}1 1 -1 end{bmatrix}$, $ begin{bmatrix}2 2 -2 end{bmatrix}$, $ begin{bmatrix}-1 -1 1 end{bmatrix}$, $ begin{bmatrix}-2 -2 2 end{bmatrix}$ . To summarize, we really have a line in $R^3$, which is a subspace: $c begin{bmatrix}1 1 -1 end{bmatrix}$ . Solution Spaces . Great! So we have defined the nullspace, which is just a subspace when $b$ is the 0 vector. Why not do the same thing with other $b$&#39;s? For example: . $Ax=b$ $ begin{bmatrix}1&amp;1&amp;2 2&amp;1&amp;3 3&amp;1&amp;4 4&amp;1&amp;5 end{bmatrix}$ $ begin{bmatrix}x_1 x_2 x_3 end{bmatrix}$ $=$ $ begin{bmatrix}1 2 3 4 end{bmatrix}$ . Clearly it isn&#39;t a space because the zero vector is not a solution. . Possible Solutions = $ begin{bmatrix}1 0 0 end{bmatrix}$, $ begin{bmatrix}2 1 -1 end{bmatrix}$, $ begin{bmatrix}3 2 -2 end{bmatrix}$, $ begin{bmatrix}0 -1 1 end{bmatrix}$ .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/16/18.06_6_ColumnSpace_Nullspace.html",
            "relUrl": "/linear%20algebra/2020/05/16/18.06_6_ColumnSpace_Nullspace.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Transposes, Permutations, and Spaces (18.06_L5)",
            "content": "Background . In previous posts, we have gone over elimination to solve systems of equations. However, every example workout out perfectly. What I mean by that is we didn&#39;t have to do row exchanges. Every example was in an order that worked on attempt 1. In this post, we are going to talk about how to solve equations when this isn&#39;t the case. . Row Exchanges . The way we do a row exchange in matrix language is that we multiply by a permutation matrix, which we will refer to as $P$. . So how do we account for row exchanges? Our previous equation of $A=LU$ does not account for row exchanges. . Permutations . A Permutation matrix executes our row exhanges. This matrix $P$ is the identify matrix with reording rows. $A=LU$ becomes $PA=LU$. We multipy a permutation matrix $P$ by $A$. . Note: The reason we were able to ignore $P$ in previous posts is because when you have no row exchanges, $P$ is just the identify matrix (no re-ordering) If we were to switch rows 2 and 3 in a 3x3 matrix, $P$ would be . $$ begin{bmatrix}1&amp;0&amp;0 0&amp;0&amp;1 0&amp;1&amp;0 end{bmatrix}$$ . Transposed Matrices . A transposed matrix has rows and columns switched. Naturally, symmetric matrices are unchanged by the transpose. . A Matrix multiplied by it&#39;s transpose always gives a symmetric matrix. . Vector Spaces . To have a vector space you need to be able to: . Add any vectors and still be within the space | Multiply by any number and stay within space | Take any linear combination and stay within space | Examples . $R^2$ is a vector space. This is all real number vectors with 2 components. Here&#39;s a few examples. . $ begin{bmatrix}3 2 end{bmatrix}$, $ begin{bmatrix}0 0 end{bmatrix}$, $ begin{bmatrix} pi e end{bmatrix}$ . $R^3$ is another vector space, but it contains all real 3D vectors. Here&#39;s a few examples. . $ begin{bmatrix}3 2 5 end{bmatrix}$, $ begin{bmatrix}3 2 0 end{bmatrix}$, $ begin{bmatrix}0 0 0 end{bmatrix}$, $ begin{bmatrix} pi e 5 end{bmatrix}$ . $R^n$ is another with all column vectors with $n$ real components. . Subspaces . What if we just want a piece of $R^n$? We still need to be able to add vectors and multiply it together. So what are some subspaces? . A line through $R^n$ that goes through the origin. Any multiple of a point on a line is still on the line. We need the origin because you can mutliply any vector by 0, which would give you the origin. | Just the Origin is a subspace. You can add it to itelf, multiply it by anything, and take any combination and you will stil lhave the origin. | In $R^3$ and above, a plane through the origin is also a subspace. | Subpace from a matrix . $ begin{bmatrix}1&amp;3 2&amp;3 4&amp;1 end{bmatrix}$ . Columns in $R^3$ and all their linear combinations are a subspace. This subspace is a plane going through the origin .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/13/18.06_5_Transposes,Permutations,Spaces.html",
            "relUrl": "/linear%20algebra/2020/05/13/18.06_5_Transposes,Permutations,Spaces.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Predictive Customer Analytics (Part 2)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. . This is the 2nd post in a series about customer analytics. The first post discusses descriptive analytics and how to create and use descriptive customer analytics. In this post we will be building on that descriptive framework to create a very basic predictive model. A predictive model allows companies to plan proactively instead of reactively. . Background . In the first post we created three descriptive metrics (Recency, Frequency, and Average Monetary Value). Recency is days since the last purchase. Frequency is the number of cycles that the customer purchased product in. Monetary Value is the average revenue brought in from that customer in cycles where the customer purchased product. How can you determine the predicted customer value and take action based on that information? . We need to group customers based on how they behave. This allows us to quickly and efficiently test different actions and see how that affects our long term revenue projections. A customer who makes many small purchases may be just as valuable as a customer who places huge orders infrequently, but they should be handled differently. RFM is one tool we can use to group customers together. . Normalize the metrics . We start with a basic table with three companies and their 3 KPIs. .   Recency (days since previous purchase) Frequency (number of months with purchases) Monetary Value (avg monthly revenue during months with purchases) . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Now that we have values for each of these three metrics (Recency, Frequency, and Monetary Value), we need to put them into a format where we can easily compare the customers. This is called normalizing the data. Normalization puts all the metrics on a scale of 0 (lowest) to 1 (highest). Here is a formula for normalizing: . (&lt;Value you want to normalize&gt; – &lt;Minimum Value in series&gt;) / (&lt;Maximum value in series&gt; – &lt;Minimum value in series&gt;) . Let’s walk through normalizing the Recency KPI. Based on the formula above we need a couple of pieces of information. Let’s identify these now. . Minimum Value in series = 3 Maximum value in series = 48 . Now let’s plug in the formula for each customer to normalize the values. .   Recency Normalized Recency . Customer A | 3 | =(3-3) / (48-3) = 0 | . Customer B | 35 | =(35-3) / (48-3) = 0.711 | . Customer C | 48 | =(48-3) / (48-3) = 1 | . The next step is to repeat this process for the other metrics. I won’t walk through the other two, but here are the accurate values if you’d like to test yourself. .   Recency Frequency Monetary Value . Customer A | 0 | 1 | 0 | . Customer B | 0.711 | 0.5 | 0.001 | . Customer C | 1 | 0 | 1 | . A low recency score is good. A high Frequency or Monetary Value Score is good. If you’d like to simplify it so you don’t have to remember that feel free to take your normalized recency score and subtract it from 1. That will make low scores bad and high scores good on all metrics. . Group the customers together . Now that we have normalized our KPIs, we need to group our customers together. In the table above we only have 3 customers, so there would be no point in grouping them together. In a more practical example we may have 1000 customers, 10,000 customers, or more. When we have that many customers, we can greatly simplify things by grouping like customers together. . We can group customers using our RFM KPIs, but there may be more information we want to pull into that decision. For example if we are a manufacturing company that sells product to end users as well as distributors we may want to take that into account. A distributor is likely to behave and react differently from an individual person. For example, an individual person may get a coupon in the mail and decide to purchase, but a distributor contact may get hundreds of coupons a week and just get annoyed. . A lot of time should be spent exploring different ways to break up customers. Explaining how you break up the customers into groups to sales people, product management, purchasing, production control, and other functional groups in your company can be a great way to get ideas. Each of these groups sees how customers react in different ways and talking to all of them can help give a more complete picture. . Create probability matrix . The goal of customer analytics is to predict how much money we are going to make next month, and the month after, and the month after that, and so on. Once we can predict that with reasonable accuracy we can determine exactly how much revenue we can expect from each customer if we do not change anything. . We need to put probabilities on customer actions. In simplest terms the customer has 2 choices during each cycle, buy or do not buy. So how do we figure out what the chance of that is for a group? . Suppose we a group of distributors we sell to that we believe act similarly as with the table below. A 1 means that the distributor bought something in that cycle, a 0 means that the distributor did not buy anything in that cycle. .   January February March . Distributor A | 1 | 0 | 0 | . Distributor B | 0 | 1 | 0 | . Distributor C | 0 | 1 | 1 | . Since there are 9 total opportunities to buy, and 4 of those have buys in them, we will assign a 4/9 chance of buying. Really this is just another way to measure frequency. . Now we know how many of the customers in the distributor group will buy in a given month (4 out of 9), so we need to turn that into a revenue figure. To do this we will take the sum of revenue from these customers and divide that by the number of buys. Here we are calculating Monetary Value (M), but instead of looking at it on a customer level, we are looking at it for a group of customers. . Once we multiply those two numbers together we have an estimated monthly revenue. . There’s several areas for future improvement that I won’t cover in this post. Depending on the market or the business model your company uses, they will vary in significance. Here’s a few of them: . Take into account recency. | Take into account customer retention. This makes forecasts accuracy go down farther out you go into the future. Since for this application we aren’t looking out very far in the future it makes less of a difference, but it is definitely high priority on our list of future enhancements. | Changes in buy patterns based on seasonal trends. | . Increase revenue using predicted value of each account . Now that we know roughly what revenue we can expect, we can use that for a variety of things. Here’s a few ideas: . First, we can look at which locations/territories are performing as forcasted. Once we know the high performing locations and the low performing locations, we can start to identify what the differences are. This can also be used to manage performance of employees, set meaningful incentives based on revenue, and set measurable objectives. | We can continue to do A/B testing, but be able to tie this to actual revenue dollars. | We can use these forecasts to determine inventory stances on either components or salable goods. | Now that we have grouped customers that behave similarly, we can start to determine how we should treat each group. We can have limited A/B testing. Instead of taking a random group of customers, we can take a random group of distributors to see how they specifically react. | We can determine who our most valuable customers are. Typically without measures, we remember the big purchases. But a regular purchaser that doesn’t make big purchases may be bringing us more revenue. Understanding which customers are most likely to be the most profitable is crucial and can help us determine pricing tiers for customers or determine customer loyalty programs. | We can identify low performing groups and create a strategy to get more penetration into those customers. Maybe for those customers we can send them surveys. Maybe we can set up a forum and do 6 month surveys where we work and collaborate with them regularly. This can help give insight into why they don’t purchase much, as well as be a good sales opportunity. We can also try to take action to take the low performing customers and get them to increase revenue through promotions or other incentives. | We can take the high performing customers and make sure that they know they are appreciated. Whether it’s a can of cashews over the holidays, or having a sales person reach out to talk once a month, building that relationship can really build customer loyalty. | .",
            "url": "https://isaac-flath.github.io/fastblog/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "relUrl": "/customer%20analytics/2020/05/12/Customer-Analytics-Part-2.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Factorization into $A=LU$ (18.06_L4)",
            "content": "Inverse of $AB$, $A^T$ . Review . So we know from previous lectures/blog posts that $A$ time the inverse of $A$ gives the Identify Matrix $I$. The Identify matrix is matrix equivilent to multiplying by 1, no change to the matrix. . $AA^{-1}=I=A^{-1}A$ . AB Inverse . So if the Inverse of a Matrix time it&#39;s inverse gives the identity, how do we find the inverse of a product of matrices? It turns out you just multiply the inverses of those matrices in reverse. . Why in reverse? If you put socks and shoes on, you wouldn&#39;t invert that process by taking socks off first. You would invert it by doing the steps in reverse. Shoes off first. . A more technical understanding of this can be seen below: . $ABB^{-1}A^{-1}= I$ . Let&#39;s multiply the middle piece first. $BB^{-1}$ We know that equals the Identify matrix, and a matrix time the identity just gives that matrix. So then what you are left wtih is $AA^{-1}=I$, which we also know is true. . This work the other way as well: . $B^{-1}(A^{-1}A)B = I$ . $A^T$ . Transposes work in a similar way. We can tak transposes in reverse to get the Identity Matrix. . $AA^{-1}=I$ . $(A^{-1})^TA^T=I$ . Converting to $A = LU$ . 2x2 to $A=LU$ . Previously we learned that using Elimination we create these Elimination Matrices. In this example of a 2x2 matrix: . $E_{21}A = U =&gt; $ $ left[ begin{array}{cc} 1 &amp; 0 -4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Now how do we get $A=LU$ from this. We can see matrix $A$ and matrix $U$ are already in the right place, so lets leave those alone. We can see that we need to turn Matrix $E_{21}$ into Matrix $L$ by switching it&#39;s sides. The way we move a matrix from one side to the other is through the inverse of the matrix. . $A=LU=&gt;$ $ left[ begin{array}{cc} 2 &amp; 1 8 &amp; 7 end{array} right]$ $=$ $ left[ begin{array}{cc} 1 &amp; 0 4 &amp; 1 end{array} right]$ $ left[ begin{array}{cc} 2 &amp; 1 0 &amp; 3 end{array} right]$ . Note: $E_{21}A = U$ is just a different format of $A=LU$. In $A=LU$ you can see $L$ is just $E_{21}^{-1}$ . 3x3 to $A = LU$ . As we saw in an earlier blog, when we do elimination of a 3x3 matrix we would get more Elmination matrices. We are still working examples with no row exchanges. . $$E_{32}E_{31}E_{21}A=U$$ . So how do we convert this into $A = LU$? As we learned above, we can invert the product of matrices as well. Let&#39;s apply that here with these matrices. . $$A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U$$ . Well that doesn&#39;t look as nice to work with as $A = LU$. So let&#39;s just simplify it by multiplying all the Elimination inverse matrices together. Meaning L would be a product of these 3 inverse matrices. . $$A=E^{-1}U=LU$$ . $A=LU$ vs $EA=U$ . What&#39;s the point of taking the inverse. It&#39;s the same formula mathamatrically, so why bother with this transformation. Why $E^{-1}$ on the right and not just stay with $E$ on the left? Let&#39;s do an example. . $E_{32}E_{21}=E =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; -5 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 -2 &amp; 1 &amp; 0 10 &amp; -5 &amp; 1 end{array} right]$ . $E_{21}^{-1}E_{32}^{-1}=E^{-1} =&gt;$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{array} right]$ $ left[ begin{array}{ccc} 1 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ $=$ $ left[ begin{array}{ccc} 1 &amp; 0 &amp; 0 2 &amp; 1 &amp; 0 0 &amp; 5 &amp; 1 end{array} right]$ . Note: 2 and 5 were the multipliers that we used in our elinimation steps. We can see that with $E^{-1}$ has 2 and 5 in the matrix. So if there are no row exchanges L is just a record of what the multipliers were, which is very convenient. Worded another way, with no row exchanges the multipliers go directly into L. .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "relUrl": "/linear%20algebra/2020/05/12/18.06_4_Factorization-into-a-lu.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Random Forest Classifier",
            "content": "Goal . The goal is to predict whether a passenger on the Titanic survived or not. The applications for binary classification are endless and could be applied to many real world problems. Does this patient have this disease? Will this customer Churn? Will price go up? These are just a few examples. . The purpose is to give a general guide to classification. If you get through this and want more detail, I highly recommend checking out the Tabular Chapter of Deep Learning for Coders with fastai &amp; Pytorch by Jeremy Howard and Sylvain Gugger. The book primarily focuses on deep learning, though decision trees are covered for tabular data. All of the material in this guide and more is covered in much greater detail in that book. . https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527 . Setup . We are going to start with loading libraries and datasets that are needed. I am going to skip over this as they are pretty self explanatory, but feel free to look close if you would like. . I am going to use Seaborn to load the Titanic dataset. . from sklearn.ensemble import RandomForestClassifier import seaborn as sns import pandas as pd import numpy as np from fastai2.tabular.all import * from fastai2 import * from sklearn.model_selection import GridSearchCV from dtreeviz.trees import * from scipy.cluster import hierarchy as hc df = sns.load_dataset(&#39;titanic&#39;) df.head() . . survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone . 0 | 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | Third | man | True | NaN | Southampton | no | False | . 1 | 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | First | woman | False | C | Cherbourg | yes | False | . 2 | 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | Third | woman | False | NaN | Southampton | yes | True | . 3 | 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | First | woman | False | C | Southampton | yes | False | . 4 | 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | Third | man | True | NaN | Southampton | no | True | . Data Setup . Dependent Variable . We want to define what we are predicting, or the dependent variable. We also see that &#39;Survived&#39; and &#39;alive&#39; columns are the same thing with different names. We need to get rid of one and predict the other. . df.drop(&#39;survived&#39;,axis = 1, inplace=True) dep_var = &#39;alive&#39; . Training and Validation Set Split . Best practice is to minimally have a training and validation set. Those are the 2 that we will use for this tutorial. . Training Set: This is what the model actually trains on | Validation Set: This is used to gauge success of the Training | Test Set: This is a held out of the total process to be an additional safeguard against overfitting | . cond = np.random.rand(len(df))&gt;.2 train = np.where(cond)[0] valid = np.where(~cond)[0] splits = (list(train),list(valid)) . Dates . We don&#39;t have any dates to deal with, but if we did we would do the following: . df = add_datepart(df,&#39;date&#39;) . This would replace that date with a ton of different columns, such as the year, the day number, the day of the week, is it month end, is it month start, and more. . Categorical Variables . Ordinal Categorical Variables . Some categorical variables have a natural heirarchy. By telling pandas the order it tends to mean trees don&#39;t have to split as many times, which speeds up training times. . df[&#39;class&#39;].unique() . [Third, First, Second] Categories (3, object): [Third, First, Second] . classes = &#39;First&#39;,&#39;Second&#39;,&#39;Third&#39; . df[&#39;class&#39;] = df[&#39;class&#39;].astype(&#39;category&#39;) df[&#39;class&#39;].cat.set_categories(classes, ordered=True, inplace=True) . Categorical Variables Final . We are now going to do some data cleaning. The Categorify and FillMissing functions in the fastai2 library make this easy. . procs = [Categorify, FillMissing] . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . Let&#39;s take a look at the training and validation sets and make sure we have a good split of each. . len(to.train),len(to.valid) . (713, 178) . We can now take a look and see that while we see all the same data, behind the scenes it is all numeric. This is exactly what we need for our random forest. . to.show(3) . sex embarked class who adult_male deck embark_town alone age_na pclass age sibsp parch fare alive . 0 male | S | Third | man | True | #na# | Southampton | False | False | 3.0 | 22.0 | 1.0 | 0.0 | 7.250000 | no | . 3 female | S | First | woman | False | C | Southampton | False | False | 1.0 | 35.0 | 1.0 | 0.0 | 53.099998 | yes | . 4 male | S | Third | man | True | #na# | Southampton | True | False | 3.0 | 35.0 | 0.0 | 0.0 | 8.050000 | no | . to.items.head(3) . pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone age_na . 0 | 3.0 | 2 | 22.0 | 1.0 | 0.0 | 7.250000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 1 | 1 | . 3 | 1.0 | 1 | 35.0 | 1.0 | 0.0 | 53.099998 | 3 | 1 | 3 | 1 | 3 | 3 | 1 | 1 | 1 | . 4 | 3.0 | 2 | 35.0 | 0.0 | 0.0 | 8.050000 | 3 | 3 | 2 | 2 | 0 | 3 | 0 | 2 | 1 | . Final Change . Finally, we will put just the data in xs and ys so they are in easy format to pass to models. . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Random Forest Model . Initial Model . Let&#39;s start by creating a model without tuning and see how it does . m = RandomForestClassifier(n_estimators=100) m = m.fit(xs,y) . from sklearn.metrics import confusion_matrix . confusion_matrix(y,m.predict(xs)) . array([[445, 1], [ 5, 262]]) . Looking pretty good! Only 6 wrong. Let&#39;s see how it did on the validation set. . confusion_matrix(valid_y,m.predict(valid_xs)) . array([[89, 14], [26, 49]]) . Still way better than 50/50, but not quite as good. This is because the model did not train based on this validation data so it doesn&#39;t perform nearly as well. . Model Tuning - Grid Search . We made our first model, and it doesn&#39;t seem to predict as well as we would like. Let&#39;s do something about that. . We are going to do a grid search. There are many more sophisticated ways to find parameters (maybe a future post), but the grid search is easy to understand. Basically you pick some ranges, and you try them all to see what works best. . We will use the built in gridsearch. All we need to do is define the range of parameters, and let it find the best model. . parameters = {&#39;n_estimators&#39;:range(10,20,20), &#39;max_depth&#39;:range(10,20,20), &#39;min_samples_split&#39;:range(2,20,1), &#39;max_features&#39;:[&#39;auto&#39;,&#39;log2&#39;]} . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) . clf.fit(xs,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . We can see below that the best esimator works better for prediciton the validation set than the model above did. Success! . confusion_matrix(y,clf.best_estimator_.predict(xs)) . array([[431, 15], [ 43, 224]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs)) . array([[95, 8], [25, 50]]) . Model Minimizing . Now that we have good results with a tuned model, we may want to simplify the model. If we can simplify the model without significantly impacting accuracy, that&#39;s good for many reasons. . The model is easier to understand | Fewer variables means fewer data quality issues and more focused data quality efforts | It takes less resources and time to run | Feature Importance . There are many ways to measure importance. How often do we use a feature to split? How high up in the tree is it used to split? We are going to use scikit learns feature importance information. . Let&#39;s look at what features are! . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . . fi = rf_feat_importance(m, xs) fi[:5] . cols imp . 13 | fare | 0.205103 | . 10 | age | 0.198317 | . 4 | adult_male | 0.104956 | . 0 | sex | 0.097934 | . 3 | who | 0.094934 | . Alright so we see that the most important variable is how much the passenger paid for their fare. Lovely. . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . Remove Low Important Variables . This isn&#39;t strictly neccesarry, but it is nice to simplify models if you can. Simpler models are easier to understand and maintain, and they take less resources to run. It is also interesting to know just how many variables are needed to predict. . to_keep = fi[fi.imp&gt;0.045].cols len(to_keep) . 8 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1) clf.fit(xs_imp,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Results . Now we see with only 8 features we still get pretty good results on on validation set. . Now the question is whether this small loss in accuracy outweighed by a simpler and more efficient model? That is a business question more than it is a data science question. . If you are detecting COVID-19, you probably want it to be as accurate as possible. If you are going to predict whether someone is a cat or a dog person based on a survey for marketing purposes, small changes in accuracy probably are not as critical. . confusion_matrix(y,clf.best_estimator_.predict(xs_imp)) . array([[422, 24], [ 62, 205]]) . confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)) . array([[98, 5], [24, 51]]) . clf.best_estimator_ . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=14, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . Redundant columns . Of the 6 remaining variables, we can see that some of them are very related. It makes sense to me that deck and fare are related. Nicer areas probably cost more. It makes sense to me that the person&#39;s sex has some redudancy with adult_male - the redundancy is even in the name. . def cluster_columns(df, figsize=(10,6), font_size=12): corr = np.round(scipy.stats.spearmanr(df).correlation, 4) corr_condensed = hc.distance.squareform(1-corr) z = hc.linkage(corr_condensed, method=&#39;average&#39;) fig = plt.figure(figsize=figsize) hc.dendrogram(z, labels=df.columns, orientation=&#39;left&#39;, leaf_font_size=font_size) plt.show() . . cluster_columns(xs_imp) . Now from the chart above, we can clearly see that class and pclass are completely redundant. We see the sex and adult_male has some redundancy as well. This makes sense as part of the adult_male column is the sex. Let&#39;s go ahead and drop one of the class or pclass variables (they are redundant so doesn&#39;t matter which). . xs_imp = xs_imp.drop(&#39;pclass&#39;, axis=1) valid_xs_imp = valid_xs_imp.drop(&#39;pclass&#39;, axis=1) . xs_imp.head() . fare age adult_male sex who deck class . 0 | 7.250000 | 22.0 | 2 | 2 | 2 | 0 | 3 | . 3 | 53.099998 | 35.0 | 1 | 1 | 3 | 3 | 1 | . 4 | 8.050000 | 35.0 | 2 | 2 | 2 | 0 | 3 | . 5 | 8.458300 | 28.0 | 2 | 2 | 2 | 0 | 3 | . 6 | 51.862499 | 54.0 | 2 | 2 | 2 | 5 | 1 | . clf.fit(xs_imp,y) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: range(10, 20, 20), &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;min_samples_split&#39;: range(2, 20), &#39;n_estimators&#39;: range(10, 20, 20)}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=None, verbose=0) . Ok, so now on to variables that are not completely redundant. Let&#39;s experiment with removing some columns and see what we get. We will use accuracy for our metric. . Here is out baseline: . print(&quot;accuracy: &quot;) (confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,clf.best_estimator_.predict(valid_xs_imp)).sum() . . accuracy: . 0.8258426966292135 . def get_accuracy(x,y,valid_x,valid_y): m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=10, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) m.fit(xs_imp,y) print((confusion_matrix(valid_y,m.predict(valid_xs_imp))[0,0] + confusion_matrix(valid_y,m.predict(valid_xs_imp))[1,1] )/ confusion_matrix(valid_y,m.predict(valid_xs_imp)).sum()) . . We will now loop through each of the remaining variables and train a model and print out the accuracy score. . Judging by the scores below, removing any 1 variable does not significantly reduce the accuracy. This means that we have redundant columns that can likely be trimmed. Sex seems to be a column we would definitely keep as removing it have the most impact on accuracy. . From this we can remove variables and iterate through to continue simplifying as much as possible. . variables = list(xs_imp.columns) for variable in variables: print(&#39;drop &#39;+variable+&#39; accuracy:&#39;) get_accuracy(xs_imp.drop(variable, axis=1), y, valid_xs_imp.drop(variable, axis=1), valid_y) . . drop fare accuracy: 0.8539325842696629 drop age accuracy: 0.8258426966292135 drop adult_male accuracy: 0.8370786516853933 drop sex accuracy: 0.8258426966292135 drop who accuracy: 0.8314606741573034 drop deck accuracy: 0.8146067415730337 drop class accuracy: 0.8258426966292135 .",
            "url": "https://isaac-flath.github.io/fastblog/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "relUrl": "/tree%20ensembles/2020/05/11/RandomForestClassifier.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Multiplication and Inverse Matrices (18.06_L3)",
            "content": "Matrix Multiplication . Rules . A matrix is laid out by row and column. Menaing, a particular cell in matrix $C$ is $C_{ij}$. For example, $C_{34}$ is the 3rd row, 4th column in matrix $C$. . $ begin{bmatrix} 1,1&amp;1,2&amp;1,3&amp;1,4 2,1&amp;2,2&amp;2,3&amp;2,4 3,1&amp;3,2&amp;3,3&amp;3,4 4,1&amp;4,2&amp;4,3&amp;4,4 end{bmatrix}$ . Number of columns in matrix A must match number of rows in matrix B. | The output of matrix multiplication will be dimensions equal to the number of rows in matrix A by the number of columns in matrix B. | You can cut the matrix into blocks and do matrix multiplication in blocks. | $ begin{bmatrix}A_1&amp;A_2 A_3&amp;A_4 end{bmatrix} begin{bmatrix}B_1&amp;B_2 B_3&amp;B_4 end{bmatrix}= begin{bmatrix}A_1B_1+A_2B_3&amp;A_1B_2+A_2B_4 A_3B_1+A_4B_3&amp;A_3B_2+A_4B_4 end{bmatrix}$ . Note: While cutting it into blocks may not seem useful immediately, it is crucial for high speed computation. In Deep Learning where you are multiplying large matrices together can speed up computation speed immensely by breaking them into blocks so you can fit the blocks into CPU Memory. In fact, this is exactly what pytorch does. . 1st Way . Let&#39;s imagine we have a matrix multiplation . $$AB=C$$ . $C_{34}=$(row 3 of A)$ cdot$(column 4 of B) . $C_{34}=a_{31}b_{14}+a_{32}b_{24}+......$ . $C_{34}= sum limits_{k=1}^n a_{3k}b_{k4}$ . 2nd Way . $$AB=C$$ . The second way to think about it is that matrix $A$ times by the first column of matrix $B$ will give you the first column of $C$. . Matrix $A$ times by the second column of matrix $B$ will give you the second column of $C$. . Now really what we are doing is thinking of columns of $C$ as combinations of columns of $A$ . $A cdot B_{n1} =C_{m1}$ . 3rd Way . $$AB=C$$ . The third way to think about it is that a row of $A$ times matrix $B$ will give you a column of $C$. . Now really what we are doing is thinking of rows of $C$ as combinations of rows of $B$. . 4th Way . If we multiply a column by a row, we get a full sized matrix. We also see that the columns are multiples of the column, and the rows and multiples of the row. This is what we expect as we just discussed above with the combinations of each other. . $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}= begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . $AB=$Sum of (Cols of A) $ cdot$ (Rows of B) . $ begin{bmatrix}2&amp;7 3&amp;8 4&amp;9 end{bmatrix} begin{bmatrix}1&amp;6 0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2 3 4 end{bmatrix} begin{bmatrix}1&amp;6 end{bmatrix}+$ $ begin{bmatrix}7 8 9 end{bmatrix} begin{bmatrix}0&amp;0 end{bmatrix}$ $=$ $ begin{bmatrix}2&amp;12 3&amp;18 4&amp;24 end{bmatrix}$ . This matrix all sit on the same line because they are just multiples. . Inverses . Not all Matrices have inverses. Most important question is whether it&#39;s invertable. If $A^-1$ exists. . $A^{-1}A = I = AA^{-1}$ . Invertable, nonsingular are the good case. . Matrices with no inverse . In the singular case there is no inverse. 2x2 matrix that has no inverse. . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix}$ . Cannot get 1,0, because I can find a vector $x$ with $Ax=0$ . $ begin{bmatrix}1&amp;3 2&amp;6 end{bmatrix} begin{bmatrix}3 -1 end{bmatrix}= begin{bmatrix}0 0 end{bmatrix}$ . But why does this matter? Because if I use these values for X anything I multiply by cannot possibly give the identify matrix, because 0 times anything gives 0. . Matrices with an inverse . $AA^{-1}=I =&gt; begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a&amp;b c&amp;d end{bmatrix}= begin{bmatrix}1&amp;0 0&amp;1 end{bmatrix}$ . Now from our matrix multiplication work above we know that $A cdot$column $j$ of $A^{-1} = $column $j$ of $I$ . So we really have a system of 2 equations to solve. . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a c end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}b d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . So we are back to solving systems of equations. . Gauss-Jordan / Find $A^{-1}$ . Solve 2 equations at once . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}a b end{bmatrix}= begin{bmatrix}1 0 end{bmatrix}$ . $ begin{bmatrix}1&amp;3 2&amp;7 end{bmatrix} begin{bmatrix}c d end{bmatrix}= begin{bmatrix}0 1 end{bmatrix}$ . The gauss-Jordan Method solved both equations at once by created an augmented matrix. . We will now do elimination steps to get the identify on the left. This will convert A to A inverse. . Start with $AI$ | Elimination step subtracting 2 of the first row from the second row | Elinination step subtracting 3 of the second row from the first. | End with $IA^{-1}$ | $ left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 2 &amp; 7 &amp; 0 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 3 &amp; 1 &amp; 0 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] =&gt; left[ begin{array}{cc|cc} 1 &amp; 0 &amp; 7 &amp; -3 0 &amp; 1 &amp; -2 &amp; 1 end{array} right] $ . Naturally we get the inverse because we really reverse the sides of the equation. Just like in algebra if you have -5 on one side, you can move it by putting the inverse on the other side (+5). .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "relUrl": "/linear%20algebra/2020/05/11/18.06_3_Multiplication-and-Inverse-Matrices.html",
            "date": " • May 11, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Descriptive Customer Analytics (Part 1)",
            "content": "Intro . Customer Analytics is a valuable tool, but often the basics are overlooked. It can have a huge impact, and it doesn’t always mean complex modeling. It always starts with the basics. Because customer analytics can make such a big impact, I am going to break this into several posts. . In this post (Part 1) I am going to explain a basic descriptive customer analytics framework and how use it to identify the current state.  This understanding allows us to conduct A/B testing so we can measure how our actions impact revenue and by how much. There are many ways to do this, this is just one of them. . In Part 2 I’ll talk about how to build on the descriptive framework to create a predictive model.  A predictive model allows companies to plan proactively instead of reactively. . Background: . Many companies rely on experience and basic summarizing of historical statistics to make decisions.  While both of these are useful tools, a powerful way to successfully sustain long term revenue growth is to add customer analytics to their tool belt.  In this post I am discussing the starting point for analytics, descriptive analytics. . An easy and solid starting point for descriptive analytics is a RFM (Recency, Frequency, Monetary Value) model.  A primary advantage to this is the only data you need is purchase history for each customer, which is information that almost every company tracks.  I’ve outlined the three main steps to creating this framework and using it to increase revenue. . Determine length of customer buying cycle: . The first step is to determine what time range is a good time range to use for a buying cycle.  For a grocery store, maybe we say typically people go grocery shopping once a week.  In that scenario we would use 1 week as our time range for our cycle.  However a company that prints business cards may find a much longer time frame as customers tend to buy 1,000 and repurchase when they get low.  For this example we could use a 6 month cycle.  I encourage people to explore their data and pick a time range based on intuition.  There are better and more complicated ways to figure out this time range, but I encourage people to demonstrate results and revenue growth before circling back with more resources and support from executive management. . Calculate descriptive customer metrics: . Let’s say we choose a 1 month cycle.  The next step is to create and fill in a table to get frequency and monetary Value.  In the table below I outline how we calculate both of those KPIs (Key Performance Indicators). .   January February March Frequency Monetary Value . Example | Rev | Rev | Rev | # of slots purchased in | Average Monetary Value | . Customer A | $102 | $60 | $75 | 3 | =($102+$60+$75) / 3 | . Customer B | $97 | $85 | $0 | 2 | =($85+97) / 2 | . Customer C | $0 | $9852 | $0 | 1 | =($9,852) / 1 | . Once this is done add another column for Recency and fill that in with how long since that last purchase.  This can be days, hours, or seconds since last purchase depending on how far apart customer purchases typically are.  For this simplified version we will use days.  We end up with a table like this: .   Recency Frequency Monetary Value . Customer A | 3 | 3 | $79 | . Customer B | 35 | 2 | $91 | . Customer C | 48 | 1 | $9852 | . Conduct A/B testing: . Now that we have three metrics (RFM) that describe customer buying patterns, we can start using them to run experiments to drive revenue growth.  A/B testing is a simple experiment that you can run to compare two things. You split your customers into two group and make no change to what you are doing to group A, but do something different in group B.  By doing this we can compare the revenue of the two groups to see what impact our efforts had.  If B is better, we will apply that action to the entire customer base.  If not, we scrap the idea and move on.  This is a never-ending process of  testing new ideas. . Here’s a few ideas for A/B tests using RFM to get you started: . Call every customer in group B to see how that impacts revenue.  Does this increase Frequency (F) or average Monetary Value (M) of orders?  Is it worth the additional resources? | Send a marketing email blast to everyone in group B.  How does this change group B’s buying patterns? | What impact does sending a one-time-only 5 % off coupon have? | If we raise the price by 5 % in group B, what is the effect on revenue?  This is especially useful because we can run limited experiments and gauge reaction of price hikes without angering every customer. | If we raise the price on a product that is nearing the end of its product life cycle, do sales increase on the newly released product?  Does a price hike on old products encourage customers to buy newer versions? | Send group A customers to the current website and group B customers to a new website layout.  Monitor sales, traffic, and clicks on each site.  This can help determine the better layout. | .",
            "url": "https://isaac-flath.github.io/fastblog/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "relUrl": "/customer%20analytics/2020/05/10/Customer-Analytics-Part-1.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Elimination with Matrices (18.06_L2)",
            "content": "Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 2 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns, just like it was with lecture 1. In the first Lecture it was about understanding how to solve them intuitively with linear combinations and matrix multiplication. In reality, that is not a scalable choice when you get to higher and higher dimensions. We need to be able to express these concepts in matrices and be more comfortable with that language and operations.. . Matrix Form . As a recap, we can express a system of equations by listing the equations. x . $x+2y+z=2$ . $3x+8y+z=12$ . $4y+z=2$ . We can write the same thing in matrix form using the formula $Ax=b$. The matrix A is from the system of equation above. . $A = begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}$ . Elimination . The general idea in elimination is to isolate variables so we can solve the equation. For example, if we have 0x + 0y + 1z = 10, it is very easy to solve for z. We can then plug it into another equation in the series that has z and 1 other unknown to get another value, and so on and so forth. . $E_{21}$ . We are going to start by eliminating the first variable in the second row. Row 2, column 1. This would leave the second row with only 2 unknowns. . The way we do this is we subtract row 1 from row 2. If we substract 3 * 1 from 3, we get 0, so 3 will be the multiplier. . $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{31}$ . We then move on to row 3, column 1. Lucky for us it&#39;s already 0 so we can skip this step . $E_{32}$ . We now move on to row 3, column 2. If we can get this to 0, then we have 1 unknown in that column. That&#39;s what we want as that&#39;s easily solvable. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . How can this fail? . A 0 cannot be in one of the pivot spots. The pivot positions are the diagonals. If that happens, elimination will fail. If you think about it, a 0 being in the pivot spot means that the left side of the equation is 0. Either the right side is also 0 and it gives you no information, or the right side is not 0 and there is no solution (ie 2 = 0 is False). . Of course, there are some tricks that can be added to minimize failures. The most common of these are row exchanges. By swapping the order of the rows, you can potentially solve an equation that would have failed in the original order. . For example, if I were to exchange the 2nd and 3rd rows I would multiply by the following matrix. . $ begin{bmatrix}1&amp;0&amp;0 0&amp;0&amp;1 0&amp;1&amp;0 end{bmatrix}$ . Back Substitution . Let&#39;s recap the steps that were taken. We started with A and through elimination in 2 steps we ended with a new matrix. . $A =&gt; begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} =&gt; begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} =&gt; U$ . Finish the Equation . You may have realized these matrices represented the left side of the equation. You may have wondered how we can modify the left side of the equation while leaving the right side alone? The answer is that we cannot. What is done on the left side must be applied to the right side. Let&#39;s do that now. . We have done 2 transformations. . The first transformation was subtracting 3 of the first row from the second. | The second transformation was subracting 2 of the second row from the third. | Let&#39;s do that to the the right side . b =&gt; $ begin{bmatrix}2 12 2 end{bmatrix} =&gt; begin{bmatrix}2 6 2 end{bmatrix} =&gt; begin{bmatrix}2 6 -10 end{bmatrix} =&gt; c$ . Let&#39;s put the left and right side of the equations together to see what it looks like. . $Ux = C$ . $ begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} begin{bmatrix}x y z end{bmatrix}= begin{bmatrix}2 6 -10 end{bmatrix}$ . Final Solution . Intuitive Solution . Great! Let&#39;s translate the $Ux=C$ matrix above back to our systems of equations view just to see if we can see how this transformation helps us. . $x + 2y + z = 2$ . $2y - 2z = 6$ . $5z = -10$ . When we look at it here, we can solve them with some simple algebra starting from the bottom equation. . $5z = -10 ; ; ; mathbf{=&gt;} ; ; ; z = -10/5 ; ; ; mathbf{=&gt;} ; ; ; z = -2$ . $2y - 2z = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y - 2(-2) = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y + 4 = 6 ; ; ; mathbf{=&gt;} ; ; ; 2y = 2 ; ; ; mathbf{=&gt;} ; ; ; y = 1$ . $x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x+2-2=2$$x+2y+z=2 ; ; ; mathbf{=&gt;} ; ; ;x=2$ . Matrix Solution . So first we should ask if we have an intuitive solution, why bother with doing the whole thing in matrix format? Isn&#39;t it the same thing? . And yes, we will be doing the same thing. The reason is scalability and ability to transfer to N equation and N unknowns. There&#39;s a limit to what can be done by hand, and matrix form allows for easier scalability. . Recap . We did several steps intuitively. Let&#39;s recap our elimination steps from above . $E_{21}$ step $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix}$ . $E_{32}$ step $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;4&amp;1 end{bmatrix} = begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ . Simplify into 1 step . Great! Now let&#39;s just combine these so we only have 1 equation. . $E_{21}E_{32}A=U$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix} $ . Now let&#39;s simplify this by multiplying and combining my $E_{21}$ and $E_{31}$ matrices together. We will call the result $E$ . $E_{21}E_{32}=E$ $ begin{bmatrix}1&amp;0&amp;0 0&amp;1&amp;0 0&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 0&amp;0&amp;1 end{bmatrix} = begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix}$ . Great! Now we can use this to simplify our formula. . $EA=U$ $ begin{bmatrix}1&amp;0&amp;0 -3&amp;1&amp;0 6&amp;-2&amp;1 end{bmatrix} begin{bmatrix}1&amp;2&amp;1 3&amp;8&amp;1 0&amp;4&amp;1 end{bmatrix}= begin{bmatrix}1&amp;2&amp;1 0&amp;2&amp;-2 0&amp;0&amp;5 end{bmatrix}$ .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "relUrl": "/linear%20algebra/2020/05/10/18.06_2_EliminationWithMatrices.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Gradient Descent for Linear Regression",
            "content": "The goal of linear regression is to find parameter values that fit a linear function to data points.  The goodness of fit is measured by a cost function.  The lower the cost, the better the model.  Gradient Descent is a method to minimize a cost function.  Gradient descent is a widely used tool and is used frequently in tuning predictive models.  It’s important to understand the method so you can apply it to various models and are not limited to using black box models. . As I just mentioned, gradient descent is a method to reduce a cost function.  To understand how to minimize a cost function, you need to understand how cost is calculated.  For this post I will be using a very simple example; linear regression with one feature, two data points, and two regression coefficients.  I will use the sum of squares cost function to take the predicted line and slowly change the regression coefficients until the line passes through both points. . . In this example we could easily draw a line through the points without using gradient descent, but if we had more data points this would get trickier.  In the table below we can see what the data looks like that we are working with. . . The tree below illustrates how to solve for cost as well as how to improve the values of $ theta$ to minimize cost.  In the illustration above, $J = a^1 + a^2$ is the cost function we want to minimize.  As we can see, if the regression coefficients ($ theta_0+ theta_1$) do not give a good fit, then the difference between our predicted values and observed values will be large and we will have a high cost ($J$).  For low values, we will have a low cost ($J$).  The figure below shows us how to calculate cost from the regression coefficients ($ theta_0$ and $ theta_1$). . . The second thing this chart shows you is how to improve values of theta.  We used the formulas in the boxes to evaluate $J$, so now we will use the values on the edges to improve the parameter values.  Each regression coefficient has a path up to the cost function.  You get a path value for each $ theta$ on the tree by multiplying the edge values along that path.  For example: . $ theta_1 ; path ;value = x^1 (y^1_{pred} - y^1_{obs}) (1) + x^2 (y^2_{pred} - y^2_{obs}) (1)$ . The last step is to improve the value of $ theta$.  In order to improve the value of $ theta$, we need to multiply the path value by $ alpha$, and subtract that from that $ theta$.  $ alpha$ is a value that determines how large the increments will be taken during optimization.  If you pick an $ alpha$ value that is is too large, you risk missing the local optima.  If you choose an $ alpha$ value that is too small you will be very accurate, but it will be more computationally expensive. With more data points there would be more edges originating at $J$, and with more features there would be more thetas originating from the predicted values, but the same concept can be applied to these more complicated examples. .",
            "url": "https://isaac-flath.github.io/fastblog/gradient%20descent/2020/05/09/GradientDescentforLinearRegression-P1.html",
            "relUrl": "/gradient%20descent/2020/05/09/GradientDescentforLinearRegression-P1.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Geometry of Linear Equations (18.06_L1)",
            "content": "import matplotlib.pyplot as plt from torch import tensor from torch import solve import numpy as np from mpl_toolkits import mplot3d from mpl_toolkits.mplot3d import Axes3D . . Motivation . One of my goals is to understand more deeply what Neural Networks are doing. Another is to have an easier time understanding and implementing cutting edge academic papers. In order to work toward those goals, I am revisiting the Math behind Neural Networks. This time my goal is to understand intuitively every piece of the material forward and backward - rather than to pass a course on a deadline. . This blog post will be my notes about Lecture 1 from the following course: . Gilbert Strang. 18.06 Linear Algebra. Spring 2010. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA. . Goal . The goal is to solve N equations with N unknowns. We will start with 2 equations with 2 unknowns, then go to 3 equations with 3 unknowns. We will use an intuitive approach here to understand a bit about how linear equations work. . How do we multiply these together? . Matrix Multiplication (Ax=b) . $ begin{bmatrix} 2 &amp; 5 1 &amp; 3 end{bmatrix}$ $ begin{bmatrix} 1 2 end{bmatrix}$ $=1$ $ begin{bmatrix} 2 1 end{bmatrix}$ $+2$ $ begin{bmatrix} 5 3 end{bmatrix}$ $=$ $ begin{bmatrix} 12 7 end{bmatrix}$ . Ax is a linear combination of columns . 2 equations 2 unknowns . Ok, Let&#39;s look at a couple equations in a few different ways. The solution to these are any values of x and y that make both equations true. . $2x - y = 0$ . $-x + 2y = 3$ . Row Picture . We can take these 2 linear equations and plot them. . $2x - y = 0$ could be writen as $y = 2x$, which is trivial to graph. If we graph them both, we can see visually where they are both true (the intersection). . def plot_equations_2d(x_range,y_dict): for y in y_dict: plt.plot(x, y_dict[y], label=y) plt.xlabel(&#39;x&#39;, color=&#39;#1C2833&#39;) plt.ylabel(&#39;y&#39;, color=&#39;#1C2833&#39;) plt.legend(loc=&#39;upper left&#39;) plt.grid() plt.show() x = tensor(np.linspace(-4,4,100)) y_dict = {&#39;2x-y=0&#39;:2*x, &#39;-x+2y=3&#39;:(3 + x)/2} plot_equations_2d(x,y_dict) . Column Picture . We can rewrite out equations into a different notation, which gives us a more concise view of what is going on. You can see that the top row is the first equation, and the bottom row is the second. Same thing, written differently. . $x$ $ begin{bmatrix} 2 -1 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Graphed as Vectors . Now that we see them it in column form, we can graph the vectors . strt_pts = tensor([[0,0],[2,-1]]) end_pts = tensor([[2,-1],[1,2]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Summed as Vectors . Now that they are represented as vectors. Let&#39;s add 1X + 2Y vectors and see that we get (0,3). Simply make one vector start where the other ends . strt_pts = tensor([[0,0],[2,-1],[1,1]]) end_pts = tensor([[2,-1],[1,1],[0,3]]) diff = end_pts - strt_pts plt.ylim([-3, 3]) plt.xlim([-3, 3]) plt.quiver(strt_pts[:,0], strt_pts[:,1], diff[:,0], diff[:,1], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1.) plt.show() . . Matrix Form AX = b . We can see the same view in matrix notation. Same as the row and column view, just in a nice compressed format. . $ begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}$ $ begin{bmatrix} x y end{bmatrix}$ $=$ $ begin{bmatrix} 0 3 end{bmatrix}$ . Equations, 3 equations 3 unknowns . $2x - y = 0$ . $-x + 2y -z = -1$ . $-3y + 4z = 4$ . Matrix Form . $A=$ $ begin{bmatrix} 2 &amp; -1 &amp; 0 -1 &amp; 2 &amp; -1 0 &amp; -3 &amp; 4 end{bmatrix}$ $b=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . Row Picture . fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) x, z = tensor(np.linspace(-8,8,100)), tensor(np.linspace(-8,8,100)) X, Z = np.meshgrid(x,z) Y1 = 2*X Y2 = (-1 + X + Z) / 2 Y3 = (4*Z - 4)/3 ax.plot_surface(X,Y1,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y2,Z, alpha=0.5, rstride=100, cstride=100) ax.plot_surface(X,Y3,Z, alpha=0.5, rstride=100, cstride=100) plt.show() . . Column Picture . We can create the column picture and graph vectors, just like in 2D space. Graphing in 3D is harder to see, but it&#39;s the same concept. . In this example we can clearly see the solution is x = 0, y = 0, z = 1. . $x$ $ begin{bmatrix} 2 -1 0 end{bmatrix}$ $+y$ $ begin{bmatrix} -1 2 -3 end{bmatrix}$ $+z$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ $=$ $ begin{bmatrix} 0 -1 4 end{bmatrix}$ . strt_pts = tensor([[0,0,0],[0,0,0],[0,0,0]]) end_pts = tensor([[2,-1,0],[-1,2,-1],[0,-3,4]]) diff = end_pts - strt_pts fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlim([-5, 5]) ax.set_ylim([-5, 5]) ax.set_zlim([-5, 5]) plt.quiver(strt_pts[0,0], strt_pts[0,1], strt_pts[0,2], end_pts[0,0], end_pts[0,1], end_pts[0,2]) plt.quiver(strt_pts[1,0], strt_pts[1,1], strt_pts[1,2], end_pts[1,0], end_pts[1,1], end_pts[1,2]) plt.quiver(strt_pts[2,0], strt_pts[2,1], strt_pts[2,2], end_pts[2,0], end_pts[2,1], end_pts[2,2]) plt.show() . . Numpy Solver . Here&#39;s how you can solve the equation using Numpy. . a = np.array([[2, -1, 0], [-1, 2, -1], [0, -3, 4]]) b = np.array([0, -1, 4]) x = np.linalg.solve(a, b) print(x) . [ 0. -0. 1.] . Can I solve Ax = b for every b? . Do the linear combinations of the columns fill 3 dimensional space. . If you have some dimensionality in each direction, then you can. 3 equations with 3 unknowns can fill the 3D space as long as they don&#39;t sit on the same line or plane. . length_b = 20 b = np.array([list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10), list(np.random.rand(length_b)*10)]) for x in range(0,length_b): x = np.linalg.solve(a, b[:,x]) print(x) . [12.34080396 17.25419494 14.45223124] [4.3199813 5.08051595 5.06761607] [11.26347828 12.97487851 12.22512736] [5.70944029 7.59603383 7.26994807] [ 8.14004194 10.62910222 9.74376037] [2.95913308 5.38708946 4.07602836] [ 5.71158581 10.15204127 9.91490065] [ 9.64601458 14.15432693 11.82836017] [14.30704025 20.64995811 17.69860633] [ 7.83852946 14.51438959 12.77361829] [8.10022402 8.2034441 6.89683312] [4.83294145 4.45944243 3.902063 ] [4.97509585 9.38184883 9.47499918] [4.21358292 5.53636436 5.35998439] [ 7.49769118 11.83894164 10.43770683] [11.07197937 17.24535878 15.22695312] [6.41919127 6.39617205 5.86620567] [10.07150973 11.6354783 11.01565376] [10.97611756 12.95884772 10.99723627] [4.40229263 4.48480965 3.50238492] .",
            "url": "https://isaac-flath.github.io/fastblog/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "relUrl": "/linear%20algebra/2020/05/09/18.06_1_GeometryOfLinearEquations.html",
            "date": " • May 9, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://isaac-flath.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is my personal blog where I write about things of interest or use to me and those around me. .",
          "url": "https://isaac-flath.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://isaac-flath.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}