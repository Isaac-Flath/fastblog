{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics (Part 2)\n",
    "> Under the hood of a basic Deep Learning Image Classifier (Multi-Class)\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Isaac Flath\n",
    "- categories: [Neural Networks, Image Classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *\n",
    "from fastai2.data.external import *\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Today we will be working with the MNIST dataset. The goal is going to be to take an image of handwritten digits and automatically predict what number it is.  We will be building a Neural Network to do this.  This is building off of the Image Classifier Basics post where we classified into 3s and 7s.  If anything in this post is confusing, I reccomend heading over to part 1.  I am assuming that the content covered in Part 1 is understood.\n",
    "\n",
    "If you get through this and want more detail, I highly recommend checking out Deep Learning for Coders with fastai & Pytorch by Jeremy Howard and Sylvain Gugger. All of the material in this guide and more is covered in much greater detail in that book.\n",
    "\n",
    "https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Naturally, the first step is to get and load the data.  We'll look at it a bit along tohe way to make sure it was loaded properly as well.  We will be using fastai's built in dataset feature rather than sourcing it ourself.  I am skimming over this quickly as this was covered in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command downloads the MNIST_TINY dataset and returns the path where it was downloaded\n",
    "path = untar_data(URLs.MNIST)\n",
    "\n",
    "# This takes that path from above, and get the path for training and validation\n",
    "training = [x.ls() for x in (path/'training').ls().sorted()]\n",
    "validation = [x.ls() for x in (path/'testing').ls().sorted()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an image.  The first thing I reccomend doing for any dataset is to view something to verify you loaded it right. The second thing is to look at the size of it.  This is not just for memory concerns, but you want to generally know some basics about whatever you are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nGNgoB4wPn7SHJdc2Mu/kbjk9F7+W4tLjvvGv3PiuCS7/12UwCUn8+1fEC45njv/7jDjknT4988Bl5zQ0X/7+HFJevz7Z4FLjvXJv6scDAzOstgkS/99lGRgKP95E4t253v/tsknb/v57989OXQ5yav//s17/+9scNbBf0fQJav+/fv371+zAAOD279nYmiSs/7/+/clkolZfcLHf7uhYixw2f8MDLd493Ga/ru6rh3d2Fn/IOBVKBaPqN/79+/P3WUVIth8ySC35JUXVomhAgBLyVozIOqObgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F6B6BCFE250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view what one of the images looks like\n",
    "im3 = Image.open(training[6][1])\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what shape the underlying matrix is that represents the picture\n",
    "tensor(im3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Equation\n",
    "\n",
    "We are looking to do wx + b = y.   It seems that a Neural network should use some super fancy equation in it's layers, but that's all it is.  In a single class classifier, y has 1 column as it is predicting 1 thing.  In a multiclass classifier y has \"however-many-classes-you-have\" columns.\n",
    "\n",
    "### Tensor Setup\n",
    "\n",
    "The first thing I will do is get my xs and ys in tensors in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_t = list()\n",
    "for x in range(0,len(training)):\n",
    "    # For each class, stack them together.  Divide by 255 so all numbers are between 0 and 1\n",
    "    training_t.append(torch.stack([tensor(Image.open(i)) for i in training[x]]).float()/255)\n",
    "    \n",
    "validation_t = list()\n",
    "for x in range(0,len(validation)):\n",
    "    # For each class, stack them together.  Divide by 255 so all numbers are between 0 and 1\n",
    "    validation_t.append(torch.stack([tensor(Image.open(i)) for i in validation[x]]).float()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make sure images are the same size as before (ie we didn't screw anything up)\n",
    "training_t[1][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple average of one of our images.  It's a nice sanity check to see that we did things ok.  We can see that after averaging, we get a recognizable number.  That's a good sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6b6ba44250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(training_t[5].mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([60000, 10]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all our different images into 1 matrix.  Convert Rank 3 tensor to rank 2 tensor.\n",
    "x = torch.cat([x for x in training_t]).view(-1, 28*28)\n",
    "valid_x = torch.cat([x for x in validation_t]).view(-1, 28*28)\n",
    "\n",
    "# Defining Y.  I am starting with a tensor of all 0.  \n",
    "# This tensor has 1 row per image, and 1 column per class\n",
    "y = tensor([[0]*len(training_t)]*len(x))\n",
    "valid_y = tensor([[0]*len(validation_t)]*len(valid_x))\n",
    "\n",
    "# Column 0 = 1 when the digit is a 0, 0 when the digit is not a 0\n",
    "# Column 1 = 1 when the digit is a 1, 0 when the digit is not a 1\n",
    "# Column 2 = 1 when the digit is a 2, 0 when the digit is not a 2\n",
    "# etc.\n",
    "j=0\n",
    "for colnum in range(0,len(training_t)):\n",
    "    y[j:j+len(training_t[colnum]):,colnum] = 1\n",
    "    j = j + len(training[colnum])\n",
    "    \n",
    "j=0\n",
    "for colnum in range(0,len(validation_t)):\n",
    "    valid_y[j:j+len(validation_t[colnum]):,colnum] = 1\n",
    "    j = j + len(validation[colnum])\n",
    "\n",
    "\n",
    "# Combine by xs and ys into 1 dataset for convenience.\n",
    "dset = list(zip(x,y))\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "# Inspect the shape of our tensors\n",
    "x.shape,y.shape,valid_x.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect.  We have exactly what we need and defined above.  60,000 images x 784 pixels for my x.  A 60,000 images x 10 classes for the predictions.\n",
    "\n",
    "10,000 images make up the validation set.\n",
    "\n",
    "\n",
    "### Calculate wx + b\n",
    "Let's initialize our weights and biases, then do the matrix multiplication and make sure the output is the expected shape (60,000 images x 10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we will initialize paramaters.  This is just giving me random numbers.\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "# Initializze w and b weight tensors\n",
    "w = init_params((28*28,10))\n",
    "b = init_params(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 10]), torch.Size([10000, 10]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets do our linear equation and see what shape we get.\n",
    "(x@w+b).shape,(valid_x@w+b).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have the right number of predictions.  Obviously the predictions are no good.  There a couple things left to do.  The first thing we need to do is turn our Linear Equation into a Neural Network.  To do that we need to do this twice, with a ReLu inbetween."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    ">Note: You can check out the first Image Classifier blog post, which explains does this in a simpler problem (single class classifier) and assumes less pre-requisite knowledge.  I am assuming that the information in Part 1 is understood.  If you understand Part 1, you are ready for Part2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple Neural Network.  \n",
    "# This can have more layers by duplicating the patten seen below, this is just the fewest layers for demonstration.\n",
    "\n",
    "def simple_net(xb): \n",
    "    \n",
    "    # Linear Equation from above\n",
    "    res = xb@w1 + b1 #Linear\n",
    "    \n",
    "    # Replace any negative values with 0.  This is called a ReLu.\n",
    "    res = res.max(tensor(0.0)) #ReLu\n",
    "    \n",
    "    # Do another Linear Equation\n",
    "    res = res@w2 + b2 #Linear\n",
    "    \n",
    "    # return the predictions\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random weights.  \n",
    "# The number 30 here can be adjusted for more or less model complexity.\n",
    "\n",
    "multipliers = 30\n",
    "\n",
    "w1 = init_params((28*28,multipliers))\n",
    "b1 = init_params(multipliers)\n",
    "w2 = init_params((multipliers,10))\n",
    "b2 = init_params(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_net(x).shape # 60,000 images with 10 predictions per class (one per digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Weights and Biases\n",
    "\n",
    "We have predictions with random weights and biases.  What we need to do is to get these weights and biases to be the right numbers rather than random numbers.  To do this we need to use Gradient Descent  to improve the weights.  Here's roughly what we need to do:\n",
    "\n",
    "+ Create a loss function to measure how close (or far) off we are\n",
    "+ Calculate the gradient (slope) so we know which direction to step\n",
    "+ Adjust our values in that direction\n",
    "+ Repeat many times\n",
    "\n",
    "The first thing we need to use gradient descent is we need a loss function.  Let's use something simple, how far off we were.  If the correct answer was 1, and we predicted a 0.5 that would be a loss of 0.5.  We will do this for every class\n",
    "\n",
    "The one addition is that we will add something called a sigmoid.  All a sigmoid is doing is ensuring that all of our predictions land between 0 and 1.  We never want to predict anything outside of these ranges.\n",
    "\n",
    ">Note: If you want more of a background on what is going on here, please take a look at my series on Gradient Descent where I dive deeper on this.  We will be calculating a gradient - which are equivilant to the \"Path Value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    \n",
    "    # make all prediction between 0 and 1\n",
    "    predictions = predictions.sigmoid()\n",
    "    \n",
    "    # Difference between predictions and target\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4779, grad_fn=<MeanBackward0>),\n",
       " tensor(0.4793, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate loss on training and validation sets to make sure the function works\n",
    "mnist_loss(simple_net(x),y),mnist_loss(simple_net(valid_x),valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Gradient\n",
    "\n",
    "Now we have a function we need to optimize and a loss function to tell us our error.  We are ready for gradient descent.  Let's create a function to change our weights. \n",
    "\n",
    "First, we will make sure our datasets are in a DataLoader.  This is convenience class that helps manage our data and get batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: This is the same from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size of 256 - feel free to change that based on your memory\n",
    "dl = DataLoader(dset, batch_size=1000, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=1000)\n",
    "\n",
    "# Example for how to get the first batch\n",
    "xb,yb = first(dl)\n",
    "valid_xb,valid_yb = first(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    \n",
    "    # calculate predictions\n",
    "    preds = model(xb)\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    \n",
    "    # Adjust weights based on gradients\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: This is the same from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Accuracy on Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):    \n",
    "    # this is checking for each row, which column has the highest score.\n",
    "    # p_inds, y_inds gives the index highest score, which is our prediction.\n",
    "    p_out, p_inds = torch.max(xb,dim=1)\n",
    "    y_out, y_inds = torch.max(yb,dim=1)\n",
    "    \n",
    "    # Compre predictions with actual\n",
    "    correct = p_inds == y_inds\n",
    "    \n",
    "    # average how often we are right (accuracy)\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Accuracy on All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: This is the same from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    # Calculate accuracy on the entire validation set\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    \n",
    "    # Combine accuracy from each batch and round\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When classifying 3 vs 7 in part one, we just used 30 weights.  \n",
    "# With this problem being much harder, I will give it more weights to work with\n",
    "\n",
    "complexity = 500 \n",
    "w1 = init_params((28*28,complexity))\n",
    "b1 = init_params(complexity)\n",
    "w2 = init_params((complexity,10))\n",
    "b2 = init_params(10)\n",
    "\n",
    "params = w1,b1,w2,b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Below we will actually train our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:22.67%\n",
      "Accuracy:32.56%\n",
      "Accuracy:33.12%\n",
      "Accuracy:34.25%\n"
     ]
    }
   ],
   "source": [
    "lr = 50\n",
    "# epoch means # of passes through our data (60,000 images)\n",
    "epochs = 30\n",
    "loss_old = 9999999\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_epoch(simple_net, lr, params)\n",
    "    \n",
    "    # Print Accuracy metric every 10 iterations\n",
    "    if (i % 10 == 0) or (i == epochs - 1):\n",
    "        print('Accuracy:'+ str(round(validate_epoch(simple_net)*100,2))+'%')\n",
    "        \n",
    "    loss_new = mnist_loss(simple_net(x),y)\n",
    "    \n",
    "    loss_old = loss_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "A few key points:\n",
    "\n",
    "+ The Loss is not the same as the metric (Accuracy).  Loss is what the models use, Accuracy is more meaningful to us humans.\n",
    "+ We see that our loss slowly decreases each epoch.  Our accuracy is getting better over time as well.\n",
    "\n",
    "We start at about 10% accuracy, which makes sense.  With random weights we predict correctly 1/10 times.  With 10 digits that sounds like a random guess.  Over time, we slowly decrease the loss and after 30 epochs we are around 36% accuracy.  Much better!  We could keep training more to keep improving accuracy, but I think we see the idea.\n",
    "\n",
    "### This Model vs SOTA\n",
    "\n",
    "What is different about this model than a best practice model?\n",
    "+ This model is only 1 layer.  State of the art for image recognitions will use more layers.  Resnet 34 and Resnet 50 are common (34 and 50 layers).  This would just mean we would alternate between the ReLu and linear layers and duplicate what we are doing with more weights and biases.\n",
    "+ More weights and Biases.  The Weights and Biases I used are fairly small - I ran this extremely quickly on a CPU.  With the appropriate size weight and biases tensors, it would make way more sense to use a GPU.\n",
    "+ Matrix Multiplication is replaced with Convolutions for image recognition.  A Convolution can be thought of as matrix multiplication if you averaged some of the pixels together.  This intuitively makes sense as 1 pixel in itself is meaningless without the context of other pixels.  So we tie them together some.\n",
    "+ Dropout would make our model less likely to overfit and less dependent on specific pixels.  It would do this by randomly ignoring different pixels so it cannot rely on them.  It's very similar to how decision trees randomly ignore variables for their splits.\n",
    "+ Discriminate learning rates means that the learning rates are not the same for all levels of the neural network.  With only 1 layer, naturally we don't worry about this.\n",
    "+ Gradient Descent - we can adjust our learning rate based on our loss to speed up the process\n",
    "+ Transfer learning - we can optimize our weights on a similar task so when we start trying to optimize weights on digits we aren't starting from random variables.  \n",
    "+ Keep training for as many epochs as we see our validation loss decrease\n",
    "\n",
    "\n",
    "As you can see, these are not completely different models.  These are small tweaks to what we have done above that make improvements - the combination of these small tweaks and a few other tricks are what elevate these models.  There are many 'advanced' variations of Neural Networks, but the concepts are typically along the lines of above.  If you boil them down to what they are really doing without all the jargon - they are pretty simple concepts.  I'll cover them in future blog posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
